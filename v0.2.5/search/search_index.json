{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\"Quantum Evolution Kernel\"","text":""},{"location":"#installation","title":"Installation","text":""},{"location":"#using-hatch-uv-or-any-pyproject-compatible-python-manager","title":"Using <code>hatch</code>, <code>uv</code> or any pyproject-compatible Python manager","text":"<p>Edit file <code>pyproject.toml</code> to add the line</p> <pre><code>  \"quantum-evolution-kernel\"\n</code></pre> <p>to the list of <code>dependencies</code>.</p>"},{"location":"#using-pip-or-pipx","title":"Using <code>pip</code> or <code>pipx</code>","text":"<p>To install the <code>pipy</code> package using <code>pip</code> or <code>pipx</code></p> <ol> <li>Create a <code>venv</code> if that's not done yet</li> </ol> <pre><code>$ python -m venv venv\n</code></pre> <ol> <li>Enter the venv</li> </ol> <pre><code>$ . venv/bin/activate\n</code></pre> <ol> <li>Install the package</li> </ol> <pre><code>$ pip install quantum-evolution-kernel\n# or\n$ pipx install quantum-evolution-kernel\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code># Load a dataset\nimport torch_geometric.datasets as pyg_dataset\nog_ptcfm = pyg_dataset.TUDataset(root=\"dataset\", name=\"PTC_FM\")\n\n# Setup a quantum feature extractor for this dataset.\n# In this example, we'll use QutipExtractor, to emulate a Quantum Device on our machine.\nimport qek.data.graphs as qek_graphs\nimport qek.data.extractors as qek_extractors\nextractor = qek_extractors.QutipExtractor(compiler=qek_graphs.PTCFMCompiler())\n\n# Add the graphs, compile them and look at the results.\nextractor.add_graphs(graphs=og_ptcfm)\nextractor.compile()\nprocessed_dataset = extractor.run().processed_data\n\n# Prepare a machine learning pipeline with Scikit Learn.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX = [data for data in processed_dataset]  # Features\ny = [data.target for data in processed_dataset]  # Targets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\n\n# Train a kernel\nfrom qek.kernel import QuantumEvolutionKernel as QEK\nkernel = QEK(mu=0.5)\nmodel = SVC(kernel=kernel, random_state=42)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p> source package qek </p> <p>The Quantum Evolution Kernel is a Python library designed for the machine learning community to help users design quantum-driven similarity metrics for graphs and to use them inside kernel-based machine learning algorithms for graph data.</p> <p>The core of the library is focused on the development of a classification algorithm for molecular-graph dataset as it is presented in the published paper Quantum feature maps for graph machine learning on a neutral atom quantum processor.</p> <p>Users setting their first steps into quantum computing will learn how to implement the core algorithm in a few simple steps and run it using the Pasqal Neutral Atom QPU. More experienced users will find this library to provide the right environment to explore new ideas - both in terms of methodologies and data domain - while always interacting with a simple and intuitive QPU interface.</p> <p> Modules </p> <ul> <li> <p>qek.data \u2014 Data manipulation utilities.</p> </li> <li> <p>qek.kernel \u2014 The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> </li> <li> <p>qek.shared \u2014 Shared utility code.</p> </li> <li> <p>qek.backends \u2014 Low-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.</p> </li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>We have a two parts tutorial:</p> <ol> <li>Using a Quantum Device to extract machine-learning features;</li> <li>Machine Learning with the Quantum Evolution Kernel</li> </ol>"},{"location":"#getting-in-touch","title":"Getting in touch","text":"<ul> <li>Pasqal Community Portal (forums, chat, tutorials, examples, code library).</li> <li>GitHub Repository (source code, issue tracker).</li> <li>Professional Support (if you need tech support, custom licenses, a variant of this library optimized for your workload, your own QPU, remote access to a QPU, ...)</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>The GitHub repository is open for contributions!</p> <p>Don't forget to read the Contributor License Agreement.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/","title":"CONTRIBUTOR AGREEMENT","text":"<p>PASQAL CONTRIBUTION AGREEMENT (the \u201cAgreement\u201d)</p> <p>The author of the License is:   Pasqal, a Soci\u00e9t\u00e9 par Actions Simplifi\u00e9e (Simplified Joint Stock Company) registered under number 849 441 522 at the Registre du commerce et des soci\u00e9t\u00e9s (Trade and Companies Register) of Evry \u2013 France, headquartered at 24 rue \u00c9mile Baudot \u2013 91120 \u2013 Palaiseau \u2013 France, duly represented by its Pr\u00e9sident, M. Georges-Olivier REYMOND, Hereafter referred to as \u00ab the Licensor \u00bb</p> <p>In the course of its activities, Pasqal carries out and leads quantic projects, in their software components. These projects aim to bring together a community of like-minded individuals to contribute to the development and improvement of Pasqal\u2019s products. Pasqal clearly outlines which projects are open to contributions (\u201cProjects\u201d).</p> <p>This Agreement documents the rights granted by Contributors to Pasqal and is legally binding.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-1-definitions","title":"Article 1 Definitions","text":"<p>You, Your or Contributor : means the intellectual property rights owner or legal entity authorized by the intellectual property rights owner that is entering into this Agreement with\u00a0Pasqal</p> <p>Contribution : means any work, protected or not, that is submitted by You to Pasqal in which You own or assert ownership of the intellectual property rights, subject to proprietary licensing terms, and not otherwise distributer through an open-source license.</p> <p>Material : means the work object of the Project, made available by Pasqal to third parties. When this Agreement covers more than one Project, the Material means the work to which the Contribution was Submitted. After You Submit the Contribution, it may be included in any type of Material.</p> <p>Submit : means any form of electronic, verbal, or written communication sent to Pasqal or its representatives, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Pasqal for the purpose of discussing and improving the Material.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-2-entering-into-this-agreement","title":"Article 2 Entering into this Agreement","text":"<p>By Submitting any Contribution to Pasqal, You agree to enter into the Agreement with this entity, and be bound by the following terms.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-3-limitations-as-to-the-licensing-of-contributions","title":"Article 3 Limitations as to the licensing of Contributions","text":"<p>You guarantee that any Contribution You Submit to any Project is not licensed under any type of contaminating (even weakly contaminating) license, and that Pasqal will be free to combine your Contribution into any type of Material, for any Project, without any risk of contamination of said Material and/or Project.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-4-granting-of-a-copyright-andor-authors-rights-license","title":"Article 4 Granting of a Copyright and/or author\u2019s rights License","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Pasqal and to recipients of any Project distributed by Pasqal a worldwide, sublicensable, non-exclusive, royalty-free license to reproduce, prepare derivative works of, publicly display, publicly perform, and distribute Your Contributions and your Contribution in combination with the Material for the entire duration of the rights under Applicable law.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-5-granting-of-a-patent-license","title":"Article 5 Granting of a patent license","text":"<p>Subject to the terms and conditions of this Agreement, You grant Pasqal a, worldwide, non-exclusive, transferable, royalty free patent license, with the right to sublicense these rights to third parties, to make, have made, use, sell, offer for sale, import and otherwise  transfer the Contribution and the Contribution in combination with the Material for the entire duration of the rights under Applicable law, if the Contribution implies the implementation of any such patent.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-6-licensing-of-the-material-and-contribution","title":"Article 6 Licensing of the Material and Contribution","text":"<p>Based on the rights granted in articles 2 and 3, if Pasqal includes Your Contribution in a Material, Pasqal may license Material including Your Contribution under any license, whether permissive, weakly contaminating, or contaminating. Pasqal will respect Your moral rights in relation to Your Contribution as provided under the Applicable law.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-7-warranties-and-disclaimer","title":"Article 7 Warranties and disclaimer","text":"<p>You represent that each of Your Contributions is Your creation, or that you have obtained the authorization from the intellectual property rights owner on the Contribution to Submit it over the course of a Project.</p> <p>By making a Contribution, You confirm that, to the best of Your knowledge, the Contribution does not violate the rights of any person or entity. If You make a Contribution on behalf of Your employer, then You confirm that an appropriate representative of that employer has authorized the inclusion of such Contribution to a Project and that it meets these requirements.</p> <p>You acknowledge that Pasqal is not obligated to use Your Contribution as part of the Material and may decide to only include Contributions Pasqal considers appropriate.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-8-applicable-law-and-jurisdiction","title":"Article 8 Applicable law and jurisdiction","text":"<p>The Agreement is governed by the laws of France. Any dispute relating to the interpretation or application of the License shall be subject to best efforts for an amicable settlement. Any dispute relating to the License, notably its execution, performance and/or termination shall be brought to, heard and tried by the Tribunal Judiciaire de Paris, regardless of the rules of jurisdiction in the matter.</p>"},{"location":"LICENSE/","title":"License","text":"<p>PASQAL OPEN-SOURCE SOFTWARE LICENSE AGREEMENT (MIT-derived)</p> <p>The author of the License is:   Pasqal, a Soci\u00e9t\u00e9 par Actions Simplifi\u00e9e (Simplified Joint Stock Company) registered under number 849 441 522 at the Registre du commerce et des soci\u00e9t\u00e9s (Trade and Companies Register) of Evry \u2013 France, headquartered at 24 rue \u00c9mile Baudot \u2013 91120 \u2013 Palaiseau \u2013 France, duly represented by its Pr\u00e9sident, M. Georges-Olivier REYMOND, Hereafter referred to as \u00ab the Licensor \u00bb</p> <ul> <li> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software (the \u201cLicensee\u201d) and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. The Software is \u201cas is\u201d, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and non-infringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise arising from, out of or in connection with the Software or the use or other dealings in the Software.</p> </li> <li> <p>If use of the Software leads to the necessary use of any patent of the Licensor and/or any of its Affiliates (defined as a company owned or controlled by the Licensor), the Licensee is granted a royalty-free license, in any country where such patent is in force, to use the object of such patent; or use the process covered by such patent,</p> </li> <li> <p>Such a patent license is granted for internal research or academic use of the Licensee's, which includes use by employees and students of the Licensee, acting on behalf of the Licensee, for research purposes only.</p> </li> <li> <p>The License is governed by the laws of France. Any dispute relating to the License, notably its execution, performance and/or termination shall be brought to, heard and tried by the Tribunal Judiciaire de Paris, regardless of the rules of jurisdiction in the matter.</p> </li> </ul>"},{"location":"api/qek/","title":"Reference","text":"qek<p> source package qek </p> <p>The Quantum Evolution Kernel is a Python library designed for the machine learning community to help users design quantum-driven similarity metrics for graphs and to use them inside kernel-based machine learning algorithms for graph data.</p> <p>The core of the library is focused on the development of a classification algorithm for molecular-graph dataset as it is presented in the published paper Quantum feature maps for graph machine learning on a neutral atom quantum processor.</p> <p>Users setting their first steps into quantum computing will learn how to implement the core algorithm in a few simple steps and run it using the Pasqal Neutral Atom QPU. More experienced users will find this library to provide the right environment to explore new ideas - both in terms of methodologies and data domain - while always interacting with a simple and intuitive QPU interface.</p> <p> Modules </p> <ul> <li> <p>qek.data \u2014 Data manipulation utilities.</p> </li> <li> <p>qek.kernel \u2014 The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> </li> <li> <p>qek.shared \u2014 Shared utility code.</p> </li> <li> <p>qek.backends \u2014 Low-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.</p> </li> </ul>"},{"location":"src/qek/","title":"qek","text":"qek<p> docs package qek </p> <pre><code>\"\"\"\nThe Quantum Evolution Kernel is a Python library designed for the machine learning community to help users design quantum-driven similarity metrics for graphs and to use them inside kernel-based machine learning algorithms for graph data.\n\nThe core of the library is focused on the development of a classification algorithm for molecular-graph dataset as it is presented in the published paper [Quantum feature maps for graph machine learning on a neutral atom quantum processor](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.107.042615).\n\nUsers setting their first steps into quantum computing will learn how to implement the core algorithm in a few simple steps and run it using the Pasqal Neutral Atom QPU. More experienced users will find this library to provide the right environment to explore new ideas - both in terms of methodologies and data domain - while always interacting with a simple and intuitive QPU interface.\n\"\"\"\n</code></pre>"},{"location":"api/qek/data/","title":"qek.data","text":"qek.data<p> source package qek.data </p> <p>Data manipulation utilities.</p> <p> Modules </p> <ul> <li> <p>qek.data.extractors \u2014 High-Level API to compile raw data (graphs) and process it on a quantum device, either a local emulator, a remote emulator or a physical QPI.</p> </li> <li> <p>qek.data.graphs \u2014 Loading graphs as raw data.</p> </li> <li> <p>qek.data.processed_data \u2014 Loading, saving, manipulation or processed data.</p> </li> <li> <p>qek.data.training_data \u2014 Manipulating training data</p> </li> </ul>"},{"location":"src/qek/data/","title":"qek.data","text":"qek.data<p> docs package qek.data </p> <pre><code>\"\"\"\nData manipulation utilities.\n\"\"\"\n</code></pre>"},{"location":"api/qek/data/extractors/","title":"qek.data.extractors","text":"qek.data.extractors<p> source module qek.data.extractors </p> <p>High-Level API to compile raw data (graphs) and process it on a quantum device, either a local emulator, a remote emulator or a physical QPI.</p> <p> Classes </p> <ul> <li> <p>Compiled \u2014 The result of compiling a graph for execution on a quantum device.</p> </li> <li> <p>Feature \u2014 A feature extracted from raw data.</p> </li> <li> <p>BaseExtracted \u2014 Data extracted by one of the subclasses of <code>BaseExtractor</code>.</p> </li> <li> <p>SyncExtracted \u2014 Data extracted synchronously, i.e. no need to wait for a remote server.</p> </li> <li> <p>BaseExtractor \u2014 The base of the hierarchy of extractors.</p> </li> <li> <p>QutipExtractor \u2014 A Extractor that uses the Qutip Emulator to run sequences compiled from graphs.</p> </li> <li> <p>EmuMPSExtractor \u2014 A Extractor that uses the emu-mps Emulator to run sequences compiled from graphs.</p> </li> <li> <p>PasqalCloudExtracted \u2014 Data extracted from the cloud API, i.e. we need wait for a remote server.</p> </li> <li> <p>BaseRemoteExtractor \u2014 An Extractor that uses a remote Quantum Device published on Pasqal Cloud, to run sequences compiled from graphs.</p> </li> <li> <p>RemoteQPUExtractor \u2014 An Extractor that uses a remote QPU published on Pasqal Cloud, to run sequences compiled from graphs.</p> </li> <li> <p>RemoteEmuMPSExtractor \u2014 An Extractor that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud, to run sequences compiled from graphs.</p> </li> </ul> <p> source dataclass Compiled(graph: BaseGraph, sequence: pl.Sequence) </p> <p>The result of compiling a graph for execution on a quantum device.</p> <p> source dataclass Feature(data: NDArray[np.floating]) </p> <p>A feature extracted from raw data.</p> <p> source class BaseExtracted(device: Device) </p> <p><p>Bases : abc.ABC</p></p> <p>Data extracted by one of the subclasses of <code>BaseExtractor</code>.</p> <p>Note that the list of processed data will generally not contain all the graphs ingested by the Extractor, as not all graphs may not be compiled for a given device.</p> <p> Attributes </p> <ul> <li> <p>raw_data :  list[BaseGraph] \u2014 A subset of the graphs ingested by the Extractor.</p> </li> <li> <p>targets :  list[int] | None \u2014 If available, the machine-learning targets for these graphs, in the same order and with the same number of entrie as <code>raw_data</code>.</p> </li> <li> <p>states :  list[dict[str, int]] \u2014 The quantum states extracted from <code>raw_data</code> by executing <code>sequences</code> on the device, in the same order and with the same number of entries as <code>raw_data</code>.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>features \u2014 The features extracted from <code>raw_data</code> by processing <code>states</code>, in the same order and with the same number of entries as <code>raw_data</code>.</p> </li> <li> <p>save_dataset \u2014 Saves the processed dataset to a JSON file.</p> </li> </ul> <p> source property BaseExtracted.processed_data: list[processed_data.ProcessedData] </p> <p> source property BaseExtracted.raw_data: list[BaseGraph] </p> <p>A subset of the graphs ingested by the Extractor.</p> <p> source property BaseExtracted.targets: list[int] | None </p> <p>If available, the machine-learning targets for these graphs, in the same order and with the same number of entrie as <code>raw_data</code>.</p> <p> source property BaseExtracted.states: list[dict[str, int]] </p> <p>The quantum states extracted from <code>raw_data</code> by executing <code>sequences</code> on the device, in the same order and with the same number of entries as <code>raw_data</code>.</p> <p> source method BaseExtracted.features(size_max: int | None) \u2192 list[Feature] </p> <p>The features extracted from <code>raw_data</code> by processing <code>states</code>, in the same order and with the same number of entries as <code>raw_data</code>.</p> <p>By default, the features extracted are the distribution of excitation levels based on <code>states</code>. However, subclasses may override this method to provide custom features extraction.</p> <p> Parameters </p> <ul> <li> <p>size_max (optional) Performance/precision lever. If specified, specifies the number of qubits to take into account from all \u2014</p> <p>the <code>states</code>. If <code>size_max</code> is lower than the number of qubits used to extract <code>self.states[i]</code> (i.e. the number of qubits in <code>self.sequences[i]</code>), then only take into account the <code>size_max</code> first qubits of this state to extract <code>self.features(size_max)[i]</code>. If, on the other hand, <code>size_max</code> is greater than the number of qubits used to extract <code>self.states[i]</code>, pad <code>self.features(size_max)[i]</code> with 0s. If unspecified, use the largest number of qubits in <code>selfsequences</code>.</p> </li> </ul> <p> source method BaseExtracted.save_dataset(file_path: Path) \u2192 None </p> <p>Saves the processed dataset to a JSON file.</p> <p>Note: This does NOT attempt to save the graphs.</p> <p> Parameters </p> <ul> <li> <p>dataset \u2014 The dataset to be saved.</p> </li> <li> <p>file_path :  Path \u2014 The path where the dataset will be saved as a JSON file.</p> </li> </ul> <p>Note</p> <p>The data is stored in a format suitable for loading with load_dataset.</p> <p> source class SyncExtracted(raw_data: list[BaseGraph], targets: list[int] | None, sequences: list[pl.Sequence], states: list[dict[str, int]]) </p> <p><p>Bases : BaseExtracted</p></p> <p>Data extracted synchronously, i.e. no need to wait for a remote server.</p> <p> source property SyncExtracted.processed_data: list[ProcessedData] </p> <p> source property SyncExtracted.raw_data: list[BaseGraph] </p> <p> source property SyncExtracted.targets: list[int] | None </p> <p> source property SyncExtracted.sequences: list[pl.Sequence] </p> <p> source property SyncExtracted.states: list[dict[str, int]] </p> <p> source class BaseExtractor(device: Device, compiler: BaseGraphCompiler[GraphType], path: Path | None = None) </p> <p><p>Bases : abc.ABC, Generic[GraphType]</p></p> <p>The base of the hierarchy of extractors.</p> <p>The role of extractors is to take a list of raw data (here, labelled graphs) into processed data containing machine-learning features (here, excitation vectors).</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 If specified, the processed data will be saved to this file as JSON once the execution is complete.</p> </li> <li> <p>device :  Device \u2014 A quantum device for which the data should be prepared.</p> </li> <li> <p>compiler :  BaseGraphCompiler[GraphType] \u2014 A graph compiler, in charge of converting graphs to Pulser Sequences, the format that can be executed on a quantum device.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>save \u2014 Saves a dataset to a JSON file.</p> </li> <li> <p>compile \u2014 Compile all pending graphs into Pulser sequences that the Quantum Device may execute.</p> </li> <li> <p>add_graphs \u2014 Add new graphs to compile and run.</p> </li> <li> <p>run \u2014 Run compiled graphs.</p> </li> </ul> <p> source method BaseExtractor.save(snapshot: list[ProcessedData]) \u2192 None </p> <p>Saves a dataset to a JSON file.</p> <p> Parameters </p> <ul> <li> <p>dataset :  list[ProcessedData] \u2014 The dataset to be saved, containing RegisterData instances.</p> </li> <li> <p>file_path :  str \u2014 The path where the dataset will be saved as a JSON file.</p> </li> </ul> <p>Note</p> <p>The data is stored in a format suitable for loading with load_dataset.</p> <p> source method BaseExtractor.compile(filter: Callable[[BaseGraph, pl.Sequence, int], bool] | None = None) \u2192 list[Compiled] </p> <p>Compile all pending graphs into Pulser sequences that the Quantum Device may execute.</p> <p>Once this method have succeeded, the results are stored in <code>self.sequences</code>.</p> <p> Raises </p> <ul> <li> <p>Exception</p> </li> </ul> <p> source method BaseExtractor.add_graphs(graphs: Sequence[GraphType] | Dataset[GraphType]) \u2192 None </p> <p>Add new graphs to compile and run.</p> <p> source method BaseExtractor.run() \u2192 BaseExtracted </p> <p>Run compiled graphs.</p> <p>You will need to call <code>self.compile</code> first, to make sure that the graphs are compiled.</p> <p> Returns </p> <ul> <li> <p>BaseExtracted \u2014 Data extracted by this extractor.</p> <p>Not all extractors may return the same data, so please take a look at the documentation of the extractor you are using.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>Exception</p> </li> </ul> <p> source class QutipExtractor(compiler: BaseGraphCompiler[GraphType], device: Device = pl.devices.AnalogDevice, path: Path | None = None) </p> <p><p>Bases : BaseExtractor[GraphType]</p></p> <p>A Extractor that uses the Qutip Emulator to run sequences compiled from graphs.</p> <p>Performance note: emulating a quantum device on a classical computer requires considerable amount of resources, so this Extractor may be slow or require too much memory.</p> <p>See Also</p> <ul> <li>EmuMPSExtractor (alternative emulator, generally much faster)</li> <li>QPUExtractor (run on a physical QPU)</li> </ul> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>compiler :  BaseGraphCompiler[GraphType] \u2014 A graph compiler, in charge of converting graphs to Pulser Sequences, the format that can be executed on a quantum device.</p> </li> <li> <p>device :  Device \u2014 A device to use. For general experiments, the default device <code>AnalogDevice</code> is a perfectly reasonable choice.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run \u2014 Run the compiled graphs.</p> </li> </ul> <p> source method QutipExtractor.run(max_qubits: int = 8) \u2192 SyncExtracted </p> <p>Run the compiled graphs.</p> <p>As emulating a quantum device is slow consumes resources and time exponential in the number of qubits, for the sake of performance, we limit the number of qubits in the execution of this extractor.</p> <p> Parameters </p> <ul> <li> <p>max_qubits :  int \u2014 Skip any sequence that require strictly more than <code>max_qubits</code>. Defaults to 8.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>SyncExtracted \u2014 Processed data for all the sequences that were executed.</p> </li> </ul> <p> source class EmuMPSExtractor(compiler: BaseGraphCompiler[GraphType], device: Device = pl.devices.AnalogDevice, path: Path | None = None) </p> <p><p>Bases : BaseExtractor[GraphType]</p></p> <p>A Extractor that uses the emu-mps Emulator to run sequences compiled from graphs.</p> <p>Performance note: emulating a quantum device on a classical computer requires considerable amount of resources, so this Extractor may be slow or require too much memory. If should, however, be faster than QutipExtractor in most cases.</p> <p>See Also</p> <ul> <li>QPUExtractor (run on a physical QPU)</li> </ul> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>compiler :  BaseGraphCompiler[GraphType] \u2014 A graph compiler, in charge of converting graphs to Pulser Sequences, the format that can be executed on a quantum device.</p> </li> <li> <p>device :  Device \u2014 A device to use. For general experiments, the default device <code>AnalogDevice</code> is a perfectly reasonable choice.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run \u2014 Run the compiled graphs.</p> </li> </ul> <p> source method EmuMPSExtractor.run(max_qubits: int = 10, dt: int = 10) \u2192 BaseExtracted </p> <p>Run the compiled graphs.</p> <p>As emulating a quantum device is slow consumes resources and time exponential in the number of qubits, for the sake of performance, we limit the number of qubits in the execution of this extractor.</p> <p> Parameters </p> <ul> <li> <p>max_qubits :  int \u2014 Skip any sequence that require strictly more than <code>max_qubits</code>. Defaults to 8.</p> </li> <li> <p>dt :  int \u2014 The duration of the simulation step, in us. Defaults to 10.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BaseExtracted \u2014 Processed data for all the sequences that were executed.</p> </li> </ul> <p> source class PasqalCloudExtracted(compiled: list[Compiled], batch_ids: list[str], sdk: SDK, state_extractor: Callable[[Job, pl.Sequence], dict[str, int] | None], path: Path | None = None) </p> <p><p>Bases : BaseExtracted</p></p> <p>Data extracted from the cloud API, i.e. we need wait for a remote server.</p> <p>Prepare for reception of data.</p> <p> Performance note </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you do not need to do so.</p> <p> Parameters </p> <ul> <li> <p>compiled :  list[Compiled] \u2014 The result of compiling a set of graphs.</p> </li> <li> <p>batch_ids :  list[str] \u2014 The ids of the batches on the cloud API, in the same order as <code>compiled</code>.</p> </li> <li> <p>state_extractor :  Callable[[Job, pl.Sequence], dict[str, int] | None] \u2014 A callback used to extract the counter from a job. Used as various cloud back-ends return different formats.</p> </li> <li> <p>path :  Path | None \u2014 If provided, a path at which to save the results once they're available.</p> </li> </ul> <p> source property PasqalCloudExtracted.processed_data: list[ProcessedData] </p> <p> source property PasqalCloudExtracted.raw_data: list[BaseGraph] </p> <p> source property PasqalCloudExtracted.targets: list[int] | None </p> <p> source property PasqalCloudExtracted.sequences: list[pl.Sequence] </p> <p> source property PasqalCloudExtracted.states: list[dict[str, int]] </p> <p> source class BaseRemoteExtractor(compiler: BaseGraphCompiler[GraphType], project_id: str, username: str, device_name: str, password: str | None = None, batch_ids: list[str] | None = None, path: Path | None = None) </p> <p><p>Bases : BaseExtractor[GraphType], Generic[GraphType]</p></p> <p>An Extractor that uses a remote Quantum Device published on Pasqal Cloud, to run sequences compiled from graphs.</p> <p> Performance note (servers and interactive applications only) </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you may ignore this performance note.</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> <li> <p>batch_id \u2014 Use this to resume a workflow e.g. after turning off your computer while the QPU was executing your sequences. Warning: A batch started with one executor MUST NOT be resumed with a different executor.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run \u2014 Launch the extraction.</p> </li> </ul> <p> source property BaseRemoteExtractor.batch_ids: list[str] | None </p> <p> source method BaseRemoteExtractor.run() \u2192 PasqalCloudExtracted </p> <p>Launch the extraction.</p> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source class RemoteQPUExtractor(compiler: BaseGraphCompiler[GraphType], project_id: str, username: str, device_name: str = 'FRESNEL', password: str | None = None, batch_ids: list[str] | None = None, path: Path | None = None) </p> <p><p>Bases : BaseRemoteExtractor[GraphType]</p></p> <p>An Extractor that uses a remote QPU published on Pasqal Cloud, to run sequences compiled from graphs.</p> <p> Performance note </p> <p>as of this writing, the waiting lines for a QPU may be very long. You may use this Extractor to resume your workflow with a computation that has been previously started.</p> <p> Performance note (servers and interactive applications only) </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you may ignore this performance note.</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> <li> <p>batch_id \u2014 Use this to resume a workflow e.g. after turning off your computer while the QPU was executing your sequences.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source method RemoteQPUExtractor.run() \u2192 PasqalCloudExtracted </p> <p> source class RemoteEmuMPSExtractor(compiler: BaseGraphCompiler[GraphType], project_id: str, username: str, device_name: str = 'FRESNEL', password: str | None = None, batch_ids: list[str] | None = None, path: Path | None = None) </p> <p><p>Bases : BaseRemoteExtractor[GraphType]</p></p> <p>An Extractor that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud, to run sequences compiled from graphs.</p> <p> Performance note (servers and interactive applications only) </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you may ignore this performance note.</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> <li> <p>batch_id \u2014 Use this to resume a workflow e.g. after turning off your computer while the QPU was executing your sequences.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source method RemoteEmuMPSExtractor.run(dt: int = 10) \u2192 PasqalCloudExtracted </p>"},{"location":"src/qek/data/extractors/","title":"qek.data.extractors","text":"qek.data.extractors<p> docs module qek.data.extractors </p> <pre><code>\"\"\"\nHigh-Level API to compile raw data (graphs) and process it on a quantum device, either a local emulator,\na remote emulator or a physical QPI.\n\"\"\"\n\nimport abc\nimport asyncio\nfrom dataclasses import dataclass\nimport itertools\nimport json\nimport logging\nfrom math import ceil\nfrom uuid import UUID\nimport time\nfrom typing import Any, Callable, Generator, Generic, Sequence, TypeVar, cast\nfrom numpy.typing import NDArray\nfrom pasqal_cloud import SDK\nfrom pasqal_cloud.batch import Batch\nfrom pasqal_cloud.device import BaseConfig, EmuTNConfig, EmulatorType\nfrom pasqal_cloud.job import Job\nfrom pasqal_cloud.utils.filters import BatchFilters\nfrom pathlib import Path\nimport numpy as np\nimport os\nimport pulser as pl\nfrom pulser.devices import Device\nfrom pulser.json.abstract_repr.deserializer import deserialize_device\nfrom pulser_simulation import QutipEmulator\nfrom torch.utils.data import Dataset\n\nfrom qek.data import processed_data\nfrom qek.data.graphs import BaseGraph, BaseGraphCompiler\nfrom qek.data.processed_data import ProcessedData\nfrom qek.shared.error import CompilationError\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Compiled:docs\n    \"\"\"\n    The result of compiling a graph for execution on a quantum device.\n    \"\"\"\n\n    # Future plans: as of this writing, this class (or a reworked version of it)\n    # is expected to move to the `qool-layer` library.\n\n    # The graph itself.\n    graph: BaseGraph\n\n    # A sequence adapted to the quantum device.\n    sequence: pl.Sequence\n\n\n@dataclass\nclass Feature:docs\n    \"\"\"\n    A feature extracted from raw data.\n    \"\"\"\n\n    data: NDArray[np.floating]\n\n\nclass BaseExtracted(abc.ABC):docs\n    \"\"\"\n    Data extracted by one of the subclasses of `BaseExtractor`.\n\n    Note that the list of processed data will generally *not* contain all the graphs ingested\n    by the Extractor, as not all graphs may not be compiled for a given device.\n    \"\"\"\n\n    def __init__(self, device: Device):\n        self.device = device\n\n    def __await__(self) -&gt; Generator[Any, Any, None]:\n        \"\"\"\n        Wait asynchronously until execution is ready.\n\n        This will avoid blocking your main thread, so calling this method once,\n        before the first call to `processed_data`, is strongly recommended\n        for use on a server or an interactive application.\n        \"\"\"\n        # By default, no need to wait.\n        yield None\n\n    @property\n    @abc.abstractmethod\n    def processed_data(self) -&gt; list[processed_data.ProcessedData]:docs\n        pass\n\n    @property\n    @abc.abstractmethod\n    def raw_data(self) -&gt; list[BaseGraph]:docs\n        \"\"\"\n        A subset of the graphs ingested by the Extractor.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def targets(self) -&gt; list[int] | None:docs\n        \"\"\"\n        If available, the machine-learning targets for these graphs, in the same order and with the same number of entrie as `raw_data`.\n        \"\"\"\n        pass\n\n    @property\n    def states(self) -&gt; list[dict[str, int]]:docs\n        \"\"\"\n        The quantum states extracted from `raw_data` by executing `sequences` on the device, in the same order and with the same number of entries as `raw_data`.\n        \"\"\"\n        return [data.state_dict for data in self.processed_data]\n\n    def features(self, size_max: int | None) -&gt; list[Feature]:docs\n        \"\"\"\n        The features extracted from `raw_data` by processing `states`, in the same order and with the same number of entries as `raw_data`.\n\n        By default, the features extracted are the distribution of excitation levels based on `states`. However, subclasses may override\n        this method to provide custom features extraction.\n\n        Arguments:\n            size_max (optional) Performance/precision lever. If specified, specifies the number of qubits to take into account from all\n                the `states`. If `size_max` is lower than the number of qubits used to extract `self.states[i]` (i.e. the number of qubits\n                in `self.sequences[i]`), then only take into account the `size_max` first qubits of this state to extract\n                `self.features(size_max)[i]`. If, on the other hand, `size_max` is greater than the number of qubits used to extract\n                `self.states[i]`, pad `self.features(size_max)[i]` with 0s.\n                If unspecified, use the largest number of qubits in `selfsequences`.\n        \"\"\"\n        if size_max is None:\n            for data in self.processed_data:\n                seq = data._sequence\n                if size_max is None or len(seq.qubit_info) &gt; size_max:\n                    size_max = len(seq.qubit_info)\n        if size_max is None:\n            # The only way size_max can be None is if `self.sequences` is empty.\n            return []\n\n        return [Feature(processed_data.dist_excitation(state, size_max)) for state in self.states]\n\n    def save_dataset(self, file_path: Path) -&gt; None:docs\n        \"\"\"Saves the processed dataset to a JSON file.\n\n        Note: This does NOT attempt to save the graphs.\n\n        Args:\n            dataset: The dataset to be saved.\n            file_path: The path where the dataset will be saved as a JSON\n                file.\n\n        Note:\n            The data is stored in a format suitable for loading with load_dataset.\n        \"\"\"\n        with open(file_path, \"w\") as file:\n            states = self.states\n            targets = self.targets\n            data = [\n                {\n                    \"sequence\": self.processed_data[i]._sequence.to_abstract_repr(),\n                    # Some emulators will actually be `dict[str, int64]` instead of `dict[str, int]` and `int64`\n                    # is not JSON-serializable.\n                    #\n                    # The reason for which `int64` is not JSON-serializable is that JSON limits ints to 2^53-1.\n                    # However, in practice, this should not be a problem, since the `int`/`int64` in our dict is\n                    # limited to the number of runs, and we don't expect to be launching 2^53 consecutive runs\n                    # for a single sequence on a device in any foreseeable future (assuming a run of 1ns,\n                    # this would still take ~4 billion years to execute).\n                    \"state_dict\": {key: int(value) for (key, value) in states[i].items()},\n                    \"target\": targets[i] if targets is not None else None,\n                }\n                for i in range(len(self.processed_data))\n            ]\n            json.dump(data, file)\n        logger.info(\"processed data saved to %s\", file_path)\n\n\nclass SyncExtracted(BaseExtracted):docs\n    \"\"\"\n    Data extracted synchronously, i.e. no need to wait for a remote server.\n    \"\"\"\n\n    def __init__(\n        self,\n        raw_data: list[BaseGraph],\n        targets: list[int] | None,\n        sequences: list[pl.Sequence],\n        states: list[dict[str, int]],\n    ):\n        assert len(raw_data) == len(sequences)\n        assert len(sequences) == len(states)\n        if targets is not None:\n            if len(targets) &lt; len(sequences):\n                # Not all graphs come with a target.\n                #\n                # This Extracted will not be usable as the training sample, so ignore all targets.\n                if len(targets) != 0:\n                    logger.debug(\n                        \"We compiled %s graphs but we only have %s targets, ignoring all targets\",\n                        len(sequences),\n                        len(targets),\n                    )\n                targets = None\n        self._raw_data = raw_data\n        self._targets = targets\n        self._sequences = sequences\n        self._states = states\n        self._processed_data = [\n            ProcessedData(\n                sequence=seq, state_dict=cast(dict[str, int | np.int64], state), target=target\n            )\n            for (seq, state, target) in itertools.zip_longest(sequences, states, targets or [])\n        ]\n\n    @property\n    def processed_data(self) -&gt; list[ProcessedData]:docs\n        return self._processed_data\n\n    @property\n    def raw_data(self) -&gt; list[BaseGraph]:docs\n        return self._raw_data\n\n    @property\n    def targets(self) -&gt; list[int] | None:docs\n        return self._targets\n\n    @property\n    def sequences(self) -&gt; list[pl.Sequence]:docs\n        return self._sequences\n\n    @property\n    def states(self) -&gt; list[dict[str, int]]:docs\n        return self._states\n\n\n# Type variable for BaseExtractor[GraphType].\nGraphType = TypeVar(\"GraphType\")\n\n\nclass BaseExtractor(abc.ABC, Generic[GraphType]):docs\n    \"\"\"\n    The base of the hierarchy of extractors.\n\n    The role of extractors is to take a list of raw data (here, labelled graphs) into\n    processed data containing machine-learning features (here, excitation vectors).\n\n    Args:\n        path: If specified, the processed data will be saved to this file as JSON once\n            the execution is complete.\n        device: A quantum device for which the data should be prepared.\n        compiler: A graph compiler, in charge of converting graphs to Pulser Sequences,\n            the format that can be executed on a quantum device.\n    \"\"\"\n\n    def __init__(\n        self, device: Device, compiler: BaseGraphCompiler[GraphType], path: Path | None = None\n    ) -&gt; None:\n        self.path = path\n\n        # The list of graphs (raw data). Fill it with `self.add_graphs`.\n        self.graphs: list[BaseGraph] = []\n        self.device: Device = device\n\n        # The compiled sequences. Filled with `self.compile`.\n        # Note that the list of compiled sequences may be shorter than the list of\n        # raw data, as not all graphs may be compiled to a given `device`.\n        self.sequences: list[Compiled] = []\n        self.compiler = compiler\n\n        # A counter used to give a unique id to each graph.\n        self._counter = 0\n\n    def save(self, snapshot: list[ProcessedData]) -&gt; None:docs\n        \"\"\"Saves a dataset to a JSON file.\n\n        Args:\n            dataset (list[ProcessedData]): The dataset to be saved, containing\n                RegisterData instances.\n            file_path (str): The path where the dataset will be saved as a JSON\n                file.\n\n        Note:\n            The data is stored in a format suitable for loading with load_dataset.\n        \"\"\"\n        if self.path is not None:\n            with open(self.path, \"w\") as file:\n                data = [\n                    {\n                        \"sequence\": instance._sequence.to_abstract_repr(),\n                        \"state_dict\": instance.state_dict,\n                        \"target\": instance.target,\n                    }\n                    for instance in snapshot\n                ]\n                json.dump(data, file)\n            logger.info(\"processed data saved to %s\", self.path)\n\n    def compile(docs\n        self, filter: Callable[[BaseGraph, pl.Sequence, int], bool] | None = None\n    ) -&gt; list[Compiled]:\n        \"\"\"\n        Compile all pending graphs into Pulser sequences that the Quantum Device may execute.\n\n        Once this method have succeeded, the results are stored in `self.sequences`.\n        \"\"\"\n        if len(self.graphs) == 0:\n            raise Exception(\"No graphs to compile, did you forget to call `import_graphs`?\")\n        if filter is None:\n            filter = lambda _graph, sequence, _index: True  # noqa: E731\n        self.sequences = []\n        for graph in self.graphs:\n            try:\n                register = graph.compile_register()\n                pulse = graph.compile_pulse()\n                sequence = pl.Sequence(register=register, device=graph.device)\n                sequence.declare_channel(\"ising\", \"rydberg_global\")\n                sequence.add(pulse, \"ising\")\n            except CompilationError as e:\n                # In some cases, we produce graphs that pass `is_embeddable` but cannot be compiled.\n                # It _looks_ like this is due to rounding errors. We're investigating this in issue #29,\n                # but for the time being, we're simply logging and skipping them.\n                logger.debug(\"Graph #%s could not be compiled (%s), skipping\", graph.id, e)\n                continue\n            if not filter(graph, sequence, graph.id):\n                logger.debug(\"Graph #%s did not pass filtering, skipping\", graph.id)\n                continue\n            logger.debug(\"Compiling graph #%s for execution on the device\", graph.id)\n            self.sequences.append(Compiled(graph=graph, sequence=sequence))\n        logger.debug(\"Compilation step complete, %s graphs compiled\", len(self.sequences))\n        return self.sequences\ndocs\n    def add_graphs(self, graphs: Sequence[GraphType] | Dataset[GraphType]) -&gt; None:\n        \"\"\"\n        Add new graphs to compile and run.\n        \"\"\"\n        for graph in graphs:\n            self._counter += 1\n            id = self._counter\n            logger.debug(\"ingesting # %s\", id)\n            processed = self.compiler.ingest(graph=graph, device=self.device, id=id)\n            # Skip graphs that are not embeddable.\n            if processed.is_embeddable():\n                logger.debug(\"graph # %s is embeddable, accepting\", id)\n                self.graphs.append(processed)\n            else:\n                logger.info(\"graph # %s is not embeddable, skipping\", id)\n        logger.info(\"imported %s graphs\", len(self.graphs))\n\n    @abc.abstractmethod\n    def run(self) -&gt; BaseExtracted:docs\n        \"\"\"\n        Run compiled graphs.\n\n        You will need to call `self.compile` first, to make sure that the graphs are compiled.\n\n        Returns:\n            Data extracted by this extractor.\n\n            Not all extractors may return the same data, so please take a look at the documentation\n            of the extractor you are using.\n        \"\"\"\n        raise Exception(\"Not implemented\")\n\n\nclass QutipExtractor(BaseExtractor[GraphType]):docs\n    \"\"\"\n    A Extractor that uses the Qutip Emulator to run sequences compiled\n    from graphs.\n\n    Performance note: emulating a quantum device on a classical\n    computer requires considerable amount of resources, so this\n    Extractor may be slow or require too much memory.\n\n    See also:\n    - EmuMPSExtractor (alternative emulator, generally much faster)\n    - QPUExtractor (run on a physical QPU)\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        compiler: A graph compiler, in charge of converting graphs to Pulser Sequences,\n            the format that can be executed on a quantum device.\n        device: A device to use. For general experiments, the default\n            device `AnalogDevice` is a perfectly reasonable choice.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        device: Device = pl.devices.AnalogDevice,\n        path: Path | None = None,\n    ):\n        super().__init__(path=path, device=device, compiler=compiler)\n        self.graphs: list[BaseGraph]\n        self.device = device\n\n    def run(self, max_qubits: int = 8) -&gt; SyncExtracted:docs\n        \"\"\"\n        Run the compiled graphs.\n\n        As emulating a quantum device is slow consumes resources and time exponential in the\n        number of qubits, for the sake of performance, we limit the number of qubits in the execution\n        of this extractor.\n\n        Args:\n            max_qubits: Skip any sequence that require strictly more than `max_qubits`. Defaults to 8.\n\n        Returns:\n            Processed data for all the sequences that were executed.\n        \"\"\"\n        if len(self.sequences) == 0:\n            logger.warning(\"No sequences to run, did you forget to call compile()?\")\n            return SyncExtracted(raw_data=[], targets=[], sequences=[], states=[])\n\n        raw_data: list[BaseGraph] = []\n        targets: list[int] = []\n        sequences: list[pl.Sequence] = []\n        states: list[dict[str, int]] = []\n        for compiled in self.sequences:\n            qubits_used = len(compiled.sequence.qubit_info)\n            if qubits_used &gt; max_qubits:\n                logger.info(\n                    \"Graph %s exceeds the qubit limit specified in QutipExtractor (%s &gt; %s), skipping\",\n                    id,\n                    qubits_used,\n                    max_qubits,\n                )\n                continue\n            logger.debug(\"Executing compiled graph # %s\", id)\n            simul = QutipEmulator.from_sequence(sequence=compiled.sequence)\n            counter = cast(dict[str, Any], simul.run().sample_final_state())\n            logger.debug(\"Execution of compiled graph # %s complete\", id)\n            raw_data.append(compiled.graph)\n            if compiled.graph.target is not None:\n                targets.append(compiled.graph.target)\n            sequences.append(compiled.sequence)\n            states.append(counter)\n\n        result = SyncExtracted(\n            raw_data=raw_data, targets=targets, sequences=sequences, states=states\n        )\n        logger.debug(\"Emulation step complete, %s compiled graphs executed\", len(raw_data))\n        if self.path is not None:\n            result.save_dataset(self.path)\n        return result\n\n\nif os.name == \"posix\":\n    # Any Unix including Linux and macOS\n\n    import emu_mps\n\n    class EmuMPSExtractor(BaseExtractor[GraphType]):docs\n        \"\"\"\n        A Extractor that uses the emu-mps Emulator to run sequences compiled\n        from graphs.\n\n        Performance note: emulating a quantum device on a classical\n        computer requires considerable amount of resources, so this\n        Extractor may be slow or require too much memory. If should,\n        however, be faster than QutipExtractor in most cases.\n\n        See also:\n        - QPUExtractor (run on a physical QPU)\n\n        Args:\n            path: Path to store the result of the run, for future uses.\n                To reload the result of a previous run, use `LoadExtractor`.\n            compiler: A graph compiler, in charge of converting graphs to Pulser Sequences,\n                the format that can be executed on a quantum device.\n            device: A device to use. For general experiments, the default\n                device `AnalogDevice` is a perfectly reasonable choice.\n        \"\"\"\n\n        def __init__(\n            self,\n            compiler: BaseGraphCompiler[GraphType],\n            device: Device = pl.devices.AnalogDevice,\n            path: Path | None = None,\n        ):\n            super().__init__(device=device, compiler=compiler, path=path)\n            self.graphs: list[BaseGraph]\n            self.device = device\ndocs\n        def run(self, max_qubits: int = 10, dt: int = 10) -&gt; BaseExtracted:\n            \"\"\"\n            Run the compiled graphs.\n\n            As emulating a quantum device is slow consumes resources and time exponential in the\n            number of qubits, for the sake of performance, we limit the number of qubits in the execution\n            of this extractor.\n\n            Args:\n                max_qubits: Skip any sequence that require strictly more than `max_qubits`. Defaults to 8.\n                dt: The duration of the simulation step, in us. Defaults to 10.\n\n            Returns:\n                Processed data for all the sequences that were executed.\n            \"\"\"\n            if len(self.sequences) == 0:\n                logger.warning(\"No sequences to run, did you forget to call compile()?\")\n                return SyncExtracted(raw_data=[], targets=[], sequences=[], states=[])\n\n            backend = emu_mps.MPSBackend()\n            raw_data = []\n            targets: list[int] = []\n            sequences = []\n            states = []\n            for compiled in self.sequences:\n                qubits_used = len(compiled.sequence.qubit_info)\n                if qubits_used &gt; max_qubits:\n                    logger.info(\n                        \"Graph %s exceeds the qubit limit specified in EmuMPSExtractor (%s &gt; %s), skipping\",\n                        id,\n                        qubits_used,\n                        max_qubits,\n                    )\n                    continue\n                logger.debug(\"Executing compiled graph # %s\", id)\n\n                # Configure observable.\n                cutoff_duration = int(ceil(compiled.sequence.get_duration() / dt) * dt)\n                observable = emu_mps.BitStrings(evaluation_times={cutoff_duration})\n                config = emu_mps.MPSConfig(observables=[observable], dt=dt)\n                counter: dict[str, Any] = backend.run(compiled.sequence, config)[observable.name][\n                    cutoff_duration\n                ]\n                logger.debug(\"Execution of compiled graph # %s complete\", id)\n                raw_data.append(compiled.graph)\n                if compiled.graph.target is not None:\n                    targets.append(compiled.graph.target)\n                sequences.append(compiled.sequence)\n                states.append(counter)\n\n            logger.debug(\"Emulation step complete, %s compiled graphs executed\", len(raw_data))\n\n            result = SyncExtracted(\n                raw_data=raw_data, targets=targets, sequences=sequences, states=states\n            )\n            logger.debug(\"Emulation step complete, %s compiled graphs executed\", len(raw_data))\n            if self.path is not None:\n                result.save_dataset(self.path)\n            return result\n\n\n# How many seconds to sleep while waiting for the results from the cloud.\nSLEEP_DELAY_S = 2\n\n\nclass PasqalCloudExtracted(BaseExtracted):docs\n    \"\"\"\n    Data extracted from the cloud API, i.e. we need wait for a remote server.\n\n    Performance note:\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        do not need to do so.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiled: list[Compiled],\n        batch_ids: list[str],\n        sdk: SDK,\n        state_extractor: Callable[[Job, pl.Sequence], dict[str, int] | None],\n        path: Path | None = None,\n    ):\n        \"\"\"\n        Prepare for reception of data.\n\n        Arguments:\n            compiled: The result of compiling a set of graphs.\n            batch_ids: The ids of the batches on the cloud API, in the same order as `compiled`.\n            state_extractor: A callback used to extract the counter from a job.\n                Used as various cloud back-ends return different formats.\n            path: If provided, a path at which to save the results once they're available.\n        \"\"\"\n        self._compiled = compiled\n        self._batch_ids = batch_ids\n        self._results: SyncExtracted | None = None\n        self._path = path\n        self._sdk = sdk\n        self._state_extractor = state_extractor\n\n    def _wait(self) -&gt; None:\n        \"\"\"\n        Wait synchronously until remote execution is ready.\n\n        This WILL BLOCK your main thread, possibly for a very long time.\n        \"\"\"\n        if self._results is not None:\n            # Results are already available.\n            return\n        pending_batch_ids: set[str] = set(self._batch_ids)\n        completed_batches: dict[str, Batch] = {}\n        while len(pending_batch_ids) &gt; 0:\n            time.sleep(SLEEP_DELAY_S)\n\n            # Fetch up to 100 pending batches (upstream limits).\n            MAX_BATCH_LEN = 100\n            check_ids: list[str | UUID] = [cast(str | UUID, id) for id in pending_batch_ids][\n                :MAX_BATCH_LEN\n            ]\n\n            # Update their status.\n            check_batches = self._sdk.get_batches(filters=BatchFilters(id=check_ids))\n            for batch in check_batches.results:\n                assert isinstance(batch, Batch)\n                if batch.status not in {\"PENDING\", \"RUNNING\"}:\n                    logger.debug(\"Job %s is now complete\", batch.id)\n                    pending_batch_ids.discard(batch.id)\n                    completed_batches[batch.id] = batch\n\n        # At this point, all batches are complete.\n        self._ingest(completed_batches)\n\n    def __await__(self) -&gt; Generator[Any, Any, None]:\n        \"\"\"\n        Wait asynchronously until remote execution is ready.\n\n        This will NOT block your main thread, so this method is strongly recommended\n        for use on a server or an interactive application.\n\n        Example:\n            await extracted\n        \"\"\"\n        if self._results is not None:\n            # Results are already available.\n            return\n        pending_batch_ids: set[str] = set(self._batch_ids)\n        completed_batches: dict[str, Batch] = {}\n        while len(pending_batch_ids) &gt; 0:\n            yield from asyncio.sleep(SLEEP_DELAY_S).__await__()\n\n            # Fetch up to 100 pending batches (upstream limits).\n            MAX_BATCH_LEN = 100\n            check_ids: list[str | UUID] = [cast(str | UUID, id) for id in pending_batch_ids][\n                :MAX_BATCH_LEN\n            ]\n\n            # Update their status.\n            check_batches = self._sdk.get_batches(\n                filters=BatchFilters(id=check_ids)\n            )  # Ideally, this should be async, see https://github.com/pasqal-io/pasqal-cloud/issues/162.\n            for batch in check_batches.results:\n                assert isinstance(batch, Batch)\n                if batch.status not in {\"PENDING\", \"RUNNING\"}:\n                    logger.debug(\"Job %s is now complete\", batch.id)\n                    pending_batch_ids.discard(batch.id)\n                    completed_batches[batch.id] = batch\n\n        # At this point, all batches are complete.\n        self._ingest(completed_batches)\n\n    def _ingest(self, batches: dict[str, Batch]) -&gt; None:\n        \"\"\"\n        Ingest data received from the remote server.\n\n        No I/O.\n        \"\"\"\n        assert len(batches) == len(self._batch_ids)\n\n        raw_data = []\n        targets: list[int] = []\n        sequences = []\n        states = []\n        for i, id in enumerate(self._batch_ids):\n            batch = batches[id]\n            compiled = self._compiled[i]\n            # Note: There's only one job per batch.\n            assert len(batch.jobs) == 1\n            for job in batch.jobs.values():\n                if job.status == \"DONE\":\n                    state_dict = self._state_extractor(job, compiled.sequence)\n                    if state_dict is None:\n                        logger.warning(\n                            \"Batch %s (graph %s) did not return a usable state, skipping\",\n                            i,\n                            compiled.graph.id,\n                        )\n                        continue\n                    raw_data.append(compiled.graph)\n                    if compiled.graph.target is not None:\n                        targets.append(compiled.graph.target)\n                    sequences.append(compiled.sequence)\n                    states.append(state_dict)\n                else:\n                    # If some sequences failed, let's skip them and proceed as well as we can.\n                    logger.warning(\n                        \"Batch %s (graph %s) failed with errors %s, skipping\",\n                        i,\n                        compiled.graph.id,\n                        job.status,\n                        job.errors,\n                    )\n        self._results = SyncExtracted(\n            raw_data=raw_data, targets=targets, sequences=sequences, states=states\n        )\n        if self._path is not None:\n            self.save_dataset(self._path)\n\n    @property\n    def processed_data(self) -&gt; list[ProcessedData]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.processed_data\n\n    @property\n    def raw_data(self) -&gt; list[BaseGraph]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.raw_data\n\n    @property\n    def targets(self) -&gt; list[int] | None:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.targets\n\n    @property\n    def sequences(self) -&gt; list[pl.Sequence]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.sequences\n\n    @property\n    def states(self) -&gt; list[dict[str, int]]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.states\n\n\nclass BaseRemoteExtractor(BaseExtractor[GraphType], Generic[GraphType]):docs\n    \"\"\"\n    An Extractor that uses a remote Quantum Device published\n    on Pasqal Cloud, to run sequences compiled from graphs.\n\n    Performance note (servers and interactive applications only):\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        may ignore this performance note.\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        project_id: The ID of the project on the Pasqal Cloud API.\n        username: Your username on the Pasqal Cloud API.\n        password: Your password on the Pasqal Cloud API. If you leave\n            this to None, you will need to enter your password manually.\n        device_name: The name of the device to use. As of this writing,\n            the default value of \"FRESNEL\" represents the latest QPU\n            available through the Pasqal Cloud API.\n        batch_id: Use this to resume a workflow e.g. after turning off\n            your computer while the QPU was executing your sequences.\n            Warning: A batch started with one executor MUST NOT be resumed\n            with a different executor.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        project_id: str,\n        username: str,\n        device_name: str,\n        password: str | None = None,\n        batch_ids: list[str] | None = None,\n        path: Path | None = None,\n    ):\n        sdk = SDK(username=username, project_id=project_id, password=password)\n\n        # Fetch the latest list of QPUs\n        specs = sdk.get_device_specs_dict()\n        device = cast(Device, deserialize_device(specs[device_name]))\n\n        super().__init__(device=device, compiler=compiler, path=path)\n        self._sdk = sdk\n        self._batch_ids: list[str] | None = batch_ids\n\n    @property\n    def batch_ids(self) -&gt; list[str] | None:docs\n        return self._batch_ids\n\n    @abc.abstractmethod\n    def run(docs\n        self,\n    ) -&gt; PasqalCloudExtracted:\n        \"\"\"\n        Launch the extraction.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _run(\n        self,\n        state_extractor: Callable[[Job, pl.Sequence], dict[str, int] | None],\n        emulator: EmulatorType | None,\n        config: BaseConfig | None,\n    ) -&gt; PasqalCloudExtracted:\n        if len(self.sequences) == 0:\n            logger.warning(\"No sequences to run, did you forget to call compile()?\")\n            return PasqalCloudExtracted(\n                compiled=[],\n                batch_ids=[],\n                sdk=self._sdk,\n                path=self.path,\n                state_extractor=state_extractor,\n            )\n\n        device: pl.devices.Device = self.sequences[0].sequence.device\n        # As of this writing, the API doesn't support runs longer than 500 jobs.\n        # If we want to add more runs, we'll need to split them across several jobs.\n        max_runs = device.max_runs if isinstance(device.max_runs, int) else 500\n\n        if self._batch_ids is None:\n            # Enqueue jobs.\n            self._batch_ids = []\n            for compiled in self.sequences:\n                logger.debug(\"Enqueuing execution of compiled graph #%s\", compiled.graph.id)\n                batch = self._sdk.create_batch(\n                    compiled.sequence.to_abstract_repr(),\n                    jobs=[{\"runs\": max_runs}],\n                    wait=False,\n                    emulator=emulator,\n                    configuration=config,\n                )\n                logger.info(\n                    \"Remote execution of compiled graph #%s starting, batched with id %s\",\n                    compiled.graph.id,\n                    batch.id,\n                )\n                self._batch_ids.append(batch.id)\n            logger.info(\n                \"All %s jobs enqueued for remote execution, with ids %s\",\n                len(self._batch_ids),\n                self._batch_ids,\n            )\n        assert len(self._batch_ids) == len(self.sequences)\n\n        return PasqalCloudExtracted(\n            compiled=self.sequences,\n            batch_ids=self._batch_ids,\n            sdk=self._sdk,\n            path=self.path,\n            state_extractor=state_extractor,\n        )\n\n\nclass RemoteQPUExtractor(BaseRemoteExtractor[GraphType]):docs\n    \"\"\"\n    An Extractor that uses a remote QPU published\n    on Pasqal Cloud, to run sequences compiled from graphs.\n\n    Performance note:\n        as of this writing, the waiting lines for a QPU\n        may be very long. You may use this Extractor to resume your workflow\n        with a computation that has been previously started.\n\n    Performance note (servers and interactive applications only):\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        may ignore this performance note.\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        project_id: The ID of the project on the Pasqal Cloud API.\n        username: Your username on the Pasqal Cloud API.\n        password: Your password on the Pasqal Cloud API. If you leave\n            this to None, you will need to enter your password manually.\n        device_name: The name of the device to use. As of this writing,\n            the default value of \"FRESNEL\" represents the latest QPU\n            available through the Pasqal Cloud API.\n        batch_id: Use this to resume a workflow e.g. after turning off\n            your computer while the QPU was executing your sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        project_id: str,\n        username: str,\n        device_name: str = \"FRESNEL\",\n        password: str | None = None,\n        batch_ids: list[str] | None = None,\n        path: Path | None = None,\n    ):\n        super().__init__(\n            compiler=compiler,\n            project_id=project_id,\n            username=username,\n            device_name=device_name,\n            password=password,\n            batch_ids=batch_ids,\n            path=path,\n        )\n\n    def run(self) -&gt; PasqalCloudExtracted:docs\n        return self._run(emulator=None, config=None, state_extractor=lambda job, _seq: job.result)\n\n\nclass RemoteEmuMPSExtractor(BaseRemoteExtractor[GraphType]):docs\n    \"\"\"\n    An Extractor that uses a remote high-performance emulator (EmuMPS)\n    published on Pasqal Cloud, to run sequences compiled from graphs.\n\n    Performance note (servers and interactive applications only):\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        may ignore this performance note.\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        project_id: The ID of the project on the Pasqal Cloud API.\n        username: Your username on the Pasqal Cloud API.\n        password: Your password on the Pasqal Cloud API. If you leave\n            this to None, you will need to enter your password manually.\n        device_name: The name of the device to use. As of this writing,\n            the default value of \"FRESNEL\" represents the latest QPU\n            available through the Pasqal Cloud API.\n        batch_id: Use this to resume a workflow e.g. after turning off\n            your computer while the QPU was executing your sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        project_id: str,\n        username: str,\n        device_name: str = \"FRESNEL\",\n        password: str | None = None,\n        batch_ids: list[str] | None = None,\n        path: Path | None = None,\n    ):\n        super().__init__(\n            compiler=compiler,\n            project_id=project_id,\n            username=username,\n            device_name=device_name,\n            password=password,\n            batch_ids=batch_ids,\n            path=path,\n        )\n\n    def run(self, dt: int = 10) -&gt; PasqalCloudExtracted:docs\n        def extractor(job: Job, sequence: pl.Sequence) -&gt; dict[str, int] | None:\n            cutoff_duration = int(ceil(sequence.get_duration() / dt) * dt)\n            full_result = job.full_result\n            if full_result is None:\n                return None\n            result = full_result[\"bitstring\"][cutoff_duration]\n            if result is None:\n                return None\n            assert isinstance(result, dict)\n            return result\n\n        return self._run(\n            emulator=EmulatorType.EMU_MPS,\n            config=EmuTNConfig(\n                dt=dt,\n            ),\n            state_extractor=extractor,\n        )\n</code></pre>"},{"location":"api/qek/data/graphs/","title":"qek.data.graphs","text":"qek.data.graphs<p> source module qek.data.graphs </p> <p>Loading graphs as raw data.</p> <p> Attributes </p> <ul> <li> <p>EPSILON_RADIUS_UM \u2014 Assumption of rounding error when determining whether a graph is a disk graph.</p> </li> <li> <p>EPSILON_RESCALE_FACTOR \u2014 A correction factor, attempting to cover for rounding error when rescaling a graph.</p> </li> </ul> <p> Classes </p> <ul> <li> <p>BaseGraph \u2014 A graph being prepared for embedding on a quantum device.</p> </li> <li> <p>MoleculeGraph \u2014 A graph based on molecular data, being prepared for embedding on a quantum device.</p> </li> <li> <p>PTCFMGraph \u2014 An ingester for molecule graphs using PTC-FM dataset conventions.</p> </li> <li> <p>BaseGraphCompiler \u2014 Abstract class, used to load a graph and compile a Pulser sequence for a device.</p> </li> <li> <p>PygWithPosCompiler \u2014 A compiler able to ingest torch_geometric graphs with positions.</p> </li> <li> <p>MoleculeGraphCompiler \u2014 A compiler able to ingest torch_geometric molecules with a target.</p> </li> <li> <p>PTCFMCompiler</p> </li> <li> <p>NXWithPos \u2014 A networkx graph and its position</p> </li> <li> <p>NXGraphCompiler</p> </li> </ul> <p> source class BaseGraph(id: int, data: pyg_data.Data, device: pl.devices.Device, target: int | None = None) </p> <p>A graph being prepared for embedding on a quantum device.</p> <p>Create a graph from geometric data.</p> <p> Parameters </p> <ul> <li> <p>id :  int \u2014 An identifier for this graph, used mostly for error messages.</p> </li> <li> <p>data :  pyg_data.Data \u2014 A homogeneous graph, in PyTorch Geometric format. Unchanged. It MUST have attributes 'pos'.</p> </li> <li> <p>device :  pl.devices.Device \u2014 The device for which the graph is prepared.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>is_disk_graph \u2014 A predicate to check if <code>self</code> is a disk graph with the specified radius, i.e. <code>self</code> is a connected graph and, for every pair of nodes <code>A</code> and <code>B</code> within <code>graph</code>, there exists an edge between <code>A</code> and <code>B</code> if and only if the positions of <code>A</code> and <code>B</code> within <code>self</code> are such that <code>|AB| &lt;= radius</code>.</p> </li> <li> <p>is_embeddable \u2014 A predicate to check if the graph can be embedded in the     quantum device.</p> </li> <li> <p>compile_register \u2014 Create a Quantum Register based on a graph.</p> </li> <li> <p>compile_pulse \u2014 Extract a Pulse for this graph.</p> </li> </ul> <p> source method BaseGraph.is_disk_graph(radius: float) \u2192 bool </p> <p>A predicate to check if <code>self</code> is a disk graph with the specified radius, i.e. <code>self</code> is a connected graph and, for every pair of nodes <code>A</code> and <code>B</code> within <code>graph</code>, there exists an edge between <code>A</code> and <code>B</code> if and only if the positions of <code>A</code> and <code>B</code> within <code>self</code> are such that <code>|AB| &lt;= radius</code>.</p> <p> Parameters </p> <ul> <li> <p>radius :  float \u2014 The maximal distance between two nodes of <code>self</code> connected be an edge.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 <code>True</code> if the graph is a disk graph with the specified radius, <code>False</code> otherwise.</p> </li> </ul> <p> source method BaseGraph.is_embeddable() \u2192 bool </p> <p>A predicate to check if the graph can be embedded in the     quantum device.</p> <p>For a graph to be embeddable on a device, all the following     criteria must be fulfilled:     - the graph must be non-empty;     - the device must have at least as many atoms as the graph has         nodes;     - the device must be physically large enough to place all the         nodes (device.max_radial_distance);     - the nodes must be distant enough that quantum interactions         may take place (device.min_atom_distance)</p> <p> Returns </p> <ul> <li> <p>bool \u2014 True if possible, False if not</p> </li> </ul> <p> source method BaseGraph.compile_register() \u2192 pl.Register </p> <p>Create a Quantum Register based on a graph.</p> <p> Returns </p> <ul> <li> <p>pulser.Register \u2014 register</p> </li> </ul> <p> Raises </p> <ul> <li> <p>CompilationError</p> </li> </ul> <p> source method BaseGraph.compile_pulse(amplitude: float = SEQUENCE_DEFAULT_AMPLITUDE_RAD_PER_US, duration: int = SEQUENCE_DEFAULT_DURATION_NS) \u2192 pl.Pulse </p> <p>Extract a Pulse for this graph.</p> <p>A Pulse represents the laser applied to the atoms on the device.</p> <p> Parameters </p> <ul> <li> <p>amplitude :  float \u2014 The amplitude for the laser pulse, in rad per microseconds. By default, use the value demonstrated in the companion paper.</p> </li> <li> <p>duration :  int \u2014 The duration of the laser pulse, in nanoseconds. By default, use the value demonstrated in the companion paper.</p> </li> </ul> <p> source class MoleculeGraph(id: Any, data: pyg_data.Data, device: pl.devices.Device, node_mapping: dict[int, str], edge_mapping: dict[int, Chem.BondType], target: int | None = None) </p> <p><p>Bases : BaseGraph</p></p> <p>A graph based on molecular data, being prepared for embedding on a quantum device.</p> <p>Compute the geometry for a molecule graph.</p> <p> Parameters </p> <ul> <li> <p>data :  pyg_data.Data \u2014 A homogeneous graph, in PyTorch Geometric format. Unchanged.</p> </li> <li> <p>blockade_radius \u2014 The radius of the Rydberg Blockade. Two connected nodes should be at a distance &lt; blockade_radius, while two disconnected nodes should be at a distance &gt; blockade_radius.</p> </li> <li> <p>node_mapping :  dict[int, str] \u2014 A mapping of node labels from numbers to strings, e.g. <code>5 =&gt; \"Cl\"</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>edge_mapping :  dict[int, Chem.BondType] \u2014 A mapping of edge labels from number to chemical bond types, e.g. <code>2 =&gt; DOUBLE</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>target :  int | None \u2014 If specified, a target for machine learning, as a value <code>0</code> or <code>1</code>.</p> </li> </ul> <p> source class PTCFMGraph(id: Any, data: pyg_data.Data, device: pl.devices.Device) </p> <p><p>Bases : MoleculeGraph</p></p> <p>An ingester for molecule graphs using PTC-FM dataset conventions.</p> <p>Compute the geometry for a molecule graph.</p> <p> Parameters </p> <ul> <li> <p>data :  pyg_data.Data \u2014 A homogeneous graph, in PyTorch Geometric format. Unchanged.</p> </li> <li> <p>blockade_radius \u2014 The radius of the Rydberg Blockade. Two connected nodes should be at a distance &lt; blockade_radius, while two disconnected nodes should be at a distance &gt; blockade_radius.</p> </li> <li> <p>node_mapping \u2014 A mapping of node labels from numbers to strings, e.g. <code>5 =&gt; \"Cl\"</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>edge_mapping \u2014 A mapping of edge labels from number to chemical bond types, e.g. <code>2 =&gt; DOUBLE</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>target \u2014 If specified, a target for machine learning, as a value <code>0</code> or <code>1</code>.</p> </li> </ul> <p> source class BaseGraphCompiler() </p> <p><p>Bases : abc.ABC, Generic[GraphType]</p></p> <p>Abstract class, used to load a graph and compile a Pulser sequence for a device.</p> <p>You should probably use one of the subclasses.</p> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method BaseGraphCompiler.ingest(graph: GraphType, device: pl.devices.Device, id: int) \u2192 BaseGraph </p> <p> Raises </p> <ul> <li> <p>Exception</p> </li> </ul> <p> source class PygWithPosCompiler() </p> <p><p>Bases : BaseGraphCompiler[pyg_data.Data]</p></p> <p>A compiler able to ingest torch_geometric graphs with positions.</p> <p> Methods </p> <ul> <li> <p>ingest \u2014 Compile a Pulser sequence from a torch_geometric graph with position.</p> </li> </ul> <p> source method PygWithPosCompiler.ingest(graph: pyg_data.Data, device: pl.devices.Device, id: int) \u2192 BaseGraph </p> <p>Compile a Pulser sequence from a torch_geometric graph with position.</p> <p> Parameters </p> <ul> <li> <p>graph :  pyg_data.Data \u2014 A graph with positions (specified as attribute <code>pos</code>) and optionally a prediction target (specified as attribute <code>y</code>, which must be an <code>int</code>). The graph will not be changed.</p> </li> <li> <p>device :  pl.devices.Device \u2014 The device for which the sequence must be compiled.</p> </li> <li> <p>id :  int \u2014 A unique identifier for the graph, used mostly for logging and displaying error messages.</p> </li> </ul> <p> source class MoleculeGraphCompiler(node_mapping: dict[int, str], edge_mapping: dict[int, Chem.BondType]) </p> <p><p>Bases : BaseGraphCompiler[tuple[pyg_data.Data, int | None]]</p></p> <p>A compiler able to ingest torch_geometric molecules with a target.</p> <p>Setup a molecule graph compiler.</p> <p> Parameters </p> <ul> <li> <p>node_mapping :  dict[int, str] \u2014 A mapping from node labels (as integers) to atom names (e.g. \"C\", \"Ar\", ...).</p> </li> <li> <p>edge_mapping :  dict[int, Chem.BondType] \u2014 A mapping from node labels (as integers) to chemical bond types (e.g. simple bound, double bound).</p> </li> </ul> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method MoleculeGraphCompiler.ingest(graph: tuple[pyg_data.Data, int | None], device: pl.devices.Device, id: int) \u2192 MoleculeGraph </p> <p> source class PTCFMCompiler() </p> <p><p>Bases : BaseGraphCompiler[pyg_data.Data]</p></p> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method PTCFMCompiler.ingest(graph: pyg_data.Data, device: pl.devices.Device, id: int) \u2192 PTCFMGraph </p> <p> source dataclass NXWithPos(graph: nx.Graph, positions: dict[Any, np.ndarray], target: int | None) </p> <p>A networkx graph and its position</p> <p> source class NXGraphCompiler() </p> <p><p>Bases : BaseGraphCompiler[NXWithPos]</p></p> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method NXGraphCompiler.ingest(graph: NXWithPos, device: pl.devices.Device, id: int) \u2192 BaseGraph </p>"},{"location":"src/qek/data/graphs/","title":"qek.data.graphs","text":"qek.data.graphs<p> docs module qek.data.graphs </p> <pre><code>\"\"\"\nLoading graphs as raw data.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nfrom dataclasses import dataclass\nimport logging\nfrom typing import Any, Final, Generic, TypeVar\n\nimport networkx as nx\nimport numpy as np\nimport pulser as pl\nimport rdkit.Chem as Chem\nimport torch\nimport torch_geometric.data as pyg_data\nimport torch_geometric.utils as pyg_utils\nfrom rdkit.Chem import AllChem\n\nfrom qek.shared.error import CompilationError\nfrom qek.shared._utils import graph_to_mol\n\nlogger = logging.getLogger(__name__)\n\nEPSILON_RADIUS_UM = 0.01\n\"\"\"\nAssumption of rounding error when determining whether a graph is a disk graph.\n\"\"\"\n\nEPSILON_RESCALE_FACTOR = 1.000000001\n\"\"\"\nA correction factor, attempting to cover for rounding error when rescaling a graph.\n\"\"\"\n\n\nclass BaseGraph:docs\n    \"\"\"\n    A graph being prepared for embedding on a quantum device.\n    \"\"\"\n\n    device: Final[pl.devices.Device]\n\n    def __init__(\n        self, id: int, data: pyg_data.Data, device: pl.devices.Device, target: int | None = None\n    ):\n        \"\"\"\n        Create a graph from geometric data.\n\n        Args:\n            id: An identifier for this graph, used mostly for error messages.\n            data:  A homogeneous graph, in PyTorch Geometric format. Unchanged.\n                It MUST have attributes 'pos'.\n            device: The device for which the graph is prepared.\n        \"\"\"\n        if not hasattr(data, \"pos\"):\n            raise AttributeError(\"The graph should have an attribute 'pos'.\")\n\n        # The device for which the graph is prepared.\n        self.device = device\n\n        # The graph in torch geometric format.\n        self.pyg = data.clone()\n\n        # The graph in networkx format, undirected.\n        self.nx_graph: nx.Graph = pyg_utils.to_networkx(\n            data=data,\n            node_attrs=[\"x\"],\n            edge_attrs=[\"edge_attr\"] if data.edge_attr is not None else None,\n            to_undirected=True,\n        )\n        self.target = target\n        self.id = id\n\n    def is_disk_graph(self, radius: float) -&gt; bool:docs\n        \"\"\"\n        A predicate to check if `self` is a disk graph with the specified\n        radius, i.e. `self` is a connected graph and, for every pair of nodes\n        `A` and `B` within `graph`, there exists an edge between `A` and `B`\n        if and only if the positions of `A` and `B` within `self` are such\n        that `|AB| &lt;= radius`.\n\n        Args:\n            radius: The maximal distance between two nodes of `self`\n                connected be an edge.\n\n        Returns:\n            `True` if the graph is a disk graph with the specified radius,\n            `False` otherwise.\n        \"\"\"\n\n        if self.pyg.num_nodes == 0 or self.pyg.num_nodes is None:\n            logger.debug(\"graph %s doesn't have any nodes, it's not a disk graph\", self.id, self.id)\n            return False\n\n        # Check if the graph is connected.\n        if len(self.nx_graph) == 0 or not nx.is_connected(self.nx_graph):\n            logger.debug(\"graph %s is not connected, it's not a disk graph\", self.id)\n            return False\n\n        # Check the distances between all pairs of nodes.\n        pos = self.pyg.pos\n        assert pos is not None\n        for u, v in nx.non_edges(self.nx_graph):\n            distance_um = np.linalg.norm(np.array(pos[u]) - np.array(pos[v]))\n            if distance_um &lt;= radius:\n                # These disjointed nodes would interact with each other, so\n                # this is not an embeddable graph.\n                logger.debug(\n                    \"graph %s has non-edges that are too close to each other, it's not a disk graph\",\n                    self.id,\n                )\n                return False\n\n        for u, v in self.nx_graph.edges():\n            distance_um = np.linalg.norm(np.array(pos[u]) - np.array(pos[v]))\n            if distance_um &gt; radius:\n                # These joined nodes would not interact with each other, so\n                # this is not an embeddable graph.\n                logger.debug(\n                    \"graph %s has edges that are too distant from each other (%s &gt; %s), it's not a disk graph\",\n                    self.id,\n                    distance_um,\n                    radius,\n                )\n                return False\n\n        return True\n\n    def is_embeddable(self) -&gt; bool:docs\n        \"\"\"\n            A predicate to check if the graph can be embedded in the\n            quantum device.\n\n            For a graph to be embeddable on a device, all the following\n            criteria must be fulfilled:\n            - the graph must be non-empty;\n            - the device must have at least as many atoms as the graph has\n                nodes;\n            - the device must be physically large enough to place all the\n                nodes (device.max_radial_distance);\n            - the nodes must be distant enough that quantum interactions\n                may take place (device.min_atom_distance)\n\n        Returns:\n            bool: True if possible, False if not\n        \"\"\"\n\n        # Reject empty graphs.\n        if self.pyg.num_nodes == 0 or self.pyg.num_nodes is None:\n            logger.debug(\"graph %s is empty, it's not embeddable\", self.id)\n            return False\n\n        # Reject graphs that have more nodes than can be represented\n        # on the device.\n        if self.pyg.num_nodes &gt; self.device.max_atom_num:\n            logger.debug(\n                \"graph %s has too many nodes (%s), it's not embeddable\", self.id, self.pyg.num_nodes\n            )\n            return False\n\n        # Check the distance from the center\n        pos = self.pyg.pos\n        assert pos is not None\n        distance_from_center = np.linalg.norm(pos, ord=2, axis=-1)\n        if any(distance_from_center &gt; self.device.max_radial_distance):\n            logger.debug(\n                \"graph %s has nodes to far from the center (%s &gt; %s), it's not embeddable\",\n                self.id,\n                max(distance_from_center),\n                self.device.max_radial_distance,\n            )\n            return False\n\n        # Check distance between nodes\n        if not self.is_disk_graph(self.device.min_atom_distance + EPSILON_RADIUS_UM):\n            logger.debug(\"graph %s is not a disk graph, it's not embeddable\", self.id)\n            return False\n\n        for u, v in self.nx_graph.edges():\n            distance_um = np.linalg.norm(np.array(pos[u]) - np.array(pos[v]))\n            if distance_um &lt; self.device.min_atom_distance:\n                # These nodes are too close to each other, preventing quantum\n                # interactions on the device.\n                logger.debug(\n                    \"graph %s has nodes that are too close to each other (%s &lt; %s), it's not embeddable\",\n                    self.id,\n                    distance_um,\n                    self.device.min_atom_distance,\n                )\n                return False\n\n        return True\n\n    # Default values for the sequence.\n    #\n    # See the companion paper for an explanation.\n    SEQUENCE_DEFAULT_AMPLITUDE_RAD_PER_US = 1.0 * 2 * np.pi\n    SEQUENCE_DEFAULT_DURATION_NS = 660\n\n    def compile_register(self) -&gt; pl.Register:docs\n        \"\"\"Create a Quantum Register based on a graph.\n\n        Returns:\n            pulser.Register: register\n        \"\"\"\n        # Note: In the low-level API, we separate register and pulse compilation for\n        # pedagogical reasons, because we want to take the opportunity to teach them\n        # about registers and pulses, rather than pulser sequences.\n\n        if not self.is_embeddable():\n            raise CompilationError(f\"The graph is not compatible with {self.device}\")\n\n        # Compile register\n        pos = self.pyg.pos\n        assert pos is not None\n        reg = pl.Register.from_coordinates(coords=pos)\n        if self.device.requires_layout:\n            reg = reg.with_automatic_layout(device=self.device)\n\n        try:\n            # Due to issue #29, we can produce a register that will not work on this device,\n            # so we need to perform a second check.\n            pl.Sequence(register=reg, device=self.device)\n        except ValueError as e:\n            raise CompilationError(f\"The graph is not compatible with {self.device}: {e}\")\n        return reg\n\n    def compile_pulse(docs\n        self,\n        amplitude: float = SEQUENCE_DEFAULT_AMPLITUDE_RAD_PER_US,\n        duration: int = SEQUENCE_DEFAULT_DURATION_NS,\n    ) -&gt; pl.Pulse:\n        \"\"\"Extract a Pulse for this graph.\n\n        A Pulse represents the laser applied to the atoms on the device.\n\n        Arguments:\n            amplitude: The amplitude for the laser pulse, in rad per microseconds.\n                By default, use the value demonstrated in the companion paper.\n            duration: The duration of the laser pulse, in nanoseconds.\n                By default, use the value demonstrated in the companion paper.\n        \"\"\"\n        # Note: In the low-level API, we separate register and pulse compilation for\n        # pedagogical reasons, because we want to take the opportunity to teach them\n        # about registers and pulses, rather than pulser sequences.\n\n        # See the companion paper for an explanation on these constants.\n        Omega_max = amplitude\n        t_max = duration\n        pulse = pl.Pulse.ConstantAmplitude(\n            amplitude=Omega_max,\n            detuning=pl.waveforms.RampWaveform(t_max, 0, 0),\n            phase=0.0,\n        )\n        return pulse\n\n\nclass MoleculeGraph(BaseGraph):docs\n    \"\"\"\n    A graph based on molecular data, being prepared for embedding on a\n    quantum device.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: Any,\n        data: pyg_data.Data,\n        device: pl.devices.Device,\n        node_mapping: dict[int, str],\n        edge_mapping: dict[int, Chem.BondType],\n        target: int | None = None,\n    ):\n        \"\"\"\n        Compute the geometry for a molecule graph.\n\n        Args:\n            data:  A homogeneous graph, in PyTorch Geometric format. Unchanged.\n            blockade_radius: The radius of the Rydberg Blockade. Two\n                connected nodes should be at a distance &lt; blockade_radius,\n                while two disconnected nodes should be at a\n                distance &gt; blockade_radius.\n            node_mapping: A mapping of node labels from numbers to strings,\n                e.g. `5 =&gt; \"Cl\"`. Used when building molecules, e.g. to compute\n                distances between nodes.\n            edge_mapping: A mapping of edge labels from number to chemical\n                bond types, e.g. `2 =&gt; DOUBLE`. Used when building molecules,\n                e.g. to compute distances between nodes.\n            target: If specified, a target for machine learning, as a value\n                `0` or `1`.\n        \"\"\"\n        pyg = data.clone()\n        pyg.pos = None  # Placeholder\n        super().__init__(id=id, data=pyg, device=device, target=target)\n\n        # Reconstruct the molecule.\n        tmp_mol = graph_to_mol(\n            graph=self.nx_graph,\n            node_mapping=node_mapping,\n            edge_mapping=edge_mapping,\n        )\n\n        # Extract the geometry.\n        AllChem.Compute2DCoords(tmp_mol, useRingTemplates=True)\n        original_pos = tmp_mol.GetConformer().GetPositions()[..., :2]  # Convert to 2D\n\n        # We now want to scale the geometry so that the smallest edge\n        # is as long as `device.min_atom_distance`.\n        pos = original_pos\n        pairs: list[tuple[Any, Any]] = []\n        for start, end in self.nx_graph.edges():\n            pairs.append((start, end))\n        for start, end in nx.non_edges(self.nx_graph):\n            pairs.append((start, end))\n\n        distances = []\n        for start, end in pairs:\n            distances.append(np.linalg.norm(pos[start] - pos[end]))\n        min_distance = np.min(distances)\n        pos = pos * device.min_atom_distance / min_distance\n\n        # The above transformation is sensitive to rouding errors, so if we realize that\n        # we accidentally made the smallest edge too small, we'll multiply by a small factor.\n        while True:\n            distances = []\n            for start, end in pairs:\n                distances.append(np.linalg.norm(pos[start] - pos[end]))\n            min_distance = np.min(distances)\n            if min_distance &gt;= device.min_atom_distance:\n                logger.debug(\n                    \"The minimal distance in graph #%s exceeds min atom distance: %s &gt; %s, it should now be compilable\",\n                    self.id,\n                    min_distance,\n                    device.min_atom_distance,\n                )\n                break\n            pos = pos * EPSILON_RESCALE_FACTOR\n\n        # Finally, store the position.\n        self.pyg.pos = pos\n\n\nclass PTCFMGraph(MoleculeGraph):docs\n    \"\"\"\n    An ingester for molecule graphs using\n    PTC-FM dataset conventions.\n    \"\"\"\n\n    # Constants used to decode the PTC-FM dataset, mapping\n    # integers (used as node attributes) to atom names.\n    PTCFM_ATOM_NAMES: Final[dict[int, str]] = {\n        0: \"In\",\n        1: \"P\",\n        2: \"C\",\n        3: \"O\",\n        4: \"N\",\n        5: \"Cl\",\n        6: \"S\",\n        7: \"Br\",\n        8: \"Na\",\n        9: \"F\",\n        10: \"As\",\n        11: \"K\",\n        12: \"Cu\",\n        13: \"I\",\n        14: \"Ba\",\n        15: \"Sn\",\n        16: \"Pb\",\n        17: \"Ca\",\n    }\n\n    # Constants used to decode the PTC-FM dataset, mapping\n    # integers (used as edge attributes) to bond types.\n    PTCFM_BOND_TYPES: Final[dict[int, Chem.BondType]] = {\n        0: Chem.BondType.TRIPLE,\n        1: Chem.BondType.SINGLE,\n        2: Chem.BondType.DOUBLE,\n        3: Chem.BondType.AROMATIC,\n    }\n\n    def __init__(\n        self,\n        id: Any,\n        data: pyg_data.Data,\n        device: pl.devices.Device,\n    ):\n        \"\"\"\n        Compute the geometry for a molecule graph.\n\n        Args:\n            data:  A homogeneous graph, in PyTorch Geometric format. Unchanged.\n            blockade_radius: The radius of the Rydberg Blockade. Two\n                connected nodes should be at a distance &lt; blockade_radius,\n                while two disconnected nodes should be at a\n                distance &gt; blockade_radius.\n            node_mapping: A mapping of node labels from numbers to strings,\n                e.g. `5 =&gt; \"Cl\"`. Used when building molecules, e.g. to compute\n                distances between nodes.\n            edge_mapping: A mapping of edge labels from number to chemical\n                bond types, e.g. `2 =&gt; DOUBLE`. Used when building molecules,\n                e.g. to compute distances between nodes.\n            target: If specified, a target for machine learning, as a value\n                `0` or `1`.\n        \"\"\"\n        target = data.y\n        if target is None:\n            raise AttributeError(\"The graph should have an attribute 'y'.\")\n\n        if isinstance(target, torch.Tensor):\n            target = target.item()\n        target = int(target)\n\n        super().__init__(\n            id=id,\n            data=data,\n            device=device,\n            node_mapping=PTCFMGraph.PTCFM_ATOM_NAMES,\n            edge_mapping=PTCFMGraph.PTCFM_BOND_TYPES,\n            target=target,\n        )\n\n\nGraphType = TypeVar(\"GraphType\")\n\n\nclass BaseGraphCompiler(abc.ABC, Generic[GraphType]):docs\n    \"\"\"\n    Abstract class, used to load a graph and compile a Pulser sequence for a device.\n\n    You should probably use one of the subclasses.\n    \"\"\"\n\n    @abc.abstractmethoddocs\n    def ingest(self, graph: GraphType, device: pl.devices.Device, id: int) -&gt; BaseGraph:\n        raise Exception(\"Please use one of the subclasses\")\n\n\nclass PygWithPosCompiler(BaseGraphCompiler[pyg_data.Data]):docs\n    \"\"\"\n    A compiler able to ingest torch_geometric graphs with positions.\n    \"\"\"\ndocs\n    def ingest(self, graph: pyg_data.Data, device: pl.devices.Device, id: int) -&gt; BaseGraph:\n        \"\"\"\n        Compile a Pulser sequence from a torch_geometric graph with position.\n\n        Args:\n            graph: A graph with positions (specified as attribute `pos`) and\n                optionally a prediction target (specified as attribute `y`, which\n                must be an `int`). The graph will not be changed.\n            device: The device for which the sequence must be compiled.\n            id: A unique identifier for the graph, used mostly for logging\n                and displaying error messages.\n        \"\"\"\n        return BaseGraph(id=id, data=graph, device=device)\n\ndocs\nclass MoleculeGraphCompiler(BaseGraphCompiler[tuple[pyg_data.Data, int | None]]):\n    \"\"\"\n    A compiler able to ingest torch_geometric molecules with a target.\n    \"\"\"\n\n    def __init__(\n        self,\n        node_mapping: dict[int, str],\n        edge_mapping: dict[int, Chem.BondType],\n    ):\n        \"\"\"\n        Setup a molecule graph compiler.\n\n        Args:\n            node_mapping: A mapping from node labels (as integers) to atom names (e.g. \"C\", \"Ar\", ...).\n            edge_mapping: A mapping from node labels (as integers) to chemical bond types (e.g. simple\n                bound, double bound).\n        \"\"\"\n        self.node_mapping = node_mapping\n        self.edge_mapping = edge_mapping\n\n    \"\"\"\n    Compile a Pulser sequence from a molecule, expressed as a torch_geometric\n    graph.\n\n    Args:\n        graph: A molecular graph.\n            This graph is expected to have:\n             - `int` labels on nodes, which may be converted into atom names\n                with `self.node_mapping`\n             - `int` labels on edges, which may be converted into chemical\n                bounds with `self.edge_mapping`\n            The graph will not be changed.\n        device: The device for which the sequence must be compiled.\n        id: A unique identifier for the graph, used mostly for logging\n            and displaying error messages.\n    \"\"\"\n\n    def ingest(docs\n        self, graph: tuple[pyg_data.Data, int | None], device: pl.devices.Device, id: int\n    ) -&gt; MoleculeGraph:\n        return MoleculeGraph(\n            id=id,\n            data=graph[0],\n            device=device,\n            node_mapping=self.node_mapping,\n            edge_mapping=self.edge_mapping,\n            target=graph[1],\n        )\n\n\nclass PTCFMCompiler(BaseGraphCompiler[pyg_data.Data]):docs\n    def ingest(self, graph: pyg_data.Data, device: pl.devices.Device, id: int) -&gt; PTCFMGraph:\n        return PTCFMGraph(\n            id=id,\n            data=graph,\n            device=device,\n        )\n\n\n@dataclass\nclass NXWithPos:docs\n    \"\"\"\n    A networkx graph and its position\n    \"\"\"\n\n    graph: nx.Graph\n\n    # Mapping from node to positions.\n    positions: dict[Any, np.ndarray]\n\n    # A machine learning target\n    target: int | None\n\n\nclass NXGraphCompiler(BaseGraphCompiler[NXWithPos]):docs\n    def ingest(self, graph: NXWithPos, device: pl.devices.Device, id: int) -&gt; BaseGraph:\n        pyg = pyg_utils.from_networkx(graph.graph)\n        pyg.y = graph.target\n        positions = np.array([graph.positions[node] for node in graph.graph.nodes()])\n        pyg.pos = torch.tensor(positions, dtype=torch.float)\n\n        return BaseGraph(id=id, device=device, data=pyg)\n</code></pre>"},{"location":"api/qek/data/processed_data/","title":"qek.data.processed_data","text":"qek.data.processed_data<p> source module qek.data.processed_data </p> <p>Loading, saving, manipulation or processed data.</p> <p> Classes </p> <ul> <li> <p>ProcessedData \u2014 Data on a single graph obtained from the Quantum Device.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>dist_excitation \u2014 Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> </li> <li> <p>save_dataset \u2014 Saves a dataset to a JSON file.</p> </li> <li> <p>load_dataset \u2014 Loads a dataset from a JSON file.</p> </li> </ul> <p> source class ProcessedData(sequence: pl.Sequence, state_dict: dict[str, int | np.int64], target: int | None) </p> <p>Data on a single graph obtained from the Quantum Device.</p> <p> Attributes </p> <ul> <li> <p>register :  pl.Register \u2014 The geometry of atoms on the Quantum Device, obtained by compiling the graph for execution on the Quantum Device.</p> </li> <li> <p>pulse :  pl.Pulse \u2014 The laser pulse, obtained by compiling the graph for execution on the Quantum Device.</p> </li> <li> <p>state_dict :  Final[dict[str, int]] \u2014 A dictionary {bitstring: number of instances} for this graph.</p> </li> <li> <p>target :  Final[int | None] \u2014 If specified, the machine-learning target, as a value <code>0</code> or <code>1</code>.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>from_register</p> </li> <li> <p>save_to_file</p> </li> <li> <p>dist_excitation \u2014 Return the distribution of excitations for this graph.</p> </li> <li> <p>draw_pulse \u2014 Draw the pulse on screen</p> </li> <li> <p>draw_register \u2014 Draw the register on screen</p> </li> <li> <p>draw_excitation \u2014 Draw an histogram for the excitation level on screen</p> </li> </ul> <p> source classmethod ProcessedData.from_register(register: pl.Register, pulse: pl.Pulse, device: pl.devices.Device, state_dict: dict[str, int | np.int64], target: int | None) \u2192 ProcessedData </p> <p> source method ProcessedData.save_to_file(file_path: str) \u2192 None </p> <p> source method ProcessedData.dist_excitation(size: int | None = None) \u2192 np.ndarray </p> <p>Return the distribution of excitations for this graph.</p> <p> Parameters </p> <ul> <li> <p>size :  int | None \u2014 If specified, truncate or pad the array to this size.</p> </li> </ul> <p> source property ProcessedData.pulse: pl.Pulse </p> <p>The laser pulse used to process this data.</p> <p> source property ProcessedData.register: pl.Register </p> <p>The register to which the graph was compiled.</p> <p> source method ProcessedData.draw_pulse() \u2192 None </p> <p>Draw the pulse on screen</p> <p> source method ProcessedData.draw_register() \u2192 None </p> <p>Draw the register on screen</p> <p> source method ProcessedData.draw_excitation() \u2192 None </p> <p>Draw an histogram for the excitation level on screen</p> <p> source dist_excitation(state_dict: dict[str, int], size: int | None = None) \u2192 np.ndarray </p> <p>Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> <p> Parameters </p> <ul> <li> <p>size :  int | None \u2014 If specified, only keep <code>size</code> energy distributions in the output. Otherwise, keep all values.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>np.ndarray \u2014 A histogram of excitation energies. - index: an excitation level (i.e. a number of <code>1</code> bits in a     bitstring) - value: normalized count of samples with this excitation level.</p> </li> </ul> <p> source save_dataset(dataset: list[ProcessedData], file_path: str) \u2192 None </p> <p>Saves a dataset to a JSON file.</p> <p> Parameters </p> <ul> <li> <p>dataset :  list[ProcessedData] \u2014 The dataset to be saved, containing RegisterData instances.</p> </li> <li> <p>file_path :  str \u2014 The path where the dataset will be saved as a JSON file.</p> </li> </ul> <p>Note</p> <p>The data is stored in a format suitable for loading with load_dataset.</p> <p> Returns </p> <ul> <li> <p>None \u2014 None</p> </li> </ul> <p> source load_dataset(file_path: str) \u2192 list[ProcessedData] </p> <p>Loads a dataset from a JSON file.</p> <p> Parameters </p> <ul> <li> <p>file_path :  str \u2014 The path to the JSON file containing the dataset.</p> </li> </ul> <p>Note</p> <p>The data is loaded in the format that was used when saving with     save_dataset.</p> <p> Returns </p> <ul> <li> <p>list[ProcessedData] \u2014 A list of ProcessedData instances, corresponding to the data stored in     the JSON file.</p> </li> </ul>"},{"location":"src/qek/data/processed_data/","title":"qek.data.processed_data","text":"qek.data.processed_data<p> docs module qek.data.processed_data </p> <pre><code>\"\"\"\nLoading, saving, manipulation or processed data.\n\"\"\"\n\nimport collections\nimport json\nfrom typing import Final\nimport matplotlib\n\nimport logging\nimport numpy as np\nimport pulser as pl\n\nfrom qek.data.graphs import EPSILON_RADIUS_UM\nfrom qek.shared._utils import make_sequence\n\nlogger = logging.getLogger(__name__)\n\n\nclass ProcessedData:docs\n    \"\"\"\n    Data on a single graph obtained from the Quantum Device.\n\n    Attributes:\n        register: The geometry of atoms on the Quantum Device, obtained\n            by compiling the graph for execution on the Quantum Device.\n        pulse: The laser pulse, obtained by compiling the graph for\n            execution on the Quantum Device.\n        state_dict: A dictionary {bitstring: number of instances}\n            for this graph.\n        target: If specified, the machine-learning target, as a\n            value `0` or `1`.\n\n    The state dictionary represents an approximation of the quantum\n    state of the device for this graph after completion of the\n    algorithm.\n\n    - keys are bitstrings, i.e. strings of N time 0 or 1, where N\n      is the number of qubits, i.e. the number of nodes in the graph.\n      Each of these {0, 1} corresponds to a possible state for the\n      corresponding qubit.\n    - values are the number of samples observed with this specific\n      state of the register.\n\n    The sum of all values for the dictionary is equal to the total\n    number of samples observed on the quantum device (for this\n    specific graph).\n    \"\"\"\n\n    state_dict: Final[dict[str, int]]\n    _dist_excitation: Final[np.ndarray]\n    target: Final[int | None]\n\n    def __init__(\n        self, sequence: pl.Sequence, state_dict: dict[str, int | np.int64], target: int | None\n    ):\n        self._sequence = sequence\n        # Some emulators will actually be `dict[str, int64]` instead of `dict[str, int]` and `int64`\n        # is not JSON-serializable.\n        #\n        # The reason for which `int64` is not JSON-serializable is that JSON limits ints to 2^53-1.\n        # In practice, this should not be a problem, since the `int`/`int64` in our dict is\n        # limited to the number of runs, and we don't expect to be launching 2^53 consecutive runs\n        # for a single sequence on a device in any foreseeable future (assuming a run of 1ns,\n        # this would still take ~4 billion years to execute).\n        self.state_dict = {k: int(value) for k, value in state_dict.items()}\n        self._dist_excitation = dist_excitation(self.state_dict)\n        self.target = target\n\n    @classmethod\n    def from_register(docs\n        cls,\n        register: pl.Register,\n        pulse: pl.Pulse,\n        device: pl.devices.Device,\n        state_dict: dict[str, int | np.int64],\n        target: int | None,\n    ) -&gt; \"ProcessedData\":\n        sequence = make_sequence(register=register, pulse=pulse, device=device)\n        return ProcessedData(sequence=sequence, state_dict=state_dict, target=target)\n\n    def save_to_file(self, file_path: str) -&gt; None:docs\n        with open(file_path, \"w\") as file:\n            tmp_dict = {\n                \"state_dict\": self.state_dict,\n                \"target\": self.target,\n            }\n            json.dump(tmp_dict, file)\n\n    def dist_excitation(self, size: int | None = None) -&gt; np.ndarray:docs\n        \"\"\"\n        Return the distribution of excitations for this graph.\n\n        Arguments:\n            size: If specified, truncate or pad the array to this\n                size.\n        \"\"\"\n        if size is None or size == len(self._dist_excitation):\n            return self._dist_excitation.copy()\n        if size &lt; len(self._dist_excitation):\n            return np.resize(self._dist_excitation, size)\n        return np.pad(self._dist_excitation, (0, size - len(self._dist_excitation)))\n\n    @property\n    def pulse(self) -&gt; pl.Pulse:docs\n        \"\"\"\n        The laser pulse used to process this data.\n        \"\"\"\n        # For the time being, we cannot obtain this information from a public API, see https://github.com/pasqal-io/Pulser/issues/801 .\n        pulse = self._sequence._schedule[\"ising\"].slots[-1].type\n        assert isinstance(pulse, pl.Pulse)\n        return pulse\n\n    @property\n    def register(self) -&gt; pl.Register:docs\n        \"\"\"\n        The register to which the graph was compiled.\n        \"\"\"\n        register = self._sequence.register\n        assert isinstance(register, pl.Register)\n        return register\n\n    def draw_pulse(self) -&gt; None:docs\n        \"\"\"\n        Draw the pulse on screen\n        \"\"\"\n        self.pulse.draw()\n\n    def draw_register(self) -&gt; None:docs\n        \"\"\"\n        Draw the register on screen\n        \"\"\"\n        self.register.draw(\n            # We increase slightly the blockade radius to take into account rounding errors.\n            blockade_radius=self._sequence.device.min_atom_distance\n            + EPSILON_RADIUS_UM\n        )\n\n    def draw_excitation(self) -&gt; None:docs\n        \"\"\"\n        Draw an histogram for the excitation level on screen\n        \"\"\"\n        x = [str(i) for i in range(len(self._dist_excitation))]\n        matplotlib.pyplot.bar(x, self._dist_excitation)\n\ndocs\ndef dist_excitation(state_dict: dict[str, int], size: int | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the distribution of excitation energies from a dictionary of\n    bitstrings to their respective counts.\n\n    Args:\n        size (int | None): If specified, only keep `size` energy\n            distributions in the output. Otherwise, keep all values.\n\n    Returns:\n        A histogram of excitation energies.\n        - index: an excitation level (i.e. a number of `1` bits in a\n            bitstring)\n        - value: normalized count of samples with this excitation level.\n    \"\"\"\n\n    if len(state_dict) == 0:\n        return np.ndarray(0)\n\n    if size is None:\n        # If size is not specified, it's the length of bitstrings.\n        # We assume that all bitstrings in `count_bitstring` have the\n        # same length and we have just checked that it's not empty.\n\n        # Pick the length of the first bitstring.\n        # We have already checked that `count_bitstring` is not empty.\n        bitstring = next(iter(state_dict.keys()))\n        size = len(bitstring)\n\n    # Make mypy realize that `size` is now always an `int`.\n    assert type(size) is int\n\n    count_occupation: dict[int, int] = collections.defaultdict(int)\n    total = 0.0\n    for bitstring, number in state_dict.items():\n        occupation = sum(1 for bit in bitstring if bit == \"1\")\n        count_occupation[occupation] += number\n        total += number\n\n    result = np.zeros(size + 1, dtype=float)\n    for occupation, count in count_occupation.items():\n        if occupation &lt; size:\n            result[occupation] = count / total\n\n    return result\n\n\ndef save_dataset(dataset: list[ProcessedData], file_path: str) -&gt; None:docs\n    \"\"\"Saves a dataset to a JSON file.\n\n    Args:\n        dataset (list[ProcessedData]): The dataset to be saved, containing\n            RegisterData instances.\n        file_path (str): The path where the dataset will be saved as a JSON\n            file.\n\n    Note:\n        The data is stored in a format suitable for loading with load_dataset.\n\n    Returns:\n        None\n    \"\"\"\n    with open(file_path, \"w\") as file:\n        data = [\n            {\n                \"sequence\": instance._sequence.to_abstract_repr(),\n                \"state_dict\": instance.state_dict,\n                \"target\": instance.target,\n            }\n            for instance in dataset\n        ]\n        json.dump(data, file)\n\n\ndef load_dataset(file_path: str) -&gt; list[ProcessedData]:docs\n    \"\"\"Loads a dataset from a JSON file.\n\n    Args:\n        file_path (str): The path to the JSON file containing the dataset.\n\n    Note:\n        The data is loaded in the format that was used when saving with\n            save_dataset.\n\n    Returns:\n        A list of ProcessedData instances, corresponding to the data stored in\n            the JSON file.\n    \"\"\"\n    with open(file_path) as file:\n        data = json.load(file)\n        return [\n            ProcessedData(\n                sequence=pl.Sequence.from_abstract_repr(item[\"sequence\"]),\n                state_dict=item[\"state_dict\"],\n                target=item[\"target\"],\n            )\n            for item in data\n        ]\n</code></pre>"},{"location":"api/qek/data/training_data/","title":"qek.data.training_data","text":"qek.data.training_data<p> source module qek.data.training_data </p> <p>Manipulating training data</p> <p> Functions </p> <ul> <li> <p>split_train_test \u2014 This function splits a torch dataset into train and val dataset.     As torch Dataset class is a mother class of pytorch_geometric dataset     class, it should work just fine for the latter.</p> </li> </ul> <p> source split_train_test(dataset: torch_data.Dataset, lengths: list[float], seed: int | None = None) \u2192 tuple[torch_data.Dataset, torch_data.Dataset] </p> <p>This function splits a torch dataset into train and val dataset.     As torch Dataset class is a mother class of pytorch_geometric dataset     class, it should work just fine for the latter.</p> <p> Parameters </p> <ul> <li> <p>dataset :  torch_data.Dataset \u2014 The original dataset to be splitted</p> </li> <li> <p>lengths :  list[float] \u2014 Percentage of the split. For instance [0.8, 0.2]</p> </li> <li> <p>seed :  int | None, optional \u2014 Seed for reproductibility. Defaults to</p> </li> <li> <p>None.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[torch_data.Dataset, torch_data.Dataset] \u2014 train and val dataset</p> </li> </ul>"},{"location":"src/qek/data/training_data/","title":"qek.data.training_data","text":"qek.data.training_data<p> docs module qek.data.training_data </p> <pre><code>\"\"\"\nManipulating training data\n\"\"\"\n\nimport torch\nimport torch.utils.data as torch_data\n\n\ndef split_train_test(docs\n    dataset: torch_data.Dataset,\n    lengths: list[float],\n    seed: int | None = None,\n) -&gt; tuple[torch_data.Dataset, torch_data.Dataset]:\n    \"\"\"\n        This function splits a torch dataset into train and val dataset.\n        As torch Dataset class is a mother class of pytorch_geometric dataset\n        class, it should work just fine for the latter.\n\n    Args:\n        dataset (torch_data.Dataset): The original dataset to be splitted\n        lengths (list[float]): Percentage of the split. For instance [0.8, 0.2]\n        seed (int | None, optional): Seed for reproductibility. Defaults to\n        None.\n\n    Returns:\n        tuple[torch_data.Dataset, torch_data.Dataset]: train and val dataset\n    \"\"\"\n    if seed is not None:\n        generator = torch.Generator().manual_seed(seed)\n    else:\n        generator = torch.Generator()\n    train, val = torch_data.random_split(dataset=dataset, lengths=lengths, generator=generator)\n    return train, val\n</code></pre>"},{"location":"api/qek/kernel/","title":"qek.kernel","text":"qek.kernel<p> source package qek.kernel </p> <p>The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> <p> Classes </p> <ul> <li> <p>QuantumEvolutionKernel \u2014 Implementation of the Quantum Evolution Kernel.</p> </li> </ul> <p> source class QuantumEvolutionKernel(mu: float, size_max: int | None = None) </p> <p>Implementation of the Quantum Evolution Kernel.</p> <p>Initialize the QuantumEvolutionKernel.</p> <p> Attributes </p> <ul> <li> <p>- params (dict) \u2014 Dictionary of training parameters. As of this writing, the only training parameter is \"mu\", the scaling factor for the Jensen-Shannon divergence.</p> </li> <li> <p>- X (Sequence[ProcessedData]) \u2014 Training data used for fitting the kernel.</p> </li> <li> <p>- kernel_matrix (np.ndarray) \u2014 Kernel matrix. This is assigned in the <code>fit()</code> method</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>mu :  float \u2014 Scaling factor for the Jensen-Shannon divergence</p> </li> <li> <p>size_max :  int, optional \u2014 If specified, only consider the first <code>size_max</code> qubits of bitstrings. Otherwise, consider all qubits. You may use this to trade precision in favor of speed.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>similarity \u2014 Compute the similarity between two graphs using Jensen-Shannon divergence.</p> </li> <li> <p>fit \u2014 Fit the kernel to the training dataset by storing the dataset.</p> </li> <li> <p>transform \u2014 Transform the dataset into the kernel space with respect to the training dataset.</p> </li> <li> <p>fit_transform \u2014 Fit the kernel to the training dataset and transform it.</p> </li> <li> <p>create_train_kernel_matrix \u2014 Compute a kernel matrix for a given training dataset.</p> </li> <li> <p>create_test_kernel_matrix \u2014 Compute a kernel matrix for a given testing dataset and training set.</p> </li> <li> <p>set_params \u2014 Set multiple parameters for the kernel.</p> </li> <li> <p>get_params \u2014 Retrieve the value of all parameters.</p> </li> </ul> <p> source method QuantumEvolutionKernel.similarity(graph_1: ProcessedData, graph_2: ProcessedData) \u2192 float </p> <p>Compute the similarity between two graphs using Jensen-Shannon divergence.</p> <p>This method computes the square of the Jensen-Shannon divergence (JSD) between two probability distributions over bitstrings. The JSD is a measure of the difference between two probability distributions, and it can be used as a kernel for machine learning algorithms that require a similarity function.</p> <p>The input graphs are assumed to have been processed using the ProcessedData class from qek_os.data_io.dataset. Parameter <code>size_max</code> controls the maximum length of the bitstrings considered in the computation. Args:     graph_1 (ProcessedData): First graph.     graph_2 (ProcessedData): Second graph.     size_max (int, optional): Maximum length of bitstrings to     consider. Defaults to all.</p> <p> Returns </p> <ul> <li> <p>float \u2014 Similarity between the two graphs, scaled by a factor that depends on mu.</p> </li> </ul> <p>Notes</p> <p>The JSD is computed using the jensenshannon function from <code>scipy.spatial.distance</code>, and it is squared because scipy function <code>jensenshannon</code> outputs the distance instead of the divergence.</p> <p> source method QuantumEvolutionKernel.fit(X: Sequence[ProcessedData], y: list | None = None) \u2192 None </p> <p>Fit the kernel to the training dataset by storing the dataset.</p> <p> Parameters </p> <ul> <li> <p>X :  Sequence[ProcessedData] \u2014 The training dataset.</p> </li> <li> <p>y :  list | None \u2014 list: Target variable for the dataset sequence. This argument is ignored, provided only for compatibility with machine-learning libraries.</p> </li> </ul> <p> source method QuantumEvolutionKernel.transform(X_test: Sequence[ProcessedData], y_test: list | None = None) \u2192 np.ndarray </p> <p>Transform the dataset into the kernel space with respect to the training dataset.</p> <p> Parameters </p> <ul> <li> <p>X_test :  Sequence[ProcessedData] \u2014 The dataset to transform. y_test: list: Target variable for the dataset sequence.     This argument is ignored, provided only for compatibility     with machine-learning libraries.</p> </li> <li> <p>Returns \u2014</p> <p>np.ndarray: Kernel matrix where each entry represents the similarity between             the given dataset and the training dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source method QuantumEvolutionKernel.fit_transform(X: Sequence[ProcessedData], y: list | None = None) \u2192 np.ndarray </p> <p>Fit the kernel to the training dataset and transform it.</p> <p> Parameters </p> <ul> <li> <p>X :  Sequence[ProcessedData] \u2014 The dataset to fit and transform. y: list: Target variable for the dataset sequence.     This argument is ignored, provided only for compatibility     with machine-learning libraries.</p> </li> <li> <p>Returns \u2014</p> <p>np.ndarray: Kernel matrix for the training dataset.</p> </li> </ul> <p> source method QuantumEvolutionKernel.create_train_kernel_matrix(train_dataset: Sequence[ProcessedData]) \u2192 np.ndarray </p> <p>Compute a kernel matrix for a given training dataset.</p> <p>This method computes a symmetric N x N kernel matrix from the Jensen-Shannon divergences between all pairs of graphs in the input dataset. The resulting matrix can be used as a similarity metric for machine learning algorithms. Args:     train_dataset (Sequence[ProcessedData]): A list of ProcessedData     objects to compute the kernel matrix from. Returns:     np.ndarray: An N x N symmetric matrix where the entry at row i and     column j represents the similarity between the graphs in positions     i and j of the input dataset.</p> <p> source method QuantumEvolutionKernel.create_test_kernel_matrix(test_dataset: Sequence[ProcessedData], train_dataset: Sequence[ProcessedData]) \u2192 np.ndarray </p> <p>Compute a kernel matrix for a given testing dataset and training set.</p> <p>This method computes an N x M kernel matrix from the Jensen-Shannon divergences between all pairs of graphs in the input testing dataset and the training dataset. The resulting matrix can be used as a similarity metric for machine learning algorithms, particularly when evaluating the performance on the test dataset using a trained model. Args:     test_dataset (Sequence[ProcessedData]): A list of ProcessedData     objects representing the testing dataset.     train_dataset (Sequence[ProcessedData]): A list of ProcessedData     objects representing the training set. Returns:     np.ndarray: An M x N matrix where the entry at row i and column j     represents the similarity between the graph in position i of the     test dataset and the graph in position j of the training set.</p> <p> source method QuantumEvolutionKernel.set_params(**kwargs: dict[str, Any]) \u2192 None </p> <p>Set multiple parameters for the kernel.</p> <p> Parameters </p> <ul> <li> <p>**kwargs :  dict[str, Any] \u2014 Arbitrary keyword dictionary where keys are attribute names</p> </li> <li> <p>and values are their respective values</p> </li> </ul> <p> source method QuantumEvolutionKernel.get_params(deep: bool = True) \u2192 dict </p> <p>Retrieve the value of all parameters.</p> <p> Parameters </p> <ul> <li> <p>deep :  bool \u2014 Ignored for the time being. Added for compatibility with various machine learning libraries, such as scikit-learn.</p> </li> </ul> <p>Returns     dict: A dictionary of parameters and their respective values.         Note that this method always performs a copy of the dictionary.</p>"},{"location":"src/qek/kernel/","title":"qek.kernel","text":"qek.kernel<p> docs package qek.kernel </p> <pre><code>\"\"\"\nThe Quantum Evolution Kernel itself, for use in a machine-learning pipeline.\n\"\"\"\n\nfrom .kernel import QuantumEvolutionKernel\n\n__all__ = [\"QuantumEvolutionKernel\"]\n</code></pre>"},{"location":"api/qek/kernel/kernel/","title":"qek.kernel.kernel","text":"qek.kernel.kernel<p> source module qek.kernel.kernel </p> <p>The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> <p> Classes </p> <ul> <li> <p>QuantumEvolutionKernel \u2014 Implementation of the Quantum Evolution Kernel.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>count_occupation_from_bitstring \u2014 Counts the number of '1' bits in a binary string.</p> </li> <li> <p>dist_excitation_and_vec \u2014 Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> </li> </ul> <p> source class QuantumEvolutionKernel(mu: float, size_max: int | None = None) </p> <p>Implementation of the Quantum Evolution Kernel.</p> <p>Initialize the QuantumEvolutionKernel.</p> <p> Attributes </p> <ul> <li> <p>- params (dict) \u2014 Dictionary of training parameters. As of this writing, the only training parameter is \"mu\", the scaling factor for the Jensen-Shannon divergence.</p> </li> <li> <p>- X (Sequence[ProcessedData]) \u2014 Training data used for fitting the kernel.</p> </li> <li> <p>- kernel_matrix (np.ndarray) \u2014 Kernel matrix. This is assigned in the <code>fit()</code> method</p> </li> </ul> <p> Parameters </p> <ul> <li> <p>mu :  float \u2014 Scaling factor for the Jensen-Shannon divergence</p> </li> <li> <p>size_max :  int, optional \u2014 If specified, only consider the first <code>size_max</code> qubits of bitstrings. Otherwise, consider all qubits. You may use this to trade precision in favor of speed.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>similarity \u2014 Compute the similarity between two graphs using Jensen-Shannon divergence.</p> </li> <li> <p>fit \u2014 Fit the kernel to the training dataset by storing the dataset.</p> </li> <li> <p>transform \u2014 Transform the dataset into the kernel space with respect to the training dataset.</p> </li> <li> <p>fit_transform \u2014 Fit the kernel to the training dataset and transform it.</p> </li> <li> <p>create_train_kernel_matrix \u2014 Compute a kernel matrix for a given training dataset.</p> </li> <li> <p>create_test_kernel_matrix \u2014 Compute a kernel matrix for a given testing dataset and training set.</p> </li> <li> <p>set_params \u2014 Set multiple parameters for the kernel.</p> </li> <li> <p>get_params \u2014 Retrieve the value of all parameters.</p> </li> </ul> <p> source method QuantumEvolutionKernel.similarity(graph_1: ProcessedData, graph_2: ProcessedData) \u2192 float </p> <p>Compute the similarity between two graphs using Jensen-Shannon divergence.</p> <p>This method computes the square of the Jensen-Shannon divergence (JSD) between two probability distributions over bitstrings. The JSD is a measure of the difference between two probability distributions, and it can be used as a kernel for machine learning algorithms that require a similarity function.</p> <p>The input graphs are assumed to have been processed using the ProcessedData class from qek_os.data_io.dataset. Parameter <code>size_max</code> controls the maximum length of the bitstrings considered in the computation. Args:     graph_1 (ProcessedData): First graph.     graph_2 (ProcessedData): Second graph.     size_max (int, optional): Maximum length of bitstrings to     consider. Defaults to all.</p> <p> Returns </p> <ul> <li> <p>float \u2014 Similarity between the two graphs, scaled by a factor that depends on mu.</p> </li> </ul> <p>Notes</p> <p>The JSD is computed using the jensenshannon function from <code>scipy.spatial.distance</code>, and it is squared because scipy function <code>jensenshannon</code> outputs the distance instead of the divergence.</p> <p> source method QuantumEvolutionKernel.fit(X: Sequence[ProcessedData], y: list | None = None) \u2192 None </p> <p>Fit the kernel to the training dataset by storing the dataset.</p> <p> Parameters </p> <ul> <li> <p>X :  Sequence[ProcessedData] \u2014 The training dataset.</p> </li> <li> <p>y :  list | None \u2014 list: Target variable for the dataset sequence. This argument is ignored, provided only for compatibility with machine-learning libraries.</p> </li> </ul> <p> source method QuantumEvolutionKernel.transform(X_test: Sequence[ProcessedData], y_test: list | None = None) \u2192 np.ndarray </p> <p>Transform the dataset into the kernel space with respect to the training dataset.</p> <p> Parameters </p> <ul> <li> <p>X_test :  Sequence[ProcessedData] \u2014 The dataset to transform. y_test: list: Target variable for the dataset sequence.     This argument is ignored, provided only for compatibility     with machine-learning libraries.</p> </li> <li> <p>Returns \u2014</p> <p>np.ndarray: Kernel matrix where each entry represents the similarity between             the given dataset and the training dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source method QuantumEvolutionKernel.fit_transform(X: Sequence[ProcessedData], y: list | None = None) \u2192 np.ndarray </p> <p>Fit the kernel to the training dataset and transform it.</p> <p> Parameters </p> <ul> <li> <p>X :  Sequence[ProcessedData] \u2014 The dataset to fit and transform. y: list: Target variable for the dataset sequence.     This argument is ignored, provided only for compatibility     with machine-learning libraries.</p> </li> <li> <p>Returns \u2014</p> <p>np.ndarray: Kernel matrix for the training dataset.</p> </li> </ul> <p> source method QuantumEvolutionKernel.create_train_kernel_matrix(train_dataset: Sequence[ProcessedData]) \u2192 np.ndarray </p> <p>Compute a kernel matrix for a given training dataset.</p> <p>This method computes a symmetric N x N kernel matrix from the Jensen-Shannon divergences between all pairs of graphs in the input dataset. The resulting matrix can be used as a similarity metric for machine learning algorithms. Args:     train_dataset (Sequence[ProcessedData]): A list of ProcessedData     objects to compute the kernel matrix from. Returns:     np.ndarray: An N x N symmetric matrix where the entry at row i and     column j represents the similarity between the graphs in positions     i and j of the input dataset.</p> <p> source method QuantumEvolutionKernel.create_test_kernel_matrix(test_dataset: Sequence[ProcessedData], train_dataset: Sequence[ProcessedData]) \u2192 np.ndarray </p> <p>Compute a kernel matrix for a given testing dataset and training set.</p> <p>This method computes an N x M kernel matrix from the Jensen-Shannon divergences between all pairs of graphs in the input testing dataset and the training dataset. The resulting matrix can be used as a similarity metric for machine learning algorithms, particularly when evaluating the performance on the test dataset using a trained model. Args:     test_dataset (Sequence[ProcessedData]): A list of ProcessedData     objects representing the testing dataset.     train_dataset (Sequence[ProcessedData]): A list of ProcessedData     objects representing the training set. Returns:     np.ndarray: An M x N matrix where the entry at row i and column j     represents the similarity between the graph in position i of the     test dataset and the graph in position j of the training set.</p> <p> source method QuantumEvolutionKernel.set_params(**kwargs: dict[str, Any]) \u2192 None </p> <p>Set multiple parameters for the kernel.</p> <p> Parameters </p> <ul> <li> <p>**kwargs :  dict[str, Any] \u2014 Arbitrary keyword dictionary where keys are attribute names</p> </li> <li> <p>and values are their respective values</p> </li> </ul> <p> source method QuantumEvolutionKernel.get_params(deep: bool = True) \u2192 dict </p> <p>Retrieve the value of all parameters.</p> <p> Parameters </p> <ul> <li> <p>deep :  bool \u2014 Ignored for the time being. Added for compatibility with various machine learning libraries, such as scikit-learn.</p> </li> </ul> <p>Returns     dict: A dictionary of parameters and their respective values.         Note that this method always performs a copy of the dictionary.</p> <p> source count_occupation_from_bitstring(bitstring: str) \u2192 int </p> <p>Counts the number of '1' bits in a binary string.</p> <p> Parameters </p> <ul> <li> <p>bitstring :  str \u2014 A binary string containing only '0's and '1's.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The number of '1' bits found in the input string.</p> </li> </ul> <p> source dist_excitation_and_vec(count_bitstring: dict[str, int], size_max: int | None = None) \u2192 np.ndarray </p> <p>Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> <p> Parameters </p> <ul> <li> <p>count_bitstring :  dict[str, int] \u2014 A dictionary mapping binary strings to their counts.</p> </li> <li> <p>size_max :  int | None \u2014 If specified, only keep <code>size_max</code> energy distributions in the output. Otherwise, keep all values.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>np.ndarray \u2014 A NumPy array where keys are the number of '1' bits     in each binary string and values are the normalized counts.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/qek/kernel/kernel/","title":"qek.kernel.kernel","text":"qek.kernel.kernel<p> docs module qek.kernel.kernel </p> <pre><code>\"\"\"\nThe Quantum Evolution Kernel itself, for use in a machine-learning pipeline.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\nimport collections\nimport copy\nfrom collections.abc import Sequence\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom scipy.spatial.distance import jensenshannon\n\nfrom qek.data.processed_data import ProcessedData\n\n\nclass QuantumEvolutionKernel:docs\n    \"\"\"Implementation of the Quantum Evolution Kernel.\n\n    Attributes:\n    - params (dict): Dictionary of training parameters. As of this writing, the only\n        training parameter is \"mu\", the scaling factor for the Jensen-Shannon divergence.\n    - X (Sequence[ProcessedData]): Training data used for fitting the kernel.\n    - kernel_matrix (np.ndarray): Kernel matrix. This is assigned in the `fit()` method\n\n\n    \"\"\"\n\n    def __init__(self, mu: float, size_max: int | None = None):\n        \"\"\"Initialize the QuantumEvolutionKernel.\n\n        Args:\n            mu (float): Scaling factor for the Jensen-Shannon divergence\n            size_max (int, optional): If specified, only consider the first `size_max`\n                qubits of bitstrings. Otherwise, consider all qubits. You may use this\n                to trade precision in favor of speed.\n        \"\"\"\n        self.params: dict[str, Any] = {\n            \"mu\": mu,\n            \"size_max\": size_max,\n        }\n        self.X: Sequence[ProcessedData]\n        self.kernel_matrix: np.ndarray\n\n    def __call__(\n        self,\n        X1: Sequence[ProcessedData],\n        X2: Sequence[ProcessedData] | None = None,\n    ) -&gt; NDArray[np.floating]:\n        \"\"\"Compute a kernel matrix from two sequences of processed data.\n\n        This method computes a M x N kernel matrix from the Jensen-Shannon divergences\n        between all pairs of graphs in the two datasets. The resulting matrix can be used\n        as a similarity metric for machine learning algorithms.\n\n        If `X1` and `X2` are two sequences representing the processed data for a\n        single graph each, the resulting matrix can be used as a measure of similarity\n        between both graphs.\n\n        Args:\n            X1: processed data to be used as rows.\n            X2 (optional): processed data to be used as columns. If unspecified, use X1\n                as both rows and columns.\n        Returns:\n            np.ndarray: A len(X1) x len(X2) matrix where entry[i, j] represents the\n            similarity between rows[i] and columns[j], scaled by a factor that depends\n            on mu.\n        Notes:\n            The JSD is computed using the jensenshannon function from\n            `scipy.spatial.distance`, and it is squared because scipy function\n            `jensenshannon` outputs the distance instead of the divergence.\n        \"\"\"\n        # If size is not specified, set it to the length of the largest bitstring.\n        size_max = self.params[\"size_max\"]\n        if size_max is None:\n            if X2 is None:\n                # No need to walk the same source twice.\n                sources = [X1]\n            else:\n                sources = [X1, X2]\n            for source in sources:\n                for data in source:\n                    length = len(data._sequence.qubit_info)\n                    if size_max is None or size_max &lt;= length:\n                        size_max = length\n\n        # Note: At this stage, size_max could theoretically still be `None``, if both `X1` and `X2`\n        # are empty. In such cases, `dist_excitation` will never be called, so we're ok.\n\n        mu = float(self.params[\"mu\"])\n        feat_rows = [row.dist_excitation(size_max) for row in X1]\n\n        if X2 is None:\n            # Fast path:\n            # - rows and columns are identical, so no need to compute a `feat_cols`;\n            # - the matrix is symmetric, we only need to compute half of it.\n            #\n            # We could avoid computing kernel[i, i], as we know that it's always 1,\n            # but we do not perform this specific optimization, as it is a useful\n            # canary to detect some bugs.\n            kernel = np.zeros([len(X1), len(X1)])\n            for i, dist_row in enumerate(feat_rows):\n                for j in range(i, len(feat_rows)):\n                    dist_col = feat_rows[j]\n                    js = jensenshannon(dist_row, dist_col) ** 2\n                    similarity = np.exp(-mu * js)\n                    kernel[i, j] = similarity\n                    if j != i:\n                        kernel[j, i] = similarity\n        else:\n            # Slow path:\n            # - we need to compute a `feat_columns`\n            # - the matrix is generally not symmetric and diagonal entries are generally not 1.\n            kernel = np.zeros([len(X1), len(X2)])\n            feat_columns = [col.dist_excitation(size_max) for col in X2]\n            for i, dist_row in enumerate(feat_rows):\n                for j, dist_col in enumerate(feat_columns):\n                    js = jensenshannon(dist_row, dist_col) ** 2\n                    kernel[i, j] = np.exp(-mu * js)\n        return kernel\ndocs\n    def similarity(self, graph_1: ProcessedData, graph_2: ProcessedData) -&gt; float:\n        \"\"\"Compute the similarity between two graphs using Jensen-Shannon\n        divergence.\n\n        This method computes the square of the Jensen-Shannon divergence (JSD)\n        between two probability distributions over bitstrings. The JSD is a\n        measure of the difference between two probability distributions, and it\n        can be used as a kernel for machine learning algorithms that require a\n        similarity function.\n\n        The input graphs are assumed to have been processed using the\n        ProcessedData class from qek_os.data_io.dataset. Parameter `size_max`\n        controls the maximum length of the bitstrings considered in the\n        computation.\n        Args:\n            graph_1 (ProcessedData): First graph.\n            graph_2 (ProcessedData): Second graph.\n            size_max (int, optional): Maximum length of bitstrings to\n            consider. Defaults to all.\n\n        Returns:\n            float: Similarity between the two graphs, scaled by a factor that\n            depends on mu.\n\n        Notes:\n            The JSD is computed using the jensenshannon function from\n            `scipy.spatial.distance`, and it is squared because scipy function\n            `jensenshannon` outputs the distance instead of the divergence.\n        \"\"\"\n        matrix = self([graph_1], [graph_2])\n        return float(matrix[0, 0])\ndocs\n    def fit(self, X: Sequence[ProcessedData], y: list | None = None) -&gt; None:\n        \"\"\"Fit the kernel to the training dataset by storing the dataset.\n\n        Args:\n            X (Sequence[ProcessedData]): The training dataset.\n            y: list: Target variable for the dataset sequence.\n                This argument is ignored, provided only for compatibility\n                with machine-learning libraries.\n        \"\"\"\n        self.X = X\n        self.kernel_matrix = self.create_train_kernel_matrix(self.X)\ndocs\n    def transform(self, X_test: Sequence[ProcessedData], y_test: list | None = None) -&gt; np.ndarray:\n        \"\"\"Transform the dataset into the kernel space with respect to the training dataset.\n\n        Args:\n            X_test (Sequence[ProcessedData]): The dataset to transform.\n            y_test: list: Target variable for the dataset sequence.\n                This argument is ignored, provided only for compatibility\n                with machine-learning libraries.\n        Returns:\n            np.ndarray: Kernel matrix where each entry represents the similarity between\n                        the given dataset and the training dataset.\n        \"\"\"\n        if self.X is None:\n            raise ValueError(\"The kernel must be fit to a training dataset before transforming.\")\n\n        return self.create_test_kernel_matrix(X_test, self.X)\ndocs\n    def fit_transform(self, X: Sequence[ProcessedData], y: list | None = None) -&gt; np.ndarray:\n        \"\"\"Fit the kernel to the training dataset and transform it.\n\n        Args:\n            X (Sequence[ProcessedData]): The dataset to fit and transform.\n            y: list: Target variable for the dataset sequence.\n                This argument is ignored, provided only for compatibility\n                with machine-learning libraries.\n        Returns:\n            np.ndarray: Kernel matrix for the training dataset.\n        \"\"\"\n        self.fit(X)\n        return self.kernel_matrix\ndocs\n    def create_train_kernel_matrix(self, train_dataset: Sequence[ProcessedData]) -&gt; np.ndarray:\n        \"\"\"Compute a kernel matrix for a given training dataset.\n\n        This method computes a symmetric N x N kernel matrix from the\n        Jensen-Shannon divergences between all pairs of graphs in the input\n        dataset. The resulting matrix can be used as a similarity metric for\n        machine learning algorithms.\n        Args:\n            train_dataset (Sequence[ProcessedData]): A list of ProcessedData\n            objects to compute the kernel matrix from.\n        Returns:\n            np.ndarray: An N x N symmetric matrix where the entry at row i and\n            column j represents the similarity between the graphs in positions\n            i and j of the input dataset.\n        \"\"\"\n        return self(train_dataset)\n\n    def create_test_kernel_matrix(docs\n        self,\n        test_dataset: Sequence[ProcessedData],\n        train_dataset: Sequence[ProcessedData],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Compute a kernel matrix for a given testing dataset and training\n        set.\n\n        This method computes an N x M kernel matrix from the Jensen-Shannon\n        divergences between all pairs of graphs in the input testing dataset\n        and the training dataset.\n        The resulting matrix can be used as a similarity metric for machine\n        learning algorithms,\n        particularly when evaluating the performance on the test dataset using\n        a trained model.\n        Args:\n            test_dataset (Sequence[ProcessedData]): A list of ProcessedData\n            objects representing the testing dataset.\n            train_dataset (Sequence[ProcessedData]): A list of ProcessedData\n            objects representing the training set.\n        Returns:\n            np.ndarray: An M x N matrix where the entry at row i and column j\n            represents the similarity between the graph in position i of the\n            test dataset and the graph in position j of the training set.\n        \"\"\"\n        return self(test_dataset, train_dataset)\n\n    def set_params(self, **kwargs: dict[str, Any]) -&gt; None:docs\n        \"\"\"Set multiple parameters for the kernel.\n\n        Args:\n            **kwargs: Arbitrary keyword dictionary where keys are attribute names\n            and values are their respective values\n        \"\"\"\n        for key, value in kwargs.items():\n            self.params[key] = value\n\n    def get_params(self, deep: bool = True) -&gt; dict:docs\n        \"\"\"Retrieve the value of all parameters.\n\n         Args:\n            deep (bool): Ignored for the time being. Added for compatibility with\n                various machine learning libraries, such as scikit-learn.\n\n        Returns\n            dict: A dictionary of parameters and their respective values.\n                Note that this method always performs a copy of the dictionary.\n        \"\"\"\n        return copy.deepcopy(self.params)\n\n\ndef count_occupation_from_bitstring(bitstring: str) -&gt; int:docs\n    \"\"\"Counts the number of '1' bits in a binary string.\n\n    Args:\n        bitstring (str): A binary string containing only '0's and '1's.\n\n    Returns:\n        int: The number of '1' bits found in the input string.\n    \"\"\"\n    return sum(int(bit) for bit in bitstring)\n\n\ndef dist_excitation_and_vec(docs\n    count_bitstring: dict[str, int], size_max: int | None = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the distribution of excitation energies from a dictionary of\n    bitstrings to their respective counts.\n\n    Args:\n        count_bitstring (dict[str, int]): A dictionary mapping binary strings\n            to their counts.\n        size_max (int | None): If specified, only keep `size_max` energy\n            distributions in the output. Otherwise, keep all values.\n\n    Returns:\n        np.ndarray: A NumPy array where keys are the number of '1' bits\n            in each binary string and values are the normalized counts.\n    \"\"\"\n\n    if len(count_bitstring) == 0:\n        raise ValueError(\"The input counter is empty\")\n\n    if size_max is None:\n        # If size is not specified, it's the length of bitstrings.\n        # We assume that all bitstrings in `count_bitstring` have the\n        # same length and we have just checked that it's not empty.\n\n        # Pick the length of the first bitstring.\n        # We have already checked that `count_bitstring` is not empty.\n        bitstring = next(iter(count_bitstring.keys()))\n        size_max = len(bitstring)\n\n    # Make mypy realize that `size_max` is now always an `int`.\n    assert type(size_max) is int\n\n    count_occupation: dict[int, int] = collections.defaultdict(int)\n    total = 0.0\n    for k, v in count_bitstring.items():\n        occupation = count_occupation_from_bitstring(k)\n        count_occupation[occupation] += v\n        total += v\n\n    numpy_vec = np.zeros(size_max + 1, dtype=float)\n    for occupation, count in count_occupation.items():\n        if occupation &lt; size_max:\n            numpy_vec[occupation] = count / total\n\n    return numpy_vec\n</code></pre>"},{"location":"api/qek/shared/","title":"qek.shared","text":"qek.shared<p> source package qek.shared </p> <p>Shared utility code.</p> <p> Modules </p> <ul> <li> <p>qek.shared.error \u2014 Exceptions raised within this library.</p> </li> </ul>"},{"location":"src/qek/shared/","title":"qek.shared","text":"qek.shared<p> docs package qek.shared </p> <pre><code>\"\"\"\nShared utility code.\n\"\"\"\n</code></pre>"},{"location":"api/qek/shared/error/","title":"qek.shared.error","text":"qek.shared.error<p> source module qek.shared.error </p> <p>Exceptions raised within this library.</p> <p> Classes </p> <ul> <li> <p>CompilationError \u2014 An error raised when attempting to compile a graph for an architecture that does not support it, e.g. because it requires too many qubits or because the physical constraints on the geometry are not satisfied.</p> </li> </ul> <p> source class CompilationError() </p> <p><p>Bases : Exception</p></p> <p>An error raised when attempting to compile a graph for an architecture that does not support it, e.g. because it requires too many qubits or because the physical constraints on the geometry are not satisfied.</p>"},{"location":"src/qek/shared/error/","title":"qek.shared.error","text":"qek.shared.error<p> docs module qek.shared.error </p> <pre><code>\"\"\"\nExceptions raised within this library.\n\"\"\"\n\n\nclass CompilationError(Exception):docs\n    \"\"\"\n    An error raised when attempting to compile a graph for an architecture\n    that does not support it, e.g. because it requires too many qubits or\n    because the physical constraints on the geometry are not satisfied.\n    \"\"\"\n</code></pre>"},{"location":"api/qek/backends/","title":"qek.backends","text":"qek.backends<p> source module qek.backends </p> <p>Low-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.</p> <p> Classes </p> <ul> <li> <p>BaseBackend \u2014 Low-level abstraction to execute a Register and a Pulse on a Quantum Device.</p> </li> <li> <p>QutipBackend \u2014 Execute a Register and a Pulse on the Qutip Emulator.</p> </li> <li> <p>BaseRemoteBackend \u2014 Base hierarch for remote backends.</p> </li> <li> <p>RemoteQPUBackend \u2014 Execute on a remote QPU.</p> </li> <li> <p>RemoteEmuMPSBackend \u2014 A backend that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud.</p> </li> <li> <p>EmuMPSBackend \u2014 Execute a Register and a Pulse on the high-performance emu-mps Emulator.</p> </li> </ul> <p> source class BaseBackend(device: Device | None) </p> <p><p>Bases : abc.ABC</p></p> <p>Low-level abstraction to execute a Register and a Pulse on a Quantum Device.</p> <p>For higher-level abstractions, see <code>BaseExtractor</code> and its subclasses.</p> <p>The sole role of these abstractions is to provide the same API for all backends. They might be removed in a future version, once Pulser has gained a similar API.</p> <p> Methods </p> <ul> <li> <p>run \u2014 Execute a register and a pulse.</p> </li> </ul> <p> source async method BaseBackend.run(self, register: Register, pulse: Pulse) \u2192 dict[str, int] </p> <p>Execute a register and a pulse.</p> <p> Returns </p> <ul> <li> <p>dict[str, int] \u2014 A bitstring counter, i.e. a data structure counting for each bitstring the number of instances of this bitstring observed at the end of runs.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source class QutipBackend(device: Device) </p> <p><p>Bases : BaseBackend</p></p> <p>Execute a Register and a Pulse on the Qutip Emulator.</p> <p>Please consider using EmuMPSBackend, which generally works much better with higher number of qubits.</p> <p> Performance warning </p> <p>Executing anything quantum related on an emulator takes an amount of resources polynomial in 2^N, where N is the number of qubits. This can easily go beyond the limit of the computer on which you're executing it.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method QutipBackend.run(self, register: Register, pulse: Pulse) \u2192 dict[str, int] </p> <p> source class BaseRemoteBackend(project_id: str, username: str, device_name: str = 'FRESNEL', password: str | None = None) </p> <p><p>Bases : BaseBackend</p></p> <p>Base hierarch for remote backends.</p> <p>Create a remote backend</p> <p> Performance warning </p> <p>As of this writing, using remote Backends to access a remote QPU or remote emulator is slower than using a RemoteExtractor, as the RemoteExtractor optimizes the number of connections used to communicate with the cloud server.</p> <p> Parameters </p> <ul> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>device \u2014 Make sure that we have fetched the latest specs for the device from the server.</p> </li> </ul> <p> source async method BaseRemoteBackend.device(self) \u2192 Device </p> <p>Make sure that we have fetched the latest specs for the device from the server.</p> <p> source class RemoteQPUBackend() </p> <p><p>Bases : BaseRemoteBackend</p></p> <p>Execute on a remote QPU.</p> <p> Performance note </p> <p>As of this writing, the waiting lines for a QPU may be very long. You may use this Extractor to resume your workflow with a computation that has been previously started.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method RemoteQPUBackend.run(self, register: Register, pulse: Pulse) \u2192 dict[str, int] </p> <p> source class RemoteEmuMPSBackend() </p> <p><p>Bases : BaseRemoteBackend</p></p> <p>A backend that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method RemoteEmuMPSBackend.run(self, register: Register, pulse: Pulse, dt: int = 10) \u2192 dict[str, int] </p> <p> source class EmuMPSBackend(device: Device) </p> <p><p>Bases : BaseBackend</p></p> <p>Execute a Register and a Pulse on the high-performance emu-mps Emulator.</p> <p>As of this writing, this local emulator is only available under Unix. However, the RemoteEmuMPSBackend is available on all platforms.</p> <p> Performance warning </p> <p>Executing anything quantum related on an emulator takes an amount of resources polynomial in 2^N, where N is the number of qubits. This can easily go beyond the limit of the computer on which you're executing it.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method EmuMPSBackend.run(self, register: Register, pulse: Pulse, dt: int = 10) \u2192 dict[str, int] </p>"},{"location":"src/qek/backends/","title":"qek.backends","text":"qek.backends<p> docs module qek.backends </p> <pre><code>\"\"\"\nLow-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.\n\"\"\"\n\nimport abc\nimport asyncio\nfrom math import ceil\nfrom typing import Counter, cast\n\nimport os\nfrom pasqal_cloud import SDK\nfrom pasqal_cloud.device import BaseConfig, EmulatorType\nfrom pasqal_cloud.job import Job\nfrom pulser import Pulse, Register, Sequence\nfrom pulser.devices import Device\nfrom pulser_simulation import QutipEmulator\n\nfrom qek.data.extractors import deserialize_device\nfrom qek.shared.error import CompilationError\nfrom qek.shared._utils import make_sequence\n\n\nclass BaseBackend(abc.ABC):docs\n    \"\"\"\n    Low-level abstraction to execute a Register and a Pulse on a Quantum Device.\n\n    For higher-level abstractions, see `BaseExtractor` and its subclasses.\n\n    The sole role of these abstractions is to provide the same API for all backends.\n    They might be removed in a future version, once Pulser has gained a similar API.\n    \"\"\"\n\n    def __init__(self, device: Device | None):\n        self._device = device\n\n    def _make_sequence(self, register: Register, pulse: Pulse) -&gt; Sequence:\n        assert self._device is not None\n        return make_sequence(register=register, pulse=pulse, device=self._device)\n\n    @abc.abstractmethoddocs\n    async def run(self, register: Register, pulse: Pulse) -&gt; dict[str, int]:\n        \"\"\"\n        Execute a register and a pulse.\n\n        Returns:\n            A bitstring counter, i.e. a data structure counting for each bitstring\n            the number of instances of this bitstring observed at the end of runs.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass QutipBackend(BaseBackend):docs\n    \"\"\"\n    Execute a Register and a Pulse on the Qutip Emulator.\n\n    Please consider using EmuMPSBackend, which generally works much better with\n    higher number of qubits.\n\n    Performance warning:\n        Executing anything quantum related on an emulator takes an amount of resources\n        polynomial in 2^N, where N is the number of qubits. This can easily go beyond\n        the limit of the computer on which you're executing it.\n    \"\"\"\n\n    def __init__(self, device: Device):\n        super().__init__(device)\ndocs\n    async def run(self, register: Register, pulse: Pulse) -&gt; dict[str, int]:\n        sequence = self._make_sequence(register=register, pulse=pulse)\n        emulator = QutipEmulator.from_sequence(sequence)\n        result: Counter[str] = emulator.run().sample_final_state()\n        return result\n\n\nclass BaseRemoteBackend(BaseBackend):docs\n    \"\"\"\n    Base hierarch for remote backends.\n\n    Performance warning:\n        As of this writing, using remote Backends to access a remote QPU or remote emulator\n        is slower than using a RemoteExtractor, as the RemoteExtractor optimizes the number\n        of connections used to communicate with the cloud server.\n    \"\"\"\n\n    def __init__(\n        self,\n        project_id: str,\n        username: str,\n        device_name: str = \"FRESNEL\",\n        password: str | None = None,\n    ):\n        \"\"\"\n        Create a remote backend\n\n        Args:\n            project_id: The ID of the project on the Pasqal Cloud API.\n            username: Your username on the Pasqal Cloud API.\n            password: Your password on the Pasqal Cloud API. If you leave\n                this to None, you will need to enter your password manually.\n            device_name: The name of the device to use. As of this writing,\n                the default value of \"FRESNEL\" represents the latest QPU\n                available through the Pasqal Cloud API.\n        \"\"\"\n        self.device_name = device_name\n        self._sdk = SDK(username=username, project_id=project_id, password=password)\n        self._max_runs = 500\n        self._sequence = None\n        self._device = None\n\n    async def device(self) -&gt; Device:docs\n        \"\"\"\n        Make sure that we have fetched the latest specs for the device from the server.\n        \"\"\"\n        if self._device is not None:\n            return self._device\n\n        # Fetch the latest list of QPUs\n        # Implementation note: Currently sync, hopefully async in the future.\n        specs = self._sdk.get_device_specs_dict()\n        self._device = cast(Device, deserialize_device(specs[self.device_name]))\n\n        # As of this writing, the API doesn't support runs longer than 500 jobs.\n        # If we want to add more runs, we'll need to split them across several jobs.\n        if isinstance(self._device.max_runs, int):\n            self._max_runs = self._device.max_runs\n\n        return self._device\n\n    async def _run(\n        self,\n        register: Register,\n        pulse: Pulse,\n        emulator: EmulatorType | None,\n        config: BaseConfig | None = None,\n        sleep_sec: int = 2,\n    ) -&gt; Job:\n        \"\"\"\n        Run the pulse + register.\n\n        Arguments:\n            register: A register to run.\n            pulse: A pulse to execute.\n            emulator: The emulator to use, or None to run on a QPU.\n            config: The backend-specific config.\n            sleep_sec (optional): The amount of time to sleep when waiting for the remote server to respond, in seconds. Defaults to 2.\n\n        Raises:\n            CompilationError: If the register/pulse may not be executed on this device.\n        \"\"\"\n        device = await self.device()\n        try:\n            sequence = Sequence(register=register, device=device)\n            sequence.declare_channel(\"ising\", \"rydberg_global\")\n            sequence.add(pulse=pulse, channel=\"ising\")\n\n            self._sequence = sequence\n        except ValueError as e:\n            raise CompilationError(f\"This register/pulse cannot be executed on the device: {e}\")\n\n        # Enqueue execution.\n        batch = self._sdk.create_batch(\n            serialized_sequence=sequence.to_abstract_repr(),\n            jobs=[{\"runs\": self._max_runs}],\n            wait=False,\n            emulator=emulator,\n            configuration=config,\n        )\n\n        # Wait for execution to complete.\n        while True:\n            await asyncio.sleep(sleep_sec)\n            # Currently sync, hopefully async in the future.\n            batch.refresh()\n            if batch.status in {\"PENDING\", \"RUNNING\"}:\n                # Continue waiting.\n                continue\n            job = next(iter(batch.jobs.values()))\n            if job.status == \"ERROR\":\n                raise Exception(f\"Error while executing remote job: {job.errors}\")\n            return job\n\n\nclass RemoteQPUBackend(BaseRemoteBackend):docs\n    \"\"\"\n    Execute on a remote QPU.\n\n    Performance note:\n        As of this writing, the waiting lines for a QPU\n        may be very long. You may use this Extractor to resume your workflow\n        with a computation that has been previously started.\n    \"\"\"\ndocs\n    async def run(self, register: Register, pulse: Pulse) -&gt; dict[str, int]:\n        job = await self._run(register, pulse, emulator=None, config=None)\n        return cast(dict[str, int], job.result)\n\n\nclass RemoteEmuMPSBackend(BaseRemoteBackend):docs\n    \"\"\"\n    A backend that uses a remote high-performance emulator (EmuMPS)\n    published on Pasqal Cloud.\n    \"\"\"\ndocs\n    async def run(self, register: Register, pulse: Pulse, dt: int = 10) -&gt; dict[str, int]:\n        job = await self._run(register, pulse, emulator=EmulatorType.EMU_MPS, config=None)\n        bag = cast(dict[str, dict[int, dict[str, int]]], job.result)\n\n        assert self._sequence is not None\n        cutoff_duration = int(ceil(self._sequence.get_duration() / dt) * dt)\n        return bag[\"bitstring\"][cutoff_duration]\n\n\nif os.name == \"posix\":\n    import emu_mps\n\n    class EmuMPSBackend(BaseBackend):docs\n        \"\"\"\n        Execute a Register and a Pulse on the high-performance emu-mps Emulator.\n\n        As of this writing, this local emulator is only available under Unix. However,\n        the RemoteEmuMPSBackend is available on all platforms.\n\n        Performance warning:\n            Executing anything quantum related on an emulator takes an amount of resources\n            polynomial in 2^N, where N is the number of qubits. This can easily go beyond\n            the limit of the computer on which you're executing it.\n        \"\"\"\n\n        def __init__(self, device: Device):\n            super().__init__(device)\ndocs\n        async def run(self, register: Register, pulse: Pulse, dt: int = 10) -&gt; dict[str, int]:\n            sequence = self._make_sequence(register=register, pulse=pulse)\n            backend = emu_mps.MPSBackend()\n\n            # Configure observable.\n            cutoff_duration = int(ceil(sequence.get_duration() / dt) * dt)\n            observable = emu_mps.BitStrings(evaluation_times={cutoff_duration})\n            config = emu_mps.MPSConfig(observables=[observable], dt=dt)\n            counter: dict[str, int] = backend.run(sequence, config)[observable.name][\n                cutoff_duration\n            ]\n            return counter\n</code></pre>"}]}