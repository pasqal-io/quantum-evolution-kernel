{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\"Quantum Evolution Kernel\"","text":"<p>The Quantum Evolution Kernel is a Python library designed for the machine learning community to help users design quantum-driven similarity metrics for graphs and to use them inside kernel-based machine learning algorithms for graph data.</p> <p>The core of the library is focused on the development of a classification algorithm for molecular-graph dataset as it is presented in the published paper Quantum feature maps for graph machine learning on a neutral atom quantum processor.</p> <p>Users setting their first steps into quantum computing will learn how to implement the core algorithm in a few simple steps and run it using the Pasqal Neutral Atom QPU. More experienced users will find this library to provide the right environment to explore new ideas - both in terms of methodologies and data domain - while always interacting with a simple and intuitive QPU interface.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>We provide several tutorials:</p> <ul> <li>tutorial 1 - Using a Quantum Device to Extract Machine-Learning Features</li> <li>tutorial 2 - Machine-Learning with the Quantum EvolutionKernel</li> </ul>"},{"location":"#getting-in-touch","title":"Getting in touch","text":"<ul> <li>Pasqal Community Portal (forums, chat, tutorials, examples, code library).</li> <li>GitHub Repository (source code, issue tracker).</li> <li>Professional Support (if you need tech support, custom licenses, a variant of this library optimized for your workload, your own QPU, remote access to a QPU, ...)</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>The GitHub repository is open for contributions!</p> <p>Don't forget to read the Contributor License Agreement.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/","title":"Contributor Agreement","text":"<p>PASQAL CONTRIBUTION AGREEMENT (the \u201cAgreement\u201d)</p> <p>The author of the License is:   Pasqal, a Soci\u00e9t\u00e9 par Actions Simplifi\u00e9e (Simplified Joint Stock Company) registered under number 849 441 522 at the Registre du commerce et des soci\u00e9t\u00e9s (Trade and Companies Register) of Evry \u2013 France, headquartered at 24 rue \u00c9mile Baudot \u2013 91120 \u2013 Palaiseau \u2013 France, duly represented by its Pr\u00e9sident, M. Georges-Olivier REYMOND, Hereafter referred to as \u00ab the Licensor \u00bb</p> <p>In the course of its activities, Pasqal carries out and leads quantic projects, in their software components. These projects aim to bring together a community of like-minded individuals to contribute to the development and improvement of Pasqal\u2019s products. Pasqal clearly outlines which projects are open to contributions (\u201cProjects\u201d).</p> <p>This Agreement documents the rights granted by Contributors to Pasqal and is legally binding.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-1-definitions","title":"Article 1 Definitions","text":"<p>You, Your or Contributor : means the intellectual property rights owner or legal entity authorized by the intellectual property rights owner that is entering into this Agreement with\u00a0Pasqal</p> <p>Contribution : means any work, protected or not, that is submitted by You to Pasqal in which You own or assert ownership of the intellectual property rights, subject to proprietary licensing terms, and not otherwise distributer through an open-source license.</p> <p>Material : means the work object of the Project, made available by Pasqal to third parties. When this Agreement covers more than one Project, the Material means the work to which the Contribution was Submitted. After You Submit the Contribution, it may be included in any type of Material.</p> <p>Submit : means any form of electronic, verbal, or written communication sent to Pasqal or its representatives, including but not limited to electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Pasqal for the purpose of discussing and improving the Material.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-2-entering-into-this-agreement","title":"Article 2 Entering into this Agreement","text":"<p>By Submitting any Contribution to Pasqal, You agree to enter into the Agreement with this entity, and be bound by the following terms.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-3-limitations-as-to-the-licensing-of-contributions","title":"Article 3 Limitations as to the licensing of Contributions","text":"<p>You guarantee that any Contribution You Submit to any Project is not licensed under any type of contaminating (even weakly contaminating) license, and that Pasqal will be free to combine your Contribution into any type of Material, for any Project, without any risk of contamination of said Material and/or Project.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-4-granting-of-a-copyright-andor-authors-rights-license","title":"Article 4 Granting of a Copyright and/or author\u2019s rights License","text":"<p>Subject to the terms and conditions of this Agreement, You hereby grant to Pasqal and to recipients of any Project distributed by Pasqal a worldwide, sublicensable, non-exclusive, royalty-free license to reproduce, prepare derivative works of, publicly display, publicly perform, and distribute Your Contributions and your Contribution in combination with the Material for the entire duration of the rights under Applicable law.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-5-granting-of-a-patent-license","title":"Article 5 Granting of a patent license","text":"<p>Subject to the terms and conditions of this Agreement, You grant Pasqal a, worldwide, non-exclusive, transferable, royalty free patent license, with the right to sublicense these rights to third parties, to make, have made, use, sell, offer for sale, import and otherwise  transfer the Contribution and the Contribution in combination with the Material for the entire duration of the rights under Applicable law, if the Contribution implies the implementation of any such patent.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-6-licensing-of-the-material-and-contribution","title":"Article 6 Licensing of the Material and Contribution","text":"<p>Based on the rights granted in articles 2 and 3, if Pasqal includes Your Contribution in a Material, Pasqal may license Material including Your Contribution under any license, whether permissive, weakly contaminating, or contaminating. Pasqal will respect Your moral rights in relation to Your Contribution as provided under the Applicable law.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-7-warranties-and-disclaimer","title":"Article 7 Warranties and disclaimer","text":"<p>You represent that each of Your Contributions is Your creation, or that you have obtained the authorization from the intellectual property rights owner on the Contribution to Submit it over the course of a Project.</p> <p>By making a Contribution, You confirm that, to the best of Your knowledge, the Contribution does not violate the rights of any person or entity. If You make a Contribution on behalf of Your employer, then You confirm that an appropriate representative of that employer has authorized the inclusion of such Contribution to a Project and that it meets these requirements.</p> <p>You acknowledge that Pasqal is not obligated to use Your Contribution as part of the Material and may decide to only include Contributions Pasqal considers appropriate.</p>"},{"location":"CONTRIBUTOR%20AGREEMENT/#article-8-applicable-law-and-jurisdiction","title":"Article 8 Applicable law and jurisdiction","text":"<p>The Agreement is governed by the laws of France. Any dispute relating to the interpretation or application of the License shall be subject to best efforts for an amicable settlement. Any dispute relating to the License, notably its execution, performance and/or termination shall be brought to, heard and tried by the Tribunal Judiciaire de Paris, regardless of the rules of jurisdiction in the matter.</p>"},{"location":"LICENSE/","title":"Licence (MIT-Derived)","text":"<p>PASQAL OPEN-SOURCE SOFTWARE LICENSE AGREEMENT (MIT-derived)</p> <p>The author of the License is:   Pasqal, a Soci\u00e9t\u00e9 par Actions Simplifi\u00e9e (Simplified Joint Stock Company) registered under number 849 441 522 at the Registre du commerce et des soci\u00e9t\u00e9s (Trade and Companies Register) of Evry \u2013 France, headquartered at 24 rue \u00c9mile Baudot \u2013 91120 \u2013 Palaiseau \u2013 France, duly represented by its Pr\u00e9sident, M. Georges-Olivier REYMOND, Hereafter referred to as \u00ab the Licensor \u00bb</p> <ul> <li> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software (the \u201cLicensee\u201d) and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. The Software is \u201cas is\u201d, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and non-infringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise arising from, out of or in connection with the Software or the use or other dealings in the Software.</p> </li> <li> <p>If use of the Software leads to the necessary use of any patent of the Licensor and/or any of its Affiliates (defined as a company owned or controlled by the Licensor), the Licensee is granted a royalty-free license, in any country where such patent is in force, to use the object of such patent; or use the process covered by such patent,</p> </li> <li> <p>Such a patent license is granted for internal research or academic use of the Licensee's, which includes use by employees and students of the Licensee, acting on behalf of the Licensee, for research purposes only.</p> </li> <li> <p>The License is governed by the laws of France. Any dispute relating to the License, notably its execution, performance and/or termination shall be brought to, heard and tried by the Tribunal Judiciaire de Paris, regardless of the rules of jurisdiction in the matter.</p> </li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#using-hatch-uv-or-any-pyproject-compatible-python-manager","title":"Using <code>hatch</code>, <code>uv</code> or any pyproject-compatible Python manager","text":"<p>Edit file <code>pyproject.toml</code> to add the line</p> <pre><code>  \"quantum-evolution-kernel\"\n</code></pre> <p>to the list of <code>dependencies</code>.</p>"},{"location":"installation/#using-pip-or-pipx","title":"Using <code>pip</code> or <code>pipx</code>","text":"<p>To install the <code>pipy</code> package using <code>pip</code> or <code>pipx</code></p> <ol> <li>Create a <code>venv</code> if that's not done yet</li> </ol> <pre><code>$ python -m venv venv\n</code></pre> <ol> <li>Enter the venv</li> </ol> <pre><code>$ . venv/bin/activate\n</code></pre> <ol> <li>Install the package</li> </ol> <pre><code>$ pip install quantum-evolution-kernel\n# or\n$ pipx install quantum-evolution-kernel\n</code></pre>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/","title":"Tutorial 1: using a Quantum Device to extract machine-learning features","text":"In\u00a0[2]: Copied! <pre>import torch_geometric.datasets as pyg_dataset\n\nfrom qek.shared.retrier import PygRetrier\n\n# Load the original PTC-FM dataset.\n# We use PygRetrier to retry the download if it fails.\nog_ptcfm = PygRetrier().insist(pyg_dataset.TUDataset, root=\"dataset\", name=\"PTC_FM\")\n\ndisplay(\"Loaded %s samples\" % (len(og_ptcfm), ))\n</pre> import torch_geometric.datasets as pyg_dataset  from qek.shared.retrier import PygRetrier  # Load the original PTC-FM dataset. # We use PygRetrier to retry the download if it fails. og_ptcfm = PygRetrier().insist(pyg_dataset.TUDataset, root=\"dataset\", name=\"PTC_FM\")  display(\"Loaded %s samples\" % (len(og_ptcfm), )) <pre>'Loaded 349 samples'</pre> <p>To extract machine-learning features from our dataset, we will need to configure a feature extractor. This library provides several feature extractors to either make use of a physical quantum device (QPU), or a variety of emulators.</p> <p>To configure a feature extractor, we will need to provide a compiler, whose task is to take a list of graphs, extract embeddings and compile these embeddings to sequences of pulses, the format that can be executed  by either a QPU or an emulator. For this tutorial, our dataset is composed of molecule graphs following the PTCFM conventions, so we will use the <code>PTCFMCompiler</code>:</p> In\u00a0[3]: Copied! <pre>import qek.data.graphs as qek_graphs\n\ncompiler = qek_graphs.PTCFMCompiler()\n</pre> import qek.data.graphs as qek_graphs  compiler = qek_graphs.PTCFMCompiler() <p>This library also provides other compilers from a variety of graph formats.</p> In\u00a0[4]: Copied! <pre>from pathlib import Path\nimport qek.data.extractors as qek_extractors\n\n# Use the Qutip Extractor.\nextractor = qek_extractors.QutipExtractor(\n    # Once computing is complete, data will be saved in this file.\n    path=Path(\"saved_data.json\"),\n    compiler=compiler\n)\n\n# Add the graphs using the compiler we've picked previously.\nextractor.add_graphs(graphs=og_ptcfm)\n\n# We may now compile them.\ncompiled = extractor.compile()\ndisplay(\"Compiled %s sequences\" % (len(compiled), ))\n</pre> from pathlib import Path import qek.data.extractors as qek_extractors  # Use the Qutip Extractor. extractor = qek_extractors.QutipExtractor(     # Once computing is complete, data will be saved in this file.     path=Path(\"saved_data.json\"),     compiler=compiler )  # Add the graphs using the compiler we've picked previously. extractor.add_graphs(graphs=og_ptcfm)  # We may now compile them. compiled = extractor.compile() display(\"Compiled %s sequences\" % (len(compiled), )) <pre>'Compiled 272 sequences'</pre> <p>As you can see, the number of sequences compiled is lower than the number of samples loaded. Some of this is due to limitations within the algorithm (not all graphs can be efficiently laid out for execution on a Quantum Device), while others are due to the limitations of the emulator we target (which at the time of this writing is limited to ~50 qubits).</p> <p>We may now run the extraction on the emulator:</p> In\u00a0[5]: Copied! <pre># Limit the number of qubits for this run, for performance reasons.\n# You can increase this value to higher number of qubits, but this\n# notebook will take longer to execute and may run out of memory.\n#\n# On our test computer, the practical limit is around 10 qubits.\nmax_qubits = 5\nprocessed_dataset = extractor.run(max_qubits=max_qubits).processed_data\ndisplay(\"Extracted features from %s samples\"% (len(processed_dataset), ))\n</pre> # Limit the number of qubits for this run, for performance reasons. # You can increase this value to higher number of qubits, but this # notebook will take longer to execute and may run out of memory. # # On our test computer, the practical limit is around 10 qubits. max_qubits = 5 processed_dataset = extractor.run(max_qubits=max_qubits).processed_data display(\"Extracted features from %s samples\"% (len(processed_dataset), )) <pre>'Extracted features from 40 samples'</pre> <p>If you wish to extract features from more samples, feel free to increase the value of <code>MAX_QUBITS</code> above. However, you will soon run into limitations of a quantum emulator, and possibly crash this notebook. At this point, you have other options, such as using <code>EmuMPSExtractor</code> instead of <code>QutipExtractor</code>, a more recent emulator that features much better performance in most cases, or you can run the extraction on a physical QPU.</p> In\u00a0[6]: Copied! <pre>from pathlib import Path\n\n\nHAVE_PASQAL_ACCOUNT = False # If you have a PASQAL Cloud account, fill in the details and set this to `True`.\n\nif HAVE_PASQAL_ACCOUNT:\n    # Use the QPU Extractor.\n    extractor = qek_extractors.RemoteQPUExtractor(\n        # Once computing is complete, data will be saved in this file.\n        path=Path(\"saved_data.json\"),\n        compiler = compiler,\n        project_id = \"XXXX\", # Replace this with your project id on the PASQAL Cloud\n        username = \"XXX\",    # Replace this with your username on PASQAL Cloud\n        password = None,     # Replace this with your password on PASQAL Cloud or enter it on the command-line\n    )\n\n    # Add the graphs, exactly as above.\n    extractor.add_graphs(graphs=og_ptcfm)\n\n    # We may now compile, exactly as above.\n    compiled = extractor.compile()\n    display(\"Compiled %s sequences\" % (len(compiled), ))\n\n    # Launch the execution.\n    extracted = extractor.run()\n    display(\"Work enqueued with ids %s\" % (extractor.batch_ids, ))\n\n    # ...and wait for the results.\n    await extracted\n    processed_data = extracted.processed_data\n    display(\"Extracted features from %s samples\"% (len(processed_data), ))\n</pre> from pathlib import Path   HAVE_PASQAL_ACCOUNT = False # If you have a PASQAL Cloud account, fill in the details and set this to `True`.  if HAVE_PASQAL_ACCOUNT:     # Use the QPU Extractor.     extractor = qek_extractors.RemoteQPUExtractor(         # Once computing is complete, data will be saved in this file.         path=Path(\"saved_data.json\"),         compiler = compiler,         project_id = \"XXXX\", # Replace this with your project id on the PASQAL Cloud         username = \"XXX\",    # Replace this with your username on PASQAL Cloud         password = None,     # Replace this with your password on PASQAL Cloud or enter it on the command-line     )      # Add the graphs, exactly as above.     extractor.add_graphs(graphs=og_ptcfm)      # We may now compile, exactly as above.     compiled = extractor.compile()     display(\"Compiled %s sequences\" % (len(compiled), ))      # Launch the execution.     extracted = extractor.run()     display(\"Work enqueued with ids %s\" % (extractor.batch_ids, ))      # ...and wait for the results.     await extracted     processed_data = extracted.processed_data     display(\"Extracted features from %s samples\"% (len(processed_data), )) <p>As you can see, the process is essentially identical to executing with an emulator. Note that, as of this writing, the waiting line to access a QPU can be very long (typically several hours).</p> <p>There are two main ways to deal with this:</p> <ol> <li><code>RemoteQPUExtractor</code> can be attached to an ongoing job from batch ids, so that you can resume your work e.g. after turning off your computer.</li> <li>Pasqal CLOUD offers access to high-performance hardware-based emulators, with dramatically shorter waiting lines. For instance, in the snippet above, you may replace <code>RemoteQPUExtractor</code> with <code>RemoteEmuMPSExtractor</code> to use the emu-mps emulator.</li> </ol> <p>See the documentation for more details.</p> In\u00a0[7]: Copied! <pre>import qek.data.processed_data as qek_dataset\nprocessed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\")\nprint(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\")\n</pre> import qek.data.processed_data as qek_dataset processed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\") print(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\") <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[7], line 2\n      1 import qek.data.processed_data as qek_dataset\n----&gt; 2 processed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\")\n      3 print(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\")\n\nFile ~/work/quantum-evolution-kernel/quantum-evolution-kernel/qek/data/processed_data.py:236, in load_dataset(file_path)\n    222 def load_dataset(file_path: str) -&gt; list[ProcessedData]:\n    223     \"\"\"Loads a dataset from a JSON file.\n    224 \n    225     Args:\n   (...)\n    234             the JSON file.\n    235     \"\"\"\n--&gt; 236     with open(file_path) as file:\n    237         data = json.load(file)\n    238         return [\n    239             ProcessedData(\n    240                 sequence=pl.Sequence.from_abstract_repr(item[\"sequence\"]),\n   (...)\n    244             for item in data\n    245         ]\n\nFileNotFoundError: [Errno 2] No such file or directory: 'ptcfm_processed_dataset.json'</pre> <p>We can check the geometry for one of the samples:</p> In\u00a0[8]: Copied! <pre>dataset_example = processed_dataset[64]\ndataset_example.draw_register()\n</pre> dataset_example = processed_dataset[64] dataset_example.draw_register() <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 dataset_example = processed_dataset[64]\n      2 dataset_example.draw_register()\n\nIndexError: list index out of range</pre> In\u00a0[9]: Copied! <pre>dataset_example.draw_pulse()\n</pre> dataset_example.draw_pulse() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 dataset_example.draw_pulse()\n\nNameError: name 'dataset_example' is not defined</pre> <p>The results of executing the embedding on the Quantum Device are in field <code>state_dict</code>:</p> In\u00a0[10]: Copied! <pre>display(dataset_example.state_dict)\nprint(f\"Total number of samples: {sum(dataset_example.state_dict.values())}\")\n</pre> display(dataset_example.state_dict) print(f\"Total number of samples: {sum(dataset_example.state_dict.values())}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 display(dataset_example.state_dict)\n      2 print(f\"Total number of samples: {sum(dataset_example.state_dict.values())}\")\n\nNameError: name 'dataset_example' is not defined</pre> <p>This dictionary represents an approximation of the quantum state of the device for this graph after completion of the algorithm.</p> <ul> <li>each of the keys represents one possible state for the register (which represents the graph), with each qubit (which represents a single node) being in state <code>0</code> or <code>1</code>;</li> <li>the corresponding value is the number of samples observed with this specific state of the register.</li> </ul> <p>In this example, for instance, we can see that the state observed most frequently is <code>10000001010</code>, with 43/1000 samples.</p> <p>Note: Since Quantum Devices are inherently non-deterministic, you will probably obtained different samples if you run this on a Quantum Device instead of loading the dataset.</p> In\u00a0[11]: Copied! <pre>dataset_example.draw_excitation()\n</pre> dataset_example.draw_excitation() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 dataset_example.draw_excitation()\n\nNameError: name 'dataset_example' is not defined</pre> <p>Is this distribution of excitation connected to the toxicity of the molecule? To check this out, we'll need to perform some machine-learning engineering.</p> <p>Note Of course, you could derive features completely unrelated to distribution of excitation.</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#tutorial-1-using-a-quantum-device-to-extract-machine-learning-features","title":"Tutorial 1: using a Quantum Device to extract machine-learning features\u00b6","text":"<p>The companion library is a machine-learning tool that uses a quantum device to predict similarities and properties of graphs. In this tutorial, we will reproduce first part of the QEK paper using the library's high-level API. The high-level goal of this tutorial is to predict toxicity properties of molecules, using quantum machine learning, but of course, the mechanisms involved are much more generic.</p> <p>By the end of this notebook, you will know how to:</p> <ol> <li>Setup import for a molecular dataset (the library supports other type of graphs, of course).</li> <li>Setup compilation and execution of these graphs for execution on a Quantum Device (either an emulator or a physical QPU).</li> <li>Launch the execution and extract the relevant machine-learning features.</li> </ol> <p>A companion notebook will guide you through machine-learning with QEK.</p> <p>If, instead of using the library's high-level API, you prefer digging a bit closer to the qubits, you may prefer the companion low-level notebook that mirrors this notebook, but using a lower-level API that will let you experiment with different quantum pulses.</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#dataset-preparation","title":"Dataset preparation\u00b6","text":"<p>As in any machine learning task, we first need to load and prepare data. QEK can work with many types of graphs, including molecular graphs. For this tutorial, we will use the PTC-FM dataset, which contains such molecular graphs, labeled with their toxicity.</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#creating-and-executing-a-feature-extractor-from-an-emulator","title":"Creating and executing a feature extractor from an emulator\u00b6","text":"<p>The easiest way to process a graph is to compile and execute it for an emulator. QEK is built on top of Pulser, which provides several emulators. The simplest of these emulators is the <code>QutipEmulator</code>, which QEK uses for the <code>QutipExtractor</code>:</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#creating-and-executing-a-feature-extractor-on-a-physical-qpu","title":"Creating and executing a feature extractor on a physical QPU\u00b6","text":"<p>Once you have checked that low qubit sequences provide the results you expect on an emulator, you will generally want to move to a QPU. For this, you will need either physical access to a QPU, or an account with PASQAL Cloud, which provides you remote access to QPUs built and hosted by Pasqal. In this section, we'll see how to use the latter.</p> <p>If you don't have an account, just skip to the next section!</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#or-using-the-provided-dataset","title":"...or using the provided dataset\u00b6","text":"<p>For this notebook, instead of spending hours running the simulator on your computer, we're going to skip this step and load on we're going to cheat and load the results, which are conveniently stored in <code>ptcfm_processed_dataset.json</code>.</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#a-look-at-the-results","title":"A look at the results\u00b6","text":""},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#machine-learning-features","title":"Machine learning-features\u00b6","text":"<p>From the state dictionary, we derive as machine-learning feature the distribution of excitation. We'll use this in a second to define our kernel.</p>"},{"location":"tutorial%201%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features/#what-now","title":"What now?\u00b6","text":"<p>What we have seen so far covers the use of a Quantum Device to extract machine-learning features.</p> <p>For the next step, we'll see how to use these features for actual machine learning.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/","title":"Tutorial 1: using a Quantum Device to extract machine-learning features","text":"In\u00a0[2]: Copied! <pre># Load the original PTC-FM dataset\nimport torch_geometric.datasets as pyg_dataset\nfrom qek.shared.retrier import PygRetrier\n\n# We use PygRetrier to retry the download if it fails.\nog_ptcfm = PygRetrier().insist(pyg_dataset.TUDataset, root=\"dataset\", name=\"PTC_FM\")\n\ndisplay(\"Loaded %s samples\" % (len(og_ptcfm), ))\n</pre> # Load the original PTC-FM dataset import torch_geometric.datasets as pyg_dataset from qek.shared.retrier import PygRetrier  # We use PygRetrier to retry the download if it fails. og_ptcfm = PygRetrier().insist(pyg_dataset.TUDataset, root=\"dataset\", name=\"PTC_FM\")  display(\"Loaded %s samples\" % (len(og_ptcfm), )) <pre>'Loaded 349 samples'</pre> <p>This package lets researchers embed graphs on Analog Quantum Devices. To do this, we need to give these graphs a geometry (their positions in, space) and to confirm that the geometry is compatible with a Quantum Device.</p> <p>This package builds upon the Pulser framework. Our objective, in this notebook, is to compile graphs into the format understood by our Quantum Devices: a Pulser Register (the position of qubits) and Pulser Pulses (the laser impulses controlling the evolution of the analog device).</p> <p>As the geometry depends on the Quantum Device, we need to specify a device to use. For the time being, we'll use Pulser's <code>AnalogDevice</code>, which is a reasonable default device. We'll show you a bit further how to use another device.</p> <p>In this example, our graphs are representations of molecules. To simplify things, we'll use the dedicated class <code>qek.data.graphs.PTCFMGraph</code> that use bio-chemical tools to compute a reasonable geometry from molecular data using the PTCFM conventions for a specific Quantum Device. For other classes of graph, you will need to decide how to compute the geometry and use <code>qek.data.graphs.BaseGraph</code>.</p> In\u00a0[3]: Copied! <pre>from tqdm import tqdm\nimport pulser as pl\nimport qek.data.graphs as qek_graphs\n\n\ngraphs_to_compile = []\n\nfor i, data in enumerate(tqdm(og_ptcfm)):\n    graph = qek_graphs.PTCFMGraph(data=data, device=pl.AnalogDevice, id=i)\n    graphs_to_compile.append(graph)\n</pre> from tqdm import tqdm import pulser as pl import qek.data.graphs as qek_graphs   graphs_to_compile = []  for i, data in enumerate(tqdm(og_ptcfm)):     graph = qek_graphs.PTCFMGraph(data=data, device=pl.AnalogDevice, id=i)     graphs_to_compile.append(graph)  In\u00a0[4]: Copied! <pre>from qek.shared.error import CompilationError\n\ncompiled = [] \n\nfor graph in tqdm(graphs_to_compile):\n    try:\n        register = graph.compile_register()\n        pulse = graph.compile_pulse()\n    except CompilationError:\n        # Let's just skip graphs that cannot be computed.\n        print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))\n        continue\n    compiled.append((graph, register, pulse))\nprint(\"Compiled %s graphs into registers/pulses\" % (len(compiled, )))\n</pre> from qek.shared.error import CompilationError  compiled = []   for graph in tqdm(graphs_to_compile):     try:         register = graph.compile_register()         pulse = graph.compile_pulse()     except CompilationError:         # Let's just skip graphs that cannot be computed.         print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))         continue     compiled.append((graph, register, pulse)) print(\"Compiled %s graphs into registers/pulses\" % (len(compiled, ))) <pre>Graph 1 cannot be compiled for this device\nGraph 16 cannot be compiled for this device\nGraph 23 cannot be compiled for this device\nGraph 25 cannot be compiled for this device\nGraph 26 cannot be compiled for this device\n</pre> <pre>Graph 34 cannot be compiled for this device\nGraph 40 cannot be compiled for this device\nGraph 43 cannot be compiled for this device\nGraph 53 cannot be compiled for this device\nGraph 58 cannot be compiled for this device\nGraph 60 cannot be compiled for this device\nGraph 61 cannot be compiled for this device\nGraph 62 cannot be compiled for this device\nGraph 65 cannot be compiled for this device\nGraph 68 cannot be compiled for this device\n</pre> <pre>Graph 78 cannot be compiled for this device\nGraph 86 cannot be compiled for this device\nGraph 97 cannot be compiled for this device\nGraph 101 cannot be compiled for this device\nGraph 104 cannot be compiled for this device\nGraph 105 cannot be compiled for this device\nGraph 107 cannot be compiled for this device\nGraph 115 cannot be compiled for this device\n</pre> <pre>Graph 117 cannot be compiled for this device\nGraph 118 cannot be compiled for this device\nGraph 122 cannot be compiled for this device\nGraph 126 cannot be compiled for this device\nGraph 127 cannot be compiled for this device\nGraph 128 cannot be compiled for this device\nGraph 129 cannot be compiled for this device\nGraph 132 cannot be compiled for this device\nGraph 135 cannot be compiled for this device\nGraph 144 cannot be compiled for this device\nGraph 155 cannot be compiled for this device\nGraph 157 cannot be compiled for this device\n</pre> <pre>Graph 165 cannot be compiled for this device\nGraph 166 cannot be compiled for this device\nGraph 169 cannot be compiled for this device\nGraph 171 cannot be compiled for this device\nGraph 175 cannot be compiled for this device\nGraph 181 cannot be compiled for this device\nGraph 185 cannot be compiled for this device\nGraph 186 cannot be compiled for this device\nGraph 193 cannot be compiled for this device\nGraph 197 cannot be compiled for this device\n</pre> <pre>Graph 203 cannot be compiled for this device\nGraph 204 cannot be compiled for this device\nGraph 206 cannot be compiled for this device\nGraph 208 cannot be compiled for this device\nGraph 214 cannot be compiled for this device\nGraph 215 cannot be compiled for this device\nGraph 220 cannot be compiled for this device\nGraph 224 cannot be compiled for this device\n</pre> <pre>Graph 238 cannot be compiled for this device\nGraph 239 cannot be compiled for this device\nGraph 243 cannot be compiled for this device\nGraph 244 cannot be compiled for this device\nGraph 245 cannot be compiled for this device\nGraph 246 cannot be compiled for this device\nGraph 247 cannot be compiled for this device\nGraph 259 cannot be compiled for this device\nGraph 260 cannot be compiled for this device\nGraph 264 cannot be compiled for this device\nGraph 268 cannot be compiled for this device\nGraph 269 cannot be compiled for this device\nGraph 270 cannot be compiled for this device\nGraph 273 cannot be compiled for this device\nGraph 278 cannot be compiled for this device\nGraph 279 cannot be compiled for this device\n</pre> <pre>Graph 281 cannot be compiled for this device\nGraph 284 cannot be compiled for this device\nGraph 313 cannot be compiled for this device\n</pre> <pre>Graph 319 cannot be compiled for this device\nGraph 327 cannot be compiled for this device\nGraph 333 cannot be compiled for this device\nGraph 338 cannot be compiled for this device\nGraph 342 cannot be compiled for this device\nCompiled 272 graphs into registers/pulses\n</pre> <p>Let's take a look at some of these registers and pulses.</p> In\u00a0[5]: Copied! <pre>example_graph, example_register, example_pulse = compiled[64]\n\n# The molecule, as laid out on the Quantum Device.\nexample_register.draw()\n\n# The laser pulse used to control its state evolution.\nexample_pulse.draw()\n</pre> example_graph, example_register, example_pulse = compiled[64]  # The molecule, as laid out on the Quantum Device. example_register.draw()  # The laser pulse used to control its state evolution. example_pulse.draw() In\u00a0[6]: Copied! <pre>example_pulse = graphs_to_compile[0].compile_pulse(normalized_amplitude=0.1, normalized_duration=0.1) # arbitrary values\nexample_pulse.draw()\n</pre> example_pulse = graphs_to_compile[0].compile_pulse(normalized_amplitude=0.1, normalized_duration=0.1) # arbitrary values example_pulse.draw()  <p>You can experiment further, using arbitrary pulses and registers, but for this, you'll have to use the low-level Pulser framework, which goes beyond the scope of this tutorial. You may find further details on pulses and registers in the documentation of Pulser.</p> In\u00a0[7]: Copied! <pre>from qek.data.processed_data import ProcessedData\nfrom qek.target.backends import QutipBackend\n\n# In this tutorial, to make things faster, we'll only run the graphs that require 5 qubits or less.\n# If you wish to run more entries, feel free to increase this value.\n#\n# # Warning\n#\n# Emulating a Quantum Device takes exponential amount of resources and time! If you set MAX_QUBITS too\n# high, you can bring your computer to its knees and/or crash this notebook.\nMAX_QUBITS = 5\n\nprocessed_dataset = []\nbackend = QutipBackend(device=pl.AnalogDevice)\nfor graph, register, pulse in tqdm(compiled):\n    if len(register) &gt; MAX_QUBITS:\n        continue\n    states = await backend.run(register=register, pulse=pulse)\n    processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=pl.AnalogDevice, state_dict=states, target=graph.target))\n</pre> from qek.data.processed_data import ProcessedData from qek.target.backends import QutipBackend  # In this tutorial, to make things faster, we'll only run the graphs that require 5 qubits or less. # If you wish to run more entries, feel free to increase this value. # # # Warning # # Emulating a Quantum Device takes exponential amount of resources and time! If you set MAX_QUBITS too # high, you can bring your computer to its knees and/or crash this notebook. MAX_QUBITS = 5  processed_dataset = [] backend = QutipBackend(device=pl.AnalogDevice) for graph, register, pulse in tqdm(compiled):     if len(register) &gt; MAX_QUBITS:         continue     states = await backend.run(register=register, pulse=pulse)     processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=pl.AnalogDevice, state_dict=states, target=graph.target)) <p>As mentioned, there are limits to what an emulator can do.</p> <p>Pasqal has also developed an emulator called emu-mps, which generally provides much better performance and resource usage, so if you hit resource limits, don't hesitate to check it out!</p> In\u00a0[8]: Copied! <pre>HAVE_PASQAL_ACCOUNT = False # If you have a PASQAL Cloud account, fill in the details and set this to `True`.\n\nif HAVE_PASQAL_ACCOUNT: \n    from qek.target.backends import RemoteQPUBackend\n    processed_dataset = []\n\n    # Initialize connection\n\n    my_project_id = \"your_project_id\"# Replace this value with your project_id on the PASQAL platform.\n    my_username   = \"your_username\"  # Replace this value with your username or email on the PASQAL platform.\n    my_password   = \"your_password\"  # Replace this value with your password on the PASQAL platform.\n        # Security note: In real life, you probably don't want to write your password in the code.\n        # See the documentation of PASQAL Cloud for other ways to provide your password.\n\n    # Initialize the cloud client\n    backend = RemoteQPUBackend(username=my_username, project_id=my_project_id, password=my_password)\n\n    # Fetch the specification of our QPU\n    device = await backend.device()\n\n    # As previously, create the list of graphs and embed them.\n    graphs_to_compile = []\n    for i, data in enumerate(tqdm(og_ptcfm)):\n        graph = qek_graphs.PTCFMGraph(data=data, device=device, id=i)\n        graphs_to_compile.append(graph)\n\n    compiled = []\n    for graph in tqdm(graphs_to_compile):\n        try:\n            register = graph.compile_register()\n            pulse = graph.compile_pulse()\n        except CompilationError:\n            # Let's just skip graphs that cannot be computed.\n            print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))\n            continue\n    compiled.append((graph, register, pulse))\n\n    # Now that the connection is initialized, we just have to send the work\n    # to the QPU and wait for the results.\n    for graph, register, pulse in tqdm(compiled):\n\n        # Send the work to the QPU and await the result\n        states = await backend.run(register=register, pulse=pulse)\n        processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=device, state_dict=states, target=graph.target))\n</pre> HAVE_PASQAL_ACCOUNT = False # If you have a PASQAL Cloud account, fill in the details and set this to `True`.  if HAVE_PASQAL_ACCOUNT:      from qek.target.backends import RemoteQPUBackend     processed_dataset = []      # Initialize connection      my_project_id = \"your_project_id\"# Replace this value with your project_id on the PASQAL platform.     my_username   = \"your_username\"  # Replace this value with your username or email on the PASQAL platform.     my_password   = \"your_password\"  # Replace this value with your password on the PASQAL platform.         # Security note: In real life, you probably don't want to write your password in the code.         # See the documentation of PASQAL Cloud for other ways to provide your password.      # Initialize the cloud client     backend = RemoteQPUBackend(username=my_username, project_id=my_project_id, password=my_password)      # Fetch the specification of our QPU     device = await backend.device()      # As previously, create the list of graphs and embed them.     graphs_to_compile = []     for i, data in enumerate(tqdm(og_ptcfm)):         graph = qek_graphs.PTCFMGraph(data=data, device=device, id=i)         graphs_to_compile.append(graph)      compiled = []     for graph in tqdm(graphs_to_compile):         try:             register = graph.compile_register()             pulse = graph.compile_pulse()         except CompilationError:             # Let's just skip graphs that cannot be computed.             print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))             continue     compiled.append((graph, register, pulse))      # Now that the connection is initialized, we just have to send the work     # to the QPU and wait for the results.     for graph, register, pulse in tqdm(compiled):          # Send the work to the QPU and await the result         states = await backend.run(register=register, pulse=pulse)         processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=device, state_dict=states, target=graph.target)) <p>There are other ways to use the SDK. For instance, you can enqueue a job and check later whether it has completed. Also, to work around the long waiting lines, Pasqal provides high-performance distributed and hardware-accelerated emulators, which you can access through the SDK.</p> <p>For more details, take a look at the documentation of the SDK.</p> In\u00a0[9]: Copied! <pre>import qek.data.processed_data as qek_dataset\nprocessed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\")\nprint(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\")\n</pre> import qek.data.processed_data as qek_dataset processed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\") print(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\") <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[9], line 2\n      1 import qek.data.processed_data as qek_dataset\n----&gt; 2 processed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\")\n      3 print(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\")\n\nFile ~/work/quantum-evolution-kernel/quantum-evolution-kernel/qek/data/processed_data.py:236, in load_dataset(file_path)\n    222 def load_dataset(file_path: str) -&gt; list[ProcessedData]:\n    223     \"\"\"Loads a dataset from a JSON file.\n    224 \n    225     Args:\n   (...)\n    234             the JSON file.\n    235     \"\"\"\n--&gt; 236     with open(file_path) as file:\n    237         data = json.load(file)\n    238         return [\n    239             ProcessedData(\n    240                 sequence=pl.Sequence.from_abstract_repr(item[\"sequence\"]),\n   (...)\n    244             for item in data\n    245         ]\n\nFileNotFoundError: [Errno 2] No such file or directory: 'ptcfm_processed_dataset.json'</pre> <p>Let's take a look at one of our samples:</p> In\u00a0[10]: Copied! <pre># The geometry we compiled from this graph for execution on the Quantum Device.\ndataset_example: ProcessedData = processed_dataset[64]\ndataset_example.draw_register()\n</pre> # The geometry we compiled from this graph for execution on the Quantum Device. dataset_example: ProcessedData = processed_dataset[64] dataset_example.draw_register() <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 2\n      1 # The geometry we compiled from this graph for execution on the Quantum Device.\n----&gt; 2 dataset_example: ProcessedData = processed_dataset[64]\n      3 dataset_example.draw_register()\n\nIndexError: list index out of range</pre> In\u00a0[11]: Copied! <pre># The laser pulses we used to drive the execution on the Quantum Device.\ndataset_example.draw_pulse()\n</pre> # The laser pulses we used to drive the execution on the Quantum Device. dataset_example.draw_pulse() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 # The laser pulses we used to drive the execution on the Quantum Device.\n----&gt; 2 dataset_example.draw_pulse()\n\nNameError: name 'dataset_example' is not defined</pre> <p>The results of executing the embedding on the Quantum Device are in field <code>state_dict</code>:</p> In\u00a0[12]: Copied! <pre>display(dataset_example.state_dict)\nprint(f\"Total number of samples: {sum(dataset_example.state_dict.values())}\")\n</pre> display(dataset_example.state_dict) print(f\"Total number of samples: {sum(dataset_example.state_dict.values())}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 display(dataset_example.state_dict)\n      2 print(f\"Total number of samples: {sum(dataset_example.state_dict.values())}\")\n\nNameError: name 'dataset_example' is not defined</pre> <p>This dictionary represents an approximation of the quantum state of the device for this graph after completion of the algorithm.</p> <ul> <li>each of the keys represents one possible state for the register (which represents the graph), with each qubit (which represents a single node) being in state <code>0</code> or <code>1</code>;</li> <li>the corresponding value is the number of samples observed with this specific state of the register.</li> </ul> <p>In this example, for instance, we can see that the state observed most frequently is <code>10000001010</code>, with 43/1000 samples.</p> <p>Note: Since Quantum Devices are inherently non-deterministic, you will probably obtained different samples if you run this on a Quantum Device instead of loading the dataset.</p> In\u00a0[13]: Copied! <pre>dataset_example.draw_excitation()\n</pre> dataset_example.draw_excitation() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 dataset_example.draw_excitation()\n\nNameError: name 'dataset_example' is not defined</pre>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#tutorial-1-using-a-quantum-device-to-extract-machine-learning-features","title":"Tutorial 1: using a Quantum Device to extract machine-learning features\u00b6","text":"<p>This notebook reproduces the first part of the QEK paper using the library's low-level API.</p> <p>By the end of this notebook, you will know how to:</p> <ol> <li>Import a molecular dataset (the library supports other type of graphs, of course).</li> <li>Compile a register and a pulse from each graph.</li> <li>Launch the execution of this compiled register/pulse on a quantum emulator or a physical QPU.</li> <li>Use the result to extract the relevant machine-learning features.</li> </ol> <p>A companion notebook reproduces the machine-learning part of the QEK paper.</p> <p>If you are not interested in quantum-level details, you may prefer the companion high-level notebook that mirrors this notebook, but using a higher-level API that takes care of all such issues.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#dataset-preparation","title":"Dataset preparation\u00b6","text":"<p>As in any machine learning task, we first need to load and prepare data. QEK can work with many types of graphs, including molecular graphs. For this tutorial, we will use the PTC-FM dataset, which contains such molecular graphs.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#compile-a-register-and-a-pulse","title":"Compile a Register and a Pulse\u00b6","text":"<p>Once the embedding is found, we compile a Register (the position of atoms on the Quantum Device) and a Pulse (the lasers applied to these atoms).</p> <p>Note that not all graphs can be embedded on a given device. In this notebook, for the sake of simplicity, we simply discard graphs that cannot be trivially embedded. Future versions of this library may succeed at embedding more graphs.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#experimenting-with-registers-and-pulses","title":"Experimenting with registers and pulses\u00b6","text":"<p>You may experiment with different pulses, by passing arguments to <code>compile_pulse</code>.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#executing-the-compiled-graphs-on-an-emulator","title":"Executing the compiled graphs on an emulator\u00b6","text":"<p>While our objective is to run the compiled graphs on a physical QPU, it is generally a good idea to test out some of these compiled graphs on an emulator first. For this example, we'll use the QutipEmulator, the simplest emulator provided with Pulser.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#executing-compiled-graphs-on-a-qpu","title":"Executing compiled graphs on a QPU\u00b6","text":"<p>Once you have checked that the compiled graphs work on an emulator, you will probably want to move to a QPU. Execution on a QPU takes resources polynomial in the number of qubits, which hopefully means an almost exponential speedup for large number of qubits.</p> <p>To experiment with a QPU, you will need either physical access to a QPU, or an account with PASQAL Cloud, which provides you remote access to QPUs built and hosted by Pasqal. In this section, we'll see how to use the latter.</p> <p>If you don't have an account, just skip to the next section!</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#or-using-the-provided-dataset","title":"...or using the provided dataset\u00b6","text":"<p>For this notebook, instead of spending hours running the simulator on your computer, we're going to skip this step and load on we're going to cheat and load the results, which are conveniently stored in <code>ptcfm_processed_dataset.json</code>.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#a-look-at-the-results","title":"A look at the results\u00b6","text":""},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#machine-learning-features","title":"Machine learning-features\u00b6","text":"<p>From the state dictionary, we derive as machine-learning feature the distribution of excitation. We'll use this in the next notebook to define our kernel.</p>"},{"location":"tutorial%201a%20-%20Using%20a%20Quantum%20Device%20to%20Extract%20Machine-Learning%20Features%20-%20low-level/#what-now","title":"What now?\u00b6","text":"<p>What we have seen so far covers the use of a Quantum Device to extract machine-learning features.</p> <p>For the next step, we'll see how to use these features for machine learning.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/","title":"(variant) Tutorial 2b - Training SVM QEK","text":"In\u00a0[2]: Copied! <pre>import math\nimport torch\nimport numpy as np\nimport networkx as nx\nfrom torch_geometric.data import Data, Dataset\n\n\nclass SyntheticGridGraphDataset(Dataset):\n    def __init__(self, num_graphs=100, nodes_list=[2, 4, 6, 10]):\n        \"\"\"\n        Creates a dataset of grid-like synthetic graphs with a controlled number of nodes.\n\n        Args:\n            num_graphs (int): Number of graphs to generate.\n            nodes_list (int): Number of nodes to be placed in the grid.\n        \"\"\"\n        super().__init__()\n        self.num_graphs = num_graphs\n        self.nodes_list = nodes_list\n        self.graphs = []\n        for i in range(self.num_graphs):\n            for n in self.nodes_list:\n                self.graphs.append(self.create_graph(n))\n\n    def create_graph(self, num_nodes) -&gt; Data:\n        \"\"\"Generates a structured grid graph where num_nodes dictate the grid size dynamically.\"\"\"\n        self.num_nodes = num_nodes\n        self.node_feat_dim = 2\n        self.edge_feat_dim = 2\n        # Determine grid size dynamically\n        rows = math.floor(math.sqrt(self.num_nodes))\n        cols = math.ceil(self.num_nodes / rows)\n\n        # Create a grid graph based on the exact number of nodes\n        G = nx.grid_2d_graph(rows, cols)\n        mapping = {node: i for i, node in enumerate(G.nodes())}\n        G = nx.relabel_nodes(G, mapping)\n\n        # Retain only the first `num_nodes` nodes\n        select = self.num_nodes\n        selected_nodes = list(G.nodes())[:select]\n        subgraph = G.subgraph(selected_nodes)\n\n        # Get edges\n        edge_transpose = np.array(list(subgraph.edges)).T\n        if edge_transpose.size &gt; 0:\n            edge_index = torch.tensor(edge_transpose, dtype=torch.long)\n        else:\n            edge_index = torch.empty((2, 0), dtype=torch.long)\n\n        # Generate structured 2D positions based on the dynamically computed grid\n        spacing = 5.0 \n        pos_array = np.array([[spacing * (i % cols), spacing * (i // cols)] for i in selected_nodes])\n        pos = torch.tensor(pos_array, dtype=torch.float)\n\n        # Node features (random)\n        node_features = torch.rand((select, self.node_feat_dim), dtype=torch.float)\n\n        # Edge attributes (random)\n        if edge_index.shape[1] &gt; 0:\n            edge_attr = torch.rand((edge_index.shape[1], self.edge_feat_dim), dtype=torch.float)  \n        else:\n            edge_attr = torch.empty((0, self.edge_feat_dim), dtype=torch.float)\n\n        # Graph label (binary classification based on connectivity pattern)\n        label = torch.tensor([1 if np.random.rand() &gt; 0.5 else 0], dtype=torch.long)\n\n        return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, pos=pos, y=label)\n\n    def len(self):\n        return self.num_graphs * len(self.nodes_list)\n\n    def get(self, idx):\n        return self.graphs[idx]\n</pre> import math import torch import numpy as np import networkx as nx from torch_geometric.data import Data, Dataset   class SyntheticGridGraphDataset(Dataset):     def __init__(self, num_graphs=100, nodes_list=[2, 4, 6, 10]):         \"\"\"         Creates a dataset of grid-like synthetic graphs with a controlled number of nodes.          Args:             num_graphs (int): Number of graphs to generate.             nodes_list (int): Number of nodes to be placed in the grid.         \"\"\"         super().__init__()         self.num_graphs = num_graphs         self.nodes_list = nodes_list         self.graphs = []         for i in range(self.num_graphs):             for n in self.nodes_list:                 self.graphs.append(self.create_graph(n))      def create_graph(self, num_nodes) -&gt; Data:         \"\"\"Generates a structured grid graph where num_nodes dictate the grid size dynamically.\"\"\"         self.num_nodes = num_nodes         self.node_feat_dim = 2         self.edge_feat_dim = 2         # Determine grid size dynamically         rows = math.floor(math.sqrt(self.num_nodes))         cols = math.ceil(self.num_nodes / rows)          # Create a grid graph based on the exact number of nodes         G = nx.grid_2d_graph(rows, cols)         mapping = {node: i for i, node in enumerate(G.nodes())}         G = nx.relabel_nodes(G, mapping)          # Retain only the first `num_nodes` nodes         select = self.num_nodes         selected_nodes = list(G.nodes())[:select]         subgraph = G.subgraph(selected_nodes)          # Get edges         edge_transpose = np.array(list(subgraph.edges)).T         if edge_transpose.size &gt; 0:             edge_index = torch.tensor(edge_transpose, dtype=torch.long)         else:             edge_index = torch.empty((2, 0), dtype=torch.long)          # Generate structured 2D positions based on the dynamically computed grid         spacing = 5.0          pos_array = np.array([[spacing * (i % cols), spacing * (i // cols)] for i in selected_nodes])         pos = torch.tensor(pos_array, dtype=torch.float)          # Node features (random)         node_features = torch.rand((select, self.node_feat_dim), dtype=torch.float)          # Edge attributes (random)         if edge_index.shape[1] &gt; 0:             edge_attr = torch.rand((edge_index.shape[1], self.edge_feat_dim), dtype=torch.float)           else:             edge_attr = torch.empty((0, self.edge_feat_dim), dtype=torch.float)          # Graph label (binary classification based on connectivity pattern)         label = torch.tensor([1 if np.random.rand() &gt; 0.5 else 0], dtype=torch.long)          return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, pos=pos, y=label)      def len(self):         return self.num_graphs * len(self.nodes_list)      def get(self, idx):         return self.graphs[idx]  In\u00a0[3]: Copied! <pre># Create dataset with different number of nodes.\ndataset = SyntheticGridGraphDataset(num_graphs=100, nodes_list=[2, 3, 4, 5, 10, 15, 20])\n\nprint(f\"\"\"Dataset created!\n      - Total Graphs: {len(dataset)}\n      - Sample Graph: {dataset[0]}\n    \"\"\")\n</pre> # Create dataset with different number of nodes. dataset = SyntheticGridGraphDataset(num_graphs=100, nodes_list=[2, 3, 4, 5, 10, 15, 20])  print(f\"\"\"Dataset created!       - Total Graphs: {len(dataset)}       - Sample Graph: {dataset[0]}     \"\"\") <pre>Dataset created!\n      - Total Graphs: 700\n      - Sample Graph: Data(x=[2, 2], edge_index=[2, 1], edge_attr=[1, 2], y=[1], pos=[2, 2])\n    \n</pre> <p>Now we will use the <code>BaseGraph</code> from qek_graphs.</p> In\u00a0[4]: Copied! <pre>from tqdm import tqdm\nimport pulser as pl\nimport qek.data.graphs as qek_graphs\n\n\ngraphs_to_compile = []\n\nfor i, data in enumerate(tqdm(dataset)):\n    graph = qek_graphs.BaseGraph(data=data, device=pl.AnalogDevice, id=i)\n    graph.target = data.y.item()\n    graphs_to_compile.append(graph)\n</pre> from tqdm import tqdm import pulser as pl import qek.data.graphs as qek_graphs   graphs_to_compile = []  for i, data in enumerate(tqdm(dataset)):     graph = qek_graphs.BaseGraph(data=data, device=pl.AnalogDevice, id=i)     graph.target = data.y.item()     graphs_to_compile.append(graph)  In\u00a0[5]: Copied! <pre>from qek.shared.error import CompilationError\n\ncompiled = [] \n\nfor graph in tqdm(graphs_to_compile):\n    try:\n        register = graph.compile_register()\n        pulse = graph.compile_pulse()\n    except CompilationError:\n        # Let's just skip graphs that cannot be computed.\n        print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))\n        continue\n    compiled.append((graph, register, pulse))\nprint(\"Compiled %s graphs into registers/pulses\" % (len(compiled, )))\n</pre> from qek.shared.error import CompilationError  compiled = []   for graph in tqdm(graphs_to_compile):     try:         register = graph.compile_register()         pulse = graph.compile_pulse()     except CompilationError:         # Let's just skip graphs that cannot be computed.         print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))         continue     compiled.append((graph, register, pulse)) print(\"Compiled %s graphs into registers/pulses\" % (len(compiled, ))) <pre>Compiled 700 graphs into registers/pulses\n</pre> <p>Let's take a look at some of these registers and pulses.</p> In\u00a0[6]: Copied! <pre>example_graph, example_register, example_pulse = compiled[53]\nexample_register.draw()\nexample_pulse.draw()\n</pre> example_graph, example_register, example_pulse = compiled[53] example_register.draw() example_pulse.draw() In\u00a0[7]: Copied! <pre>from qek.data.processed_data import ProcessedData\nfrom qek.target.backends import QutipBackend\n\n# In this tutorial, to make things faster, we'll only run 5 qubits or less.\n# If you wish to run more entries, feel free to increase this value.\n#\n# # Warning\n#\n# Emulating a Quantum Device takes exponential amount of resources and time! If you set MAX_QUBITS too\n# high, you can bring your computer to its knees and/or crash this notebook.\nMAX_QUBITS = 5\n\nprocessed_dataset = []\nexecutor = QutipBackend(device=pl.AnalogDevice)\nfor graph, register, pulse in tqdm(compiled):\n    if len(register) &gt; MAX_QUBITS:\n        continue\n    states = await executor.run(register=register, pulse=pulse)\n    processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=pl.AnalogDevice, state_dict=states, target=graph.target))\n</pre> from qek.data.processed_data import ProcessedData from qek.target.backends import QutipBackend  # In this tutorial, to make things faster, we'll only run 5 qubits or less. # If you wish to run more entries, feel free to increase this value. # # # Warning # # Emulating a Quantum Device takes exponential amount of resources and time! If you set MAX_QUBITS too # high, you can bring your computer to its knees and/or crash this notebook. MAX_QUBITS = 5  processed_dataset = [] executor = QutipBackend(device=pl.AnalogDevice) for graph, register, pulse in tqdm(compiled):     if len(register) &gt; MAX_QUBITS:         continue     states = await executor.run(register=register, pulse=pulse)     processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=pl.AnalogDevice, state_dict=states, target=graph.target)) In\u00a0[8]: Copied! <pre>HAVE_PASQAL_ACCOUNT = False # If you have a PASQAL Cloud account, fill in the details and set this to `True`.\n\nif HAVE_PASQAL_ACCOUNT: \n    from qek.target.backends import RemoteQPUBackend\n    processed_dataset = []\n\n    # Initialize connection\n\n    my_project_id = \"your_project_id\"# Replace this value with your project_id on the PASQAL platform.\n    my_username   = \"your_username\"  # Replace this value with your username or email on the PASQAL platform.\n    my_password   = \"your_password\"  # Replace this value with your password on the PASQAL platform.\n        # Security note: In real life, you probably don't want to write your password in the code.\n        # See the documentation of PASQAL Cloud for other ways to provide your password.\n\n    # Initialize the cloud client\n    executor = RemoteQPUBackend(username=my_username, project_id=my_project_id, password=my_password)\n\n    # Fetch the specification of our QPU\n    device = await executor.device()\n\n    # As previously, create the list of graphs and embed them.\n    graphs_to_compile = []\n    for i, data in enumerate(tqdm(dataset)):\n        graph = qek_graphs.PTCFMGraph(data=data, device=device, id=i)\n        graphs_to_compile.append(graph)\n\n    compiled = []\n    for graph in tqdm(graphs_to_compile):\n        try:\n            register = graph.compile_register()\n            pulse = graph.compile_pulse()\n        except CompilationError:\n            # Let's just skip graphs that cannot be computed.\n            print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))\n            continue\n    compiled.append((graph, register, pulse))\n\n    # Now that the connection is initialized, we just have to send the work\n    # to the QPU and wait for the results.\n    for graph, register, pulse in tqdm(compiled):\n\n        # Send the work to the QPU and await the result\n        states = await executor.run(register=register, pulse=pulse)\n        processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=device, state_dict=states, target=graph.target))\n</pre> HAVE_PASQAL_ACCOUNT = False # If you have a PASQAL Cloud account, fill in the details and set this to `True`.  if HAVE_PASQAL_ACCOUNT:      from qek.target.backends import RemoteQPUBackend     processed_dataset = []      # Initialize connection      my_project_id = \"your_project_id\"# Replace this value with your project_id on the PASQAL platform.     my_username   = \"your_username\"  # Replace this value with your username or email on the PASQAL platform.     my_password   = \"your_password\"  # Replace this value with your password on the PASQAL platform.         # Security note: In real life, you probably don't want to write your password in the code.         # See the documentation of PASQAL Cloud for other ways to provide your password.      # Initialize the cloud client     executor = RemoteQPUBackend(username=my_username, project_id=my_project_id, password=my_password)      # Fetch the specification of our QPU     device = await executor.device()      # As previously, create the list of graphs and embed them.     graphs_to_compile = []     for i, data in enumerate(tqdm(dataset)):         graph = qek_graphs.PTCFMGraph(data=data, device=device, id=i)         graphs_to_compile.append(graph)      compiled = []     for graph in tqdm(graphs_to_compile):         try:             register = graph.compile_register()             pulse = graph.compile_pulse()         except CompilationError:             # Let's just skip graphs that cannot be computed.             print(\"Graph %s cannot be compiled for this device\" % (graph.id, ))             continue     compiled.append((graph, register, pulse))      # Now that the connection is initialized, we just have to send the work     # to the QPU and wait for the results.     for graph, register, pulse in tqdm(compiled):          # Send the work to the QPU and await the result         states = await executor.run(register=register, pulse=pulse)         processed_dataset.append(ProcessedData.custom(register=register, pulse=pulse, device=device, state_dict=states, target=graph.target)) <p>Let's take a look at one of our samples:</p> In\u00a0[9]: Copied! <pre>dataset_example: ProcessedData = processed_dataset[53]\nprint(f\"\"\"Total number of samples: {len(processed_dataset)}\n        - Example state_dict {dataset_example.state_dict}\"\"\")\ndataset_example.draw_register()\ndataset_example.draw_pulse()\ndisplay()\n</pre> dataset_example: ProcessedData = processed_dataset[53] print(f\"\"\"Total number of samples: {len(processed_dataset)}         - Example state_dict {dataset_example.state_dict}\"\"\") dataset_example.draw_register() dataset_example.draw_pulse() display()  <pre>Total number of samples: 400\n        - Example state_dict {'000': 33, '001': 401, '010': 78, '011': 2, '100': 434, '101': 52}\n</pre> In\u00a0[10]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# Prepare features (X) and targets (y)\nX = [data for data in processed_dataset]  \ny = [data.target for data in processed_dataset] \n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\nprint(f'Size of the training quantum compatible dataset = {len(X_train)}')\nprint(f'Size of the testing quantum compatible dataset = {len(X_test)}')\n</pre> from sklearn.model_selection import train_test_split  # Prepare features (X) and targets (y) X = [data for data in processed_dataset]   y = [data.target for data in processed_dataset]   # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42) print(f'Size of the training quantum compatible dataset = {len(X_train)}') print(f'Size of the testing quantum compatible dataset = {len(X_test)}') <pre>Size of the training quantum compatible dataset = 320\nSize of the testing quantum compatible dataset = 80\n</pre> In\u00a0[11]: Copied! <pre>X_train[53].draw_excitation()\n</pre> X_train[53].draw_excitation() <p>The Quantum Evolution Kernel computes a similarity score between two graphs based on quantum-inspired measures. It is designed to work with graph-structured data.</p> In\u00a0[12]: Copied! <pre>from qek.kernel import QuantumEvolutionKernel as QEK\n\n# Initialize the Quantum Evolution Kernel with a parameter mu\nkernel = QEK(mu=0.5)\n\n# Fit\nkernel.fit(X_train)\n\n# Transform\nK_train = kernel.transform(X_train)\nK_test = kernel.transform(X_test)\n\nprint(f\"Training Kernel Matrix Shape: {K_train.shape}\")\nprint(f\"Testing Kernel Matrix Shape: {K_test.shape}\")\n</pre> from qek.kernel import QuantumEvolutionKernel as QEK  # Initialize the Quantum Evolution Kernel with a parameter mu kernel = QEK(mu=0.5)  # Fit kernel.fit(X_train)  # Transform K_train = kernel.transform(X_train) K_test = kernel.transform(X_test)  print(f\"Training Kernel Matrix Shape: {K_train.shape}\") print(f\"Testing Kernel Matrix Shape: {K_test.shape}\") <pre>Training Kernel Matrix Shape: (320, 320)\nTesting Kernel Matrix Shape: (80, 320)\n</pre> In\u00a0[13]: Copied! <pre>from sklearn.svm import SVC\n\n# Define a SVC model with QuantumEvolutionKernel\nqek_kernel = QEK(mu=0.5)\nmodel = SVC(kernel=qek_kernel, random_state=42)\n</pre> from sklearn.svm import SVC  # Define a SVC model with QuantumEvolutionKernel qek_kernel = QEK(mu=0.5) model = SVC(kernel=qek_kernel, random_state=42) In\u00a0[14]: Copied! <pre>model.fit(X_train, y_train)\n</pre> model.fit(X_train, y_train) Out[14]: <pre>SVC(kernel=&lt;qek.kernel.kernel.FastQEK object at 0x7f2a03846710&gt;,\n    random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted<pre>SVC(kernel=&lt;qek.kernel.kernel.FastQEK object at 0x7f2a03846710&gt;,\n    random_state=42)</pre> kernel: FastQEK<pre>&lt;qek.kernel.kernel.FastQEK object at 0x7f2a03846710&gt;</pre> FastQEK<pre>&lt;qek.kernel.kernel.FastQEK object at 0x7f2a03846710&gt;</pre> <p>We can use the trained model to make predictions on the test dataset</p> In\u00a0[15]: Copied! <pre># Making predictions using the trained model and test data\ny_pred = model.predict(X_test)\n</pre> # Making predictions using the trained model and test data y_pred = model.predict(X_test) In\u00a0[16]: Copied! <pre>from sklearn.metrics import f1_score, balanced_accuracy_score\n\nprint(\"\\nEvaluation Results:\")\nprint(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\")\nprint(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.2f}\")\n</pre> from sklearn.metrics import f1_score, balanced_accuracy_score  print(\"\\nEvaluation Results:\") print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.2f}\") print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred):.2f}\") <pre>\nEvaluation Results:\nF1 Score: 0.56\nBalanced Accuracy: 0.56\n</pre> <p>As this was a synthetic dataset with random features, the accuracy is low. The model can also be tuned further, For extra details on a real dataset, please see the  companion notebook for advanced ML steps.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#tutorial-1b-training-a-svm-using-qek","title":"Tutorial 1b: Training a SVM using QEK\u00b6","text":"<p>This tutorial demonstrates how to create a classification model using the QEK kernel with a generic database. By the end of this guide, you will know how to:</p> <ol> <li>Define a Generic Graph Dataset: Specify coordinates for the nodes in the dataset.</li> <li>Compile a Register and Pulse from Each Graph: Use these components that are used foe QPU execution.</li> <li>Launch the Compiled Execution: Run on either a quantum emulator or a physical QPU.<ul> <li>3.1. Execution on an emulator</li> <li>3.2. Execution on Pasqals QPU</li> </ul> </li> <li>Extract QEK Features for Machine-Learning: Utilize the results from the quantum execution to derive relevant features using the <code>QEK</code> Kernel.</li> <li>Train a Machine Learning Model: Build and train the model using the extracted features.</li> </ol>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#notes","title":"Notes\u00b6","text":"<ul> <li>A companion notebook demonstrates advanced machine learning methods\u2014including Grid Search\u2014that can be used with the QEK kernel - using a real world molecular dataset.</li> <li>If you prefer to work at a higher level without getting into quantum-level details, you might opt for the high-level notebook, which abstracts these details using a more user-friendly API.</li> </ul>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#1-dataset-preparation","title":"1) Dataset Preparation\u00b6","text":"<p>As with any machine learning task, the first step is to load and prepare the data.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#overview","title":"Overview\u00b6","text":"<p>QEK (Quantum-Enhanced Kernel) can work with various types of graphs. In this tutorial, we will use a generic synthetic graph dataset. The dataset will be generated using the <code>networkx</code> library, and we will incorporate features and target values to facilitate a classification task.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#steps","title":"Steps\u00b6","text":"<ul> <li>Graph Generation: We will create a synthetic graph using <code>networkx</code> and <code>torch_geometric</code> dataset.</li> <li>Assigning Node and Edge Features: Each node and edge will be assigned relevant features.</li> <li>Target Value Assignment: The target value for each graph will be based on its density. This value will be used in a classification task.</li> <li>Defining Graph Geometry<ul> <li>A generic grid-like geometry will be assigned.</li> <li>The positions of nodes will be stored in the <code>pos</code> variable.</li> <li>Users can define custom positions, ensuring compatibility with the QPU register.</li> </ul> </li> </ul>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#customization-and-considerations","title":"Customization and Considerations\u00b6","text":"<ul> <li>Users can modify the node and edge features as needed.</li> <li>Custom graph layouts should be carefully designed to maintain compatibility with quantum processing unit (QPU)/mulator registers.</li> </ul> <p>By following these steps, we will prepare a dataset suitable for quantum-enhanced graph machine learning tasks.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#notes-on-geometry-and-coordinates-pos","title":"Notes on Geometry and Coordinates (<code>Pos</code>):\u00b6","text":"<p>This package lets researchers embed graphs on Analog Quantum Devices. To do this we need to give these graphs a geometry (their positions in space) and to confirm that the geometry is compatible with a Quantum Device.</p> <p>This package builds upon the Pulser framework. Our objective, in this notebook, is to compile graphs into a format understood by our Quantum Devices. Which include, a Pulser Register (the position of qubits) and Pulser Pulses (the laser impulses controlling the evolution of the analog device).</p> <p>As the geometry depends on the Quantum Device, we need to specify a device to use. For the time being, we'll use Pulser's <code>AnalogDevice</code>, which is a reasonable default device. We'll show you a bit further how to use another device.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#2-compile-a-register-and-a-pulse","title":"2) Compile a Register and a Pulse\u00b6","text":"<p>Once the embedding is found, we compile a Register (the position of atoms on the Quantum Device) and a Pulse (the lasers applied to these atoms).</p> <p>Note that not all graphs can be embedded on a given device. In this notebook, for the sake of simplicity, we simply discard graphs that cannot be trivially embedded. Future versions of this library may succeed at embedding more graphs.</p> <p>The user can also define custom register and pulses using the  Pulser framework</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#3-execution","title":"3) Execution\u00b6","text":"<p>In this section we will showcase how to execute from register and pulses. This execution will lead to a processed dataset, that we can eventually use to create the QEK Kernel.</p> <ul> <li>The results of executing the embedding on the Quantum Device are in field <code>state_dict</code>.</li> <li><code>state_dict</code> will be used in the <code>QEK</code> Kernel in the next steps.</li> </ul>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#31-executing-on-an-emulator","title":"3.1) Executing on an emulator\u00b6","text":"<p>While our objective is to run on a physical QPU, it is generally a good idea to test on an emulator first. For this example, we'll use the QutipEmulator, the simplest emulator provided with Pulser.</p> <p>Pasqal has also developed an emulator called emu-mps, which generally provides much better performance and resource usage, so if you hit resource limits, don't hesitate to check it out!</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#32-executing-on-a-qpu","title":"3.2) Executing on a QPU\u00b6","text":"<p>Once you have checked that the pulses work on an emulator, you will probably want to move to a QPU. Execution on a QPU takes resources polynomial in the number of qubits, which hopefully means an almost exponential speedup for large number of qubits.</p> <p>To experiment with a QPU, you will need either physical access to a QPU, or an account with PASQAL Cloud, which provides you remote access to QPUs built and hosted by Pasqal. In this section, we'll see how to use the latter.</p> <p>If you don't have an account, just skip to the next section!</p> <p>There are other ways to use the SDK. For instance, you can enqueue a job and check later whether it has completed. Also, to work around the long waiting lines, Pasqal provides high-performance distributed and hardware-accelerated emulators, which you can access through the SDK. For more details, take a look at the documentation of the SDK.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#a-look-at-the-results","title":"A look at the results\u00b6","text":""},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#state-dict","title":"State Dict\u00b6","text":"<p>This dictionary represents an approximation of the quantum state of the device for this graph after completion of the algorithm.</p> <ul> <li>each of the keys represents one possible state for the register (which represents the graph), with each qubit (which represents a single node) being in state <code>0</code> or <code>1</code>;</li> <li>the corresponding value is the number of samples observed with this specific state of the register.</li> </ul> <p>Note: Since Quantum Devices are inherently non-deterministic, you will probably obtained different samples if you run this on a Quantum Device instead of loading the dataset.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#4-extract-qek-features-for-machine-learning","title":"4). Extract QEK Features for Machine-Learning\u00b6","text":"<p>What we have seen so far covers the use of a Quantum Device to extract features. Now we will utilize the execution result to create the QEK Kernel.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#introduction","title":"Introduction\u00b6","text":"<p>This tutorial uses scikit-learn for common machine learning tasks, but the concepts would work with any other machine learning framework as well.</p> <ul> <li>First we will split the dataset into train and test datasets</li> <li>Secondly, we will initialize and <code>fit</code> the QEK Kernel.</li> </ul> <p>From the state dictionary, we derive as machine-learning feature the distribution of excitation. We'll use this in the next parts to define our kernel.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#5-train-a-machine-learning-model","title":"5) Train a Machine Learning Model\u00b6","text":"<p>We will use an SVM (Support Vector Machine) to learn how to predict the toxicity of a molecule based on the precomputed kernel.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#training","title":"Training\u00b6","text":"<p>This precomputed kernel will allow us to evaluate the algorithm QEK.</p>"},{"location":"tutorial%201b%20-%20Training%20SVM%20QEK%20-%20low-level%20-%20generic%20dataset/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/","title":"Tutorial 2: Quantum Evolution Kernel-Based Machine Learning Tutorial","text":"<p>We begin by loading a dataset that contains graph-structured data. Each data point is represented as a processed object with features (in this example, the quantum distribution excitation for a molecule) and a target value (in this example, the toxicity of the molecule). We will split the data into training and testing sets for model evaluation.</p> In\u00a0[1]: Copied! <pre>import qek.data.processed_data as qek_dataset\n\n# Load the dataset we processed in the quantum extraction tutorial\nprocessed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\")\nprint(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\")\n</pre> import qek.data.processed_data as qek_dataset  # Load the dataset we processed in the quantum extraction tutorial processed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\") print(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\") <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[1], line 4\n      1 import qek.data.processed_data as qek_dataset\n      3 # Load the dataset we processed in the quantum extraction tutorial\n----&gt; 4 processed_dataset = qek_dataset.load_dataset(file_path=\"ptcfm_processed_dataset.json\")\n      5 print(f\"Size of the quantum compatible dataset = {len(processed_dataset)}\")\n\nFile ~/work/quantum-evolution-kernel/quantum-evolution-kernel/qek/data/processed_data.py:236, in load_dataset(file_path)\n    222 def load_dataset(file_path: str) -&gt; list[ProcessedData]:\n    223     \"\"\"Loads a dataset from a JSON file.\n    224 \n    225     Args:\n   (...)\n    234             the JSON file.\n    235     \"\"\"\n--&gt; 236     with open(file_path) as file:\n    237         data = json.load(file)\n    238         return [\n    239             ProcessedData(\n    240                 sequence=pl.Sequence.from_abstract_repr(item[\"sequence\"]),\n   (...)\n    244             for item in data\n    245         ]\n\nFileNotFoundError: [Errno 2] No such file or directory: 'ptcfm_processed_dataset.json'</pre> In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# Prepare features (X) and targets (y)\nX = [data for data in processed_dataset] # Quantum distribution excitation.\ny = [data.target for data in processed_dataset] # Toxicity.\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\n</pre> from sklearn.model_selection import train_test_split  # Prepare features (X) and targets (y) X = [data for data in processed_dataset] # Quantum distribution excitation. y = [data.target for data in processed_dataset] # Toxicity.  # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 4\n      1 from sklearn.model_selection import train_test_split\n      3 # Prepare features (X) and targets (y)\n----&gt; 4 X = [data for data in processed_dataset] # Quantum distribution excitation.\n      5 y = [data.target for data in processed_dataset] # Toxicity.\n      7 # Split data into training and testing sets\n\nNameError: name 'processed_dataset' is not defined</pre> In\u00a0[3]: Copied! <pre>print(f'Size of the training quantum compatible dataset = {len(X_train)}')\nprint(f'Size of the testing quantum compatible dataset = {len(X_test)}')\n</pre> print(f'Size of the training quantum compatible dataset = {len(X_train)}') print(f'Size of the testing quantum compatible dataset = {len(X_test)}') <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 print(f'Size of the training quantum compatible dataset = {len(X_train)}')\n      2 print(f'Size of the testing quantum compatible dataset = {len(X_test)}')\n\nNameError: name 'X_train' is not defined</pre> In\u00a0[4]: Copied! <pre>dataset_example = X[64]\n# The features we have extracted for this sample\ndataset_example.draw_excitation()\n</pre> dataset_example = X[64] # The features we have extracted for this sample dataset_example.draw_excitation() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 dataset_example = X[64]\n      2 # The features we have extracted for this sample\n      3 dataset_example.draw_excitation()\n\nNameError: name 'X' is not defined</pre> <p>The Quantum Evolution Kernel computes a similarity score between two graphs based on quantum-inspired measures. It is designed to work with graph-structured data.</p> <ul> <li>A kernel is a function that computes a similarity measure between two data points. In ML, kernels are often used to implicitly map data into a higher-dimensional space without computing the transformation explicitly. This enables algorithms like Support Vector Machines (SVM) to solve problems that are not linearly separable in the original feature space.</li> </ul> In\u00a0[5]: Copied! <pre>from qek.kernel import FastQEK as QEK\n\n# Initialize the Quantum Evolution Kernel with a parameter mu\nkernel = QEK(mu=0.5)\n</pre> from qek.kernel import FastQEK as QEK  # Initialize the Quantum Evolution Kernel with a parameter mu kernel = QEK(mu=0.5) In\u00a0[6]: Copied! <pre># Fit\nkernel.fit(X_train)\n\n# Transform\nK_train = kernel.transform(X_train)\nK_test = kernel.transform(X_test)\n\nprint(f\"Training Kernel Matrix Shape: {K_train.shape}\")\nprint(f\"Testing Kernel Matrix Shape: {K_test.shape}\")\n</pre> # Fit kernel.fit(X_train)  # Transform K_train = kernel.transform(X_train) K_test = kernel.transform(X_test)  print(f\"Training Kernel Matrix Shape: {K_train.shape}\") print(f\"Testing Kernel Matrix Shape: {K_test.shape}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 2\n      1 # Fit\n----&gt; 2 kernel.fit(X_train)\n      4 # Transform\n      5 K_train = kernel.transform(X_train)\n\nNameError: name 'X_train' is not defined</pre> <p>The kernel matrix for the testing dataset looks like:</p> In\u00a0[7]: Copied! <pre>K_train\n</pre> K_train <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 K_train\n\nNameError: name 'K_train' is not defined</pre> <p>Parameter $\\mu$ controls the rate of exponential decay. A large value of $\\mu$ makes QEK very sensitive to small variations of the Jensen-Shanon distance. Conversely, when $\\mu$ is small, the kernel is less affected by small variations in of $JS$.</p> <p>QEK compares two processed graphs by their distribution of excitations. If <code>a</code> and <code>b</code> are two graphs, a value of <code>kernel(a, b)</code> close to 1 indicates a big similarity between graphs <code>a</code> and <code>b</code>, while a value close to 0 means a small graph similarity.</p> <p>Let's try that:</p> In\u00a0[8]: Copied! <pre>graph_1 = processed_dataset[2]\ngraph_2 = processed_dataset[0]\n\ndisplay(f\"Comparing a graph with itself: {kernel.similarity(graph_1, graph_1)}\")\ndisplay(f\"Comparing two much dissimilar graphs: {kernel.similarity(graph_1, graph_2)}\")\n</pre> graph_1 = processed_dataset[2] graph_2 = processed_dataset[0]  display(f\"Comparing a graph with itself: {kernel.similarity(graph_1, graph_1)}\") display(f\"Comparing two much dissimilar graphs: {kernel.similarity(graph_1, graph_2)}\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 graph_1 = processed_dataset[2]\n      2 graph_2 = processed_dataset[0]\n      4 display(f\"Comparing a graph with itself: {kernel.similarity(graph_1, graph_1)}\")\n\nNameError: name 'processed_dataset' is not defined</pre> <p>We can further validate this by checking that the two graphs are registered differently and have different excitations.</p> In\u00a0[9]: Copied! <pre>graph_1.draw_register()\ngraph_2.draw_register()\n</pre> graph_1.draw_register() graph_2.draw_register() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 graph_1.draw_register()\n      2 graph_2.draw_register()\n\nNameError: name 'graph_1' is not defined</pre> In\u00a0[10]: Copied! <pre>graph_1.draw_excitation()\ngraph_2.draw_excitation()\n</pre> graph_1.draw_excitation() graph_2.draw_excitation() <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 graph_1.draw_excitation()\n      2 graph_2.draw_excitation()\n\nNameError: name 'graph_1' is not defined</pre> <p>We will use an SVM (Support Vector Machine) to learn how to predict the toxicity of a molecule based on the precomputed kernel.</p> In\u00a0[11]: Copied! <pre>from sklearn.svm import SVC\n\n# Define a SVC model with FastQEK\nqek_kernel = QEK(mu=0.5)\nmodel = SVC(kernel=qek_kernel, random_state=42)\n</pre> from sklearn.svm import SVC  # Define a SVC model with FastQEK qek_kernel = QEK(mu=0.5) model = SVC(kernel=qek_kernel, random_state=42) In\u00a0[12]: Copied! <pre>model.fit(X_train, y_train)\n</pre> model.fit(X_train, y_train) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 model.fit(X_train, y_train)\n\nNameError: name 'X_train' is not defined</pre> In\u00a0[13]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import classification_report, f1_score, balanced_accuracy_score\n\ndef evaluate_predictions(y_test, y_pred):\n    \"\"\"\n    Evaluates the model predictions using multiple metrics: F1 score, and\n    balanced accuracy. Prints the classification report \n    and other evaluation results.\n\n    Args:\n        y_test (array-like): The true labels.\n        y_pred (array-like): The predicted labels.\n    \"\"\"\n    # Calculate F1 score and balanced accuracy\n    max_f1_score = f1_score(y_test, y_pred, average='weighted')\n    final_f1_std = np.std(f1_score(y_test, y_pred, average=None))\n    max_bal_acc = balanced_accuracy_score(y_test, y_pred)\n    std_bal_acc = np.std(balanced_accuracy_score(y_test, y_pred))\n\n    # Print the evaluation results\n    print(\"\\nEvaluation Results:\")\n    print(f\"F1 Score: {max_f1_score:.2f}\")\n    print(f\"Standard Deviation of F1 Score: {final_f1_std:.2f}\")\n    print(f\"Balanced Accuracy: {max_bal_acc:.2f}\")\n    print(f\"Standard Deviation of Balanced Accuracy: {std_bal_acc:.2f}\")\n\n    # Print classification report\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n</pre> import numpy as np from sklearn.metrics import classification_report, f1_score, balanced_accuracy_score  def evaluate_predictions(y_test, y_pred):     \"\"\"     Evaluates the model predictions using multiple metrics: F1 score, and     balanced accuracy. Prints the classification report      and other evaluation results.      Args:         y_test (array-like): The true labels.         y_pred (array-like): The predicted labels.     \"\"\"     # Calculate F1 score and balanced accuracy     max_f1_score = f1_score(y_test, y_pred, average='weighted')     final_f1_std = np.std(f1_score(y_test, y_pred, average=None))     max_bal_acc = balanced_accuracy_score(y_test, y_pred)     std_bal_acc = np.std(balanced_accuracy_score(y_test, y_pred))      # Print the evaluation results     print(\"\\nEvaluation Results:\")     print(f\"F1 Score: {max_f1_score:.2f}\")     print(f\"Standard Deviation of F1 Score: {final_f1_std:.2f}\")     print(f\"Balanced Accuracy: {max_bal_acc:.2f}\")     print(f\"Standard Deviation of Balanced Accuracy: {std_bal_acc:.2f}\")      # Print classification report     print(\"Classification Report:\")     print(classification_report(y_test, y_pred))  <p>We can use the trained model to make predictions on the test dataset</p> In\u00a0[14]: Copied! <pre># Making predictions using the trained model\ny_pred = model.predict(X_test)\n\nevaluate_predictions(y_test, y_pred)\n</pre> # Making predictions using the trained model y_pred = model.predict(X_test)  evaluate_predictions(y_test, y_pred) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 # Making predictions using the trained model\n----&gt; 2 y_pred = model.predict(X_test)\n      4 evaluate_predictions(y_test, y_pred)\n\nNameError: name 'X_test' is not defined</pre> <p>Pipelines in scikit-learn allow us to streamline the workflow by chaining preprocessing steps and models. In this step, we integrate the Quantum Evolution Kernel with an SVM classifier in a pipeline for end-to-end model training and prediction.</p> In\u00a0[15]: Copied! <pre>from sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\n# Define the pipeline\npipeline = Pipeline([\n    ('QEK', QEK(mu=0.5)),\n    ('svm', SVC(kernel='precomputed', random_state=42))\n])\n</pre> from sklearn.svm import SVC from sklearn.pipeline import Pipeline  # Define the pipeline pipeline = Pipeline([     ('QEK', QEK(mu=0.5)),     ('svm', SVC(kernel='precomputed', random_state=42)) ]) In\u00a0[16]: Copied! <pre># Train the pipeline\npipeline.fit(X_train, y_train)\n</pre> # Train the pipeline pipeline.fit(X_train, y_train) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 # Train the pipeline\n----&gt; 2 pipeline.fit(X_train, y_train)\n\nNameError: name 'X_train' is not defined</pre> In\u00a0[17]: Copied! <pre># Making predictions using the trained model\ny_pred = pipeline.predict(X_test)\n\nevaluate_predictions(y_test, y_pred)\n</pre> # Making predictions using the trained model y_pred = pipeline.predict(X_test)  evaluate_predictions(y_test, y_pred) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 2\n      1 # Making predictions using the trained model\n----&gt; 2 y_pred = pipeline.predict(X_test)\n      4 evaluate_predictions(y_test, y_pred)\n\nNameError: name 'X_test' is not defined</pre> <p>Hyperparameter tuning is a critical step in improving machine learning model performance. GridSearchCV systematically searches through a predefined set of hyperparameters to find the combination that yields the best results. Here, we optimize:</p> <ul> <li><code>mu</code>: A parameter of the Quantum Evolution Kernel.</li> <li><code>C</code>: The regularization parameter of the SVM.</li> </ul> <p>We will use the pipeline defined in the previous section inside the grid search. Additionally, we employ multiple scoring metrics such as F1 Score and Balanced Accuracy to evaluate the performance of the models comprehensively.</p> In\u00a0[18]: Copied! <pre>from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import make_scorer\nimport matplotlib.pyplot as plt\n</pre> from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold from sklearn.metrics import make_scorer import matplotlib.pyplot as plt In\u00a0[19]: Copied! <pre># Define scoring metrics\nscoring = {\n    \"balanced_accuracy\": make_scorer(balanced_accuracy_score),\n    \"f1_score\": make_scorer(f1_score, average=\"weighted\")\n}\n\n# Define cross-validation strategy\nskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n\n# Define parameter grid\nparam_grid = {\n    'svm__C': np.logspace(0.001, 1, 5),\n    'QEK__mu': [0.25, 0.5, 0.75],\n}\n\ngrid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, cv=skf, refit=\"f1_score\", n_jobs=8, return_train_score=True)\n</pre> # Define scoring metrics scoring = {     \"balanced_accuracy\": make_scorer(balanced_accuracy_score),     \"f1_score\": make_scorer(f1_score, average=\"weighted\") }  # Define cross-validation strategy skf = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)  # Define parameter grid param_grid = {     'svm__C': np.logspace(0.001, 1, 5),     'QEK__mu': [0.25, 0.5, 0.75], }  grid_search = GridSearchCV(pipeline, param_grid, scoring=scoring, cv=skf, refit=\"f1_score\", n_jobs=8, return_train_score=True) In\u00a0[20]: Copied! <pre># Perform grid search\ngrid_search.fit(X_train, y_train)\n</pre> # Perform grid search grid_search.fit(X_train, y_train) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 2\n      1 # Perform grid search\n----&gt; 2 grid_search.fit(X_train, y_train)\n\nNameError: name 'X_train' is not defined</pre> <p>We can access best trained model from the grid search using .best_estimator_.</p> In\u00a0[21]: Copied! <pre># Evaluate the best model\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\n\nevaluate_predictions(y_test, y_pred)\n</pre> # Evaluate the best model best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test)  evaluate_predictions(y_test, y_pred) <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[21], line 2\n      1 # Evaluate the best model\n----&gt; 2 best_model = grid_search.best_estimator_\n      3 y_pred = best_model.predict(X_test)\n      5 evaluate_predictions(y_test, y_pred)\n\nAttributeError: 'GridSearchCV' object has no attribute 'best_estimator_'</pre> <p>We can also access the results of grid search using .cv_results_. This allows us to plot learning curves, as well as see the impact of different hyperparamters on model performance.</p> In\u00a0[22]: Copied! <pre># Extract results\ncv_results = grid_search.cv_results_\n</pre> # Extract results cv_results = grid_search.cv_results_ <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[22], line 2\n      1 # Extract results\n----&gt; 2 cv_results = grid_search.cv_results_\n\nAttributeError: 'GridSearchCV' object has no attribute 'cv_results_'</pre> In\u00a0[23]: Copied! <pre># Simple plot function to plot results\ndef plot_grid_search_results(grid_search, param_name=\"svm__C\"):\n    cv_results = grid_search.cv_results_\n    param_values = [p[param_name] for p in cv_results['params']]\n    unique_param_values = sorted(set(param_values))\n\n    mean_f1_scores = []\n    std_f1_scores = []\n\n    for param_value in unique_param_values:\n        indices = [i for i, p in enumerate(cv_results['params']) if p[param_name] == param_value]\n        mean_f1_scores.append(np.mean([cv_results['mean_test_f1_score'][i] for i in indices]))\n        std_f1_scores.append(np.mean([cv_results['std_test_f1_score'][i] for i in indices]))\n\n    plt.figure(figsize=(6, 5))\n    plt.plot(unique_param_values, mean_f1_scores, label=\"Mean F1 Score\")\n    plt.fill_between(unique_param_values, \n                     np.array(mean_f1_scores) - np.array(std_f1_scores),\n                     np.array(mean_f1_scores) + np.array(std_f1_scores), \n                     alpha=0.2)\n    \n    plt.title(f\"Grid Search Results for {param_name}\")\n    plt.xlabel(f\"{param_name}\")\n    plt.ylabel(\"Mean F1 Score\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n</pre> # Simple plot function to plot results def plot_grid_search_results(grid_search, param_name=\"svm__C\"):     cv_results = grid_search.cv_results_     param_values = [p[param_name] for p in cv_results['params']]     unique_param_values = sorted(set(param_values))      mean_f1_scores = []     std_f1_scores = []      for param_value in unique_param_values:         indices = [i for i, p in enumerate(cv_results['params']) if p[param_name] == param_value]         mean_f1_scores.append(np.mean([cv_results['mean_test_f1_score'][i] for i in indices]))         std_f1_scores.append(np.mean([cv_results['std_test_f1_score'][i] for i in indices]))      plt.figure(figsize=(6, 5))     plt.plot(unique_param_values, mean_f1_scores, label=\"Mean F1 Score\")     plt.fill_between(unique_param_values,                       np.array(mean_f1_scores) - np.array(std_f1_scores),                      np.array(mean_f1_scores) + np.array(std_f1_scores),                       alpha=0.2)          plt.title(f\"Grid Search Results for {param_name}\")     plt.xlabel(f\"{param_name}\")     plt.ylabel(\"Mean F1 Score\")     plt.legend()     plt.grid(True)     plt.show() In\u00a0[24]: Copied! <pre>plot_grid_search_results(grid_search, 'svm__C')\n</pre> plot_grid_search_results(grid_search, 'svm__C') <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 plot_grid_search_results(grid_search, 'svm__C')\n\nCell In[23], line 3, in plot_grid_search_results(grid_search, param_name)\n      2 def plot_grid_search_results(grid_search, param_name=\"svm__C\"):\n----&gt; 3     cv_results = grid_search.cv_results_\n      4     param_values = [p[param_name] for p in cv_results['params']]\n      5     unique_param_values = sorted(set(param_values))\n\nAttributeError: 'GridSearchCV' object has no attribute 'cv_results_'</pre> In\u00a0[25]: Copied! <pre>plot_grid_search_results(grid_search, 'QEK__mu')\n</pre> plot_grid_search_results(grid_search, 'QEK__mu') <pre>\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 plot_grid_search_results(grid_search, 'QEK__mu')\n\nCell In[23], line 3, in plot_grid_search_results(grid_search, param_name)\n      2 def plot_grid_search_results(grid_search, param_name=\"svm__C\"):\n----&gt; 3     cv_results = grid_search.cv_results_\n      4     param_values = [p[param_name] for p in cv_results['params']]\n      5     unique_param_values = sorted(set(param_values))\n\nAttributeError: 'GridSearchCV' object has no attribute 'cv_results_'</pre>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#tutorial-2-quantum-evolution-kernel-based-machine-learning-tutorial","title":"Tutorial 2: Quantum Evolution Kernel-Based Machine Learning Tutorial\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#introduction","title":"Introduction\u00b6","text":"<p>Machine Learning (ML) is a field of artificial intelligence that focuses on building systems capable of learning from data to make predictions or decisions. A common ML task is classification, where we try to assign labels to data points. In this tutorial, we will use the Quantum Evolution Kernel, a custom kernel that computes the similarity between graph-structured data to model a classification problem using Support Vector Machine (SVM) on graph dataset.</p>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#tutorial-objectives","title":"Tutorial Objectives\u00b6","text":"<p>In the previous part of this tutorial, we have loaded a dataset of molecules, labelled with their toxicity, and used a quantum device to extract machine-learning features. Now, our high-level objective is to use these features to predict the toxicity of molecules.</p> <p>This tutorial will guide you through:</p> <ol> <li>Loading and preprocessing a dataset for kernel-based machine learning.</li> <li>Introduction to Quantum Evolution Kernel (QEK), including fit and transform methods.</li> <li>Training and evaluating a Support Vector Machine (SVM) using the Quantum Evolution Kernel.</li> <li>Integrating the kernel and SVM into a scikit-learn Pipeline for streamlined workflows.</li> <li>Performing hyperparameter optimization using GridSearchCV to improve model performance.</li> </ol> <p>A companion notebook will guide you through using a Quantum Device to extract machine-learning features from graphs.</p> <p>In this tutorial, we use the results of the Quantum Device execution on a classical device (i.e. your computer) to create a Quantum Evolution Kernel. Since our algorithm combines steps that are executed on a Quantum Device and steps that are executed on a classical device, we call this a hybrid algorithm.</p> <p>This tutorial uses scikit-learn for common machine learning tasks, but the concepts would work with any other machine learning framework as well.</p>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#1-loading-the-data","title":"1. Loading the Data\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#2-quantum-evolution-kernel","title":"2. Quantum Evolution Kernel\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#introducing-the-quantum-evolution-kernel","title":"Introducing the Quantum Evolution Kernel\u00b6","text":"<p>For a graph $G$, let's call the excitation distribution $P_G$.</p> <p>We may now construct the Quantum Evolution Kernel, or QEK. Mathematically, QEK is defined as: $$ K(G, G') = \\exp \\left( -\\mu JS(P_G, P_{G'}) \\right) $$</p> <p>where $\\mu$ is an hyperparameter of our kernel and $JS$ is the Jensen-Shannon divergence.</p>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#fit-and-transform-graph-datasets-using-qek","title":"<code>Fit</code> and <code>Transform</code> graph datasets using <code>QEK</code>\u00b6","text":"<p>To use the kernel in machine learning algorithms, we can fit the kernel on a training dataset, and use it to transform training/testing datasets. The result of such a transformation is a kernel matrix, which represents the similarities between graphs.</p>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#3-training-a-simple-model","title":"3. Training a simple model\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#training","title":"Training\u00b6","text":"<p>This precomputed kernel will allow us to evaluate the algorithm QEK.</p>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#evaluation","title":"Evaluation\u00b6","text":"<p>We are using the following metrics:</p> <ul> <li><p>The F1 score is a way to measure how well a model performs, especially when the data is uneven (e.g., more examples of one category than another). It combines two important aspects: how precise the model is (precision) and how well it captures all the actual positives (recall). It provides a single number that balances these two aspects, making it useful for evaluating performance in real-world scenarios where some categories are much more common than others.</p> </li> <li><p>Balanced accuracy is a method to evaluate a model's performance fairly, even when the data is imbalanced (e.g., one category is much more frequent than others). Instead of just looking at overall accuracy, which can be misleading in such cases, balanced accuracy considers how well the model performs for each category separately and then averages these performances. This ensures that the evaluation is not skewed by the more common categories, giving a more honest picture of the model's effectiveness across all categories.</p> </li> </ul>"},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#4-creating-a-pipeline-with-svm","title":"4. Creating a Pipeline with SVM\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#5-gridsearchcv-for-hyperparameter-optimization","title":"5. GridSearchCV for Hyperparameter Optimization\u00b6","text":""},{"location":"tutorial%202%20-%20Machine-Learning%20with%20the%20Quantum%20EvolutionKernel/#evaluation","title":"Evaluation\u00b6","text":""},{"location":"usage/","title":"Usage","text":"<pre><code># Load a dataset\nimport torch_geometric.datasets as pyg_dataset\nog_ptcfm = pyg_dataset.TUDataset(root=\"dataset\", name=\"PTC_FM\")\n\n# Setup a quantum feature extractor for this dataset.\n# In this example, we'll use QutipExtractor, to emulate a Quantum Device on our machine.\nimport qek.data.graphs as qek_graphs\nimport qek.data.extractors as qek_extractors\nextractor = qek_extractors.QutipExtractor(compiler=qek_graphs.PTCFMCompiler())\n\n# Add the graphs, compile them and look at the results.\nextractor.add_graphs(graphs=og_ptcfm)\nextractor.compile()\nprocessed_dataset = extractor.run().processed_data\n\n# Prepare a machine learning pipeline with Scikit Learn.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nX = [data for data in processed_dataset]  # Features\ny = [data.target for data in processed_dataset]  # Targets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=42)\n\n# Train a kernel\nfrom qek.kernel import QuantumEvolutionKernel as QEK\nkernel = QEK(mu=0.5)\nmodel = SVC(kernel=kernel, random_state=42)\nmodel.fit(X_train, y_train)\n</code></pre>"},{"location":"api/qek/","title":"Reference","text":"qek<p> source package qek </p> <p>The Quantum Evolution Kernel is a Python library designed for the machine learning community to help users design quantum-driven similarity metrics for graphs and to use them inside kernel-based machine learning algorithms for graph data.</p> <p>The core of the library is focused on the development of a classification algorithm for molecular-graph dataset as it is presented in the published paper Quantum feature maps for graph machine learning on a neutral atom quantum processor.</p> <p>Users setting their first steps into quantum computing will learn how to implement the core algorithm in a few simple steps and run it using the Pasqal Neutral Atom QPU. More experienced users will find this library to provide the right environment to explore new ideas - both in terms of methodologies and data domain - while always interacting with a simple and intuitive QPU interface.</p> <p> Modules </p> <ul> <li> <p>qek.data \u2014 Data manipulation utilities.</p> </li> <li> <p>qek.kernel \u2014 The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> </li> <li> <p>qek.shared \u2014 Shared utility code.</p> </li> <li> <p>qek.target \u2014 Quantum compilation targets</p> </li> </ul>"},{"location":"src/qek/","title":"qek","text":"qek<p> docs package qek </p> <pre><code>\"\"\"\nThe Quantum Evolution Kernel is a Python library designed for the machine learning community to help users design quantum-driven similarity metrics for graphs and to use them inside kernel-based machine learning algorithms for graph data.\n\nThe core of the library is focused on the development of a classification algorithm for molecular-graph dataset as it is presented in the published paper [Quantum feature maps for graph machine learning on a neutral atom quantum processor](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.107.042615).\n\nUsers setting their first steps into quantum computing will learn how to implement the core algorithm in a few simple steps and run it using the Pasqal Neutral Atom QPU. More experienced users will find this library to provide the right environment to explore new ideas - both in terms of methodologies and data domain - while always interacting with a simple and intuitive QPU interface.\n\"\"\"\n</code></pre>"},{"location":"api/qek/data/","title":"qek.data","text":"qek.data<p> source package qek.data </p> <p>Data manipulation utilities.</p> <p> Modules </p> <ul> <li> <p>qek.data.extractors \u2014 High-Level API to compile raw data (graphs) and process it on a quantum device, either a local emulator, a remote emulator or a physical QPI.</p> </li> <li> <p>qek.data.graphs \u2014 Loading graphs as raw data.</p> </li> <li> <p>qek.data.processed_data \u2014 Loading, saving, manipulation or processed data.</p> </li> <li> <p>qek.data.training_data \u2014 Manipulating training data</p> </li> </ul>"},{"location":"src/qek/data/","title":"qek.data","text":"qek.data<p> docs package qek.data </p> <pre><code>\"\"\"\nData manipulation utilities.\n\"\"\"\n</code></pre>"},{"location":"api/qek/data/extractors/","title":"qek.data.extractors","text":"qek.data.extractors<p> source module qek.data.extractors </p> <p>High-Level API to compile raw data (graphs) and process it on a quantum device, either a local emulator, a remote emulator or a physical QPI.</p> <p> Classes </p> <ul> <li> <p>Compiled \u2014 The result of compiling a graph for execution on a quantum device.</p> </li> <li> <p>Feature \u2014 A feature extracted from raw data.</p> </li> <li> <p>BaseExtracted \u2014 Data extracted by one of the subclasses of <code>BaseExtractor</code>.</p> </li> <li> <p>SyncExtracted \u2014 Data extracted synchronously, i.e. no need to wait for a remote server.</p> </li> <li> <p>BaseExtractor \u2014 The base of the hierarchy of extractors.</p> </li> <li> <p>QutipExtractor \u2014 A Extractor that uses the Qutip Emulator to run sequences compiled from graphs.</p> </li> <li> <p>EmuMPSExtractor \u2014 A Extractor that uses the emu-mps Emulator to run sequences compiled from graphs.</p> </li> <li> <p>PasqalCloudExtracted \u2014 Data extracted from the cloud API, i.e. we need wait for a remote server.</p> </li> <li> <p>BaseRemoteExtractor \u2014 An Extractor that uses a remote Quantum Device published on Pasqal Cloud, to run sequences compiled from graphs.</p> </li> <li> <p>RemoteQPUExtractor \u2014 An Extractor that uses a remote QPU published on Pasqal Cloud, to run sequences compiled from graphs.</p> </li> <li> <p>RemoteEmuMPSExtractor \u2014 An Extractor that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud, to run sequences compiled from graphs.</p> </li> </ul> <p> source dataclass Compiled(graph: BaseGraph, sequence: pl.Sequence) </p> <p>The result of compiling a graph for execution on a quantum device.</p> <p> source dataclass Feature(data: NDArray[np.floating]) </p> <p>A feature extracted from raw data.</p> <p> source class BaseExtracted(device: Device) </p> <p><p>Bases : abc.ABC</p></p> <p>Data extracted by one of the subclasses of <code>BaseExtractor</code>.</p> <p>Note that the list of processed data will generally not contain all the graphs ingested by the Extractor, as not all graphs may not be compiled for a given device.</p> <p> Attributes </p> <ul> <li> <p>raw_data :  list[BaseGraph] \u2014 A subset of the graphs ingested by the Extractor.</p> </li> <li> <p>targets :  list[int] | None \u2014 If available, the machine-learning targets for these graphs, in the same order and with the same number of entrie as <code>raw_data</code>.</p> </li> <li> <p>states :  list[dict[str, int]] \u2014 The quantum states extracted from <code>raw_data</code> by executing <code>sequences</code> on the device, in the same order and with the same number of entries as <code>raw_data</code>.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>features \u2014 The features extracted from <code>raw_data</code> by processing <code>states</code>, in the same order and with the same number of entries as <code>raw_data</code>.</p> </li> <li> <p>save_dataset \u2014 Saves the processed dataset to a JSON file.</p> </li> </ul> <p> source property BaseExtracted.processed_data: list[processed_data.ProcessedData] </p> <p> source property BaseExtracted.raw_data: list[BaseGraph] </p> <p>A subset of the graphs ingested by the Extractor.</p> <p> source property BaseExtracted.targets: list[int] | None </p> <p>If available, the machine-learning targets for these graphs, in the same order and with the same number of entrie as <code>raw_data</code>.</p> <p> source property BaseExtracted.states: list[dict[str, int]] </p> <p>The quantum states extracted from <code>raw_data</code> by executing <code>sequences</code> on the device, in the same order and with the same number of entries as <code>raw_data</code>.</p> <p> source method BaseExtracted.features(size_max: int | None) \u2192 list[Feature] </p> <p>The features extracted from <code>raw_data</code> by processing <code>states</code>, in the same order and with the same number of entries as <code>raw_data</code>.</p> <p>By default, the features extracted are the distribution of excitation levels based on <code>states</code>. However, subclasses may override this method to provide custom features extraction.</p> <p> Parameters </p> <ul> <li> <p>size_max (optional) Performance/precision lever. If specified, specifies the number of qubits to take into account from all \u2014 the <code>states</code>. If <code>size_max</code> is lower than the number of qubits used to extract <code>self.states[i]</code> (i.e. the number of qubits in <code>self.sequences[i]</code>), then only take into account the <code>size_max</code> first qubits of this state to extract <code>self.features(size_max)[i]</code>. If, on the other hand, <code>size_max</code> is greater than the number of qubits used to extract <code>self.states[i]</code>, pad <code>self.features(size_max)[i]</code> with 0s. If unspecified, use the largest number of qubits in <code>selfsequences</code>.</p> </li> </ul> <p> source method BaseExtracted.save_dataset(file_path: Path) \u2192 None </p> <p>Saves the processed dataset to a JSON file.</p> <p>Note: This does NOT attempt to save the graphs.</p> <p> Parameters </p> <ul> <li> <p>dataset \u2014 The dataset to be saved.</p> </li> <li> <p>file_path :  Path \u2014 The path where the dataset will be saved as a JSON file.</p> </li> </ul> <p>Note</p> <p>The data is stored in a format suitable for loading with load_dataset.</p> <p> source class SyncExtracted(raw_data: list[BaseGraph], targets: list[int] | None, sequences: list[pl.Sequence], states: list[dict[str, int]]) </p> <p><p>Bases : BaseExtracted</p></p> <p>Data extracted synchronously, i.e. no need to wait for a remote server.</p> <p> source property SyncExtracted.processed_data: list[ProcessedData] </p> <p> source property SyncExtracted.raw_data: list[BaseGraph] </p> <p> source property SyncExtracted.targets: list[int] | None </p> <p> source property SyncExtracted.sequences: list[pl.Sequence] </p> <p> source property SyncExtracted.states: list[dict[str, int]] </p> <p> source class BaseExtractor(device: Device, compiler: BaseGraphCompiler[GraphType], path: Path | None = None) </p> <p><p>Bases : abc.ABC, Generic[GraphType]</p></p> <p>The base of the hierarchy of extractors.</p> <p>The role of extractors is to take a list of raw data (here, labelled graphs) into processed data containing machine-learning features (here, excitation vectors).</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 If specified, the processed data will be saved to this file as JSON once the execution is complete.</p> </li> <li> <p>device :  Device \u2014 A quantum device for which the data should be prepared.</p> </li> <li> <p>compiler :  BaseGraphCompiler[GraphType] \u2014 A graph compiler, in charge of converting graphs to Pulser Sequences, the format that can be executed on a quantum device.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>save \u2014 Saves a dataset to a JSON file.</p> </li> <li> <p>compile \u2014 Compile all pending graphs into Pulser sequences that the Quantum Device may execute.</p> </li> <li> <p>add_graphs \u2014 Add new graphs to compile and run.</p> </li> <li> <p>run \u2014 Run compiled graphs.</p> </li> </ul> <p> source method BaseExtractor.save(snapshot: list[ProcessedData]) \u2192 None </p> <p>Saves a dataset to a JSON file.</p> <p> Parameters </p> <ul> <li> <p>dataset :  list[ProcessedData] \u2014 The dataset to be saved, containing RegisterData instances.</p> </li> <li> <p>file_path :  str \u2014 The path where the dataset will be saved as a JSON file.</p> </li> </ul> <p>Note</p> <p>The data is stored in a format suitable for loading with load_dataset.</p> <p> source method BaseExtractor.compile(filter: Callable[[BaseGraph, pl.Sequence, int], bool] | None = None) \u2192 list[Compiled] </p> <p>Compile all pending graphs into Pulser sequences that the Quantum Device may execute.</p> <p>Once this method has succeeded, the results are stored in <code>self.sequences</code>.</p> <p> Raises </p> <ul> <li> <p>Exception</p> </li> </ul> <p> source method BaseExtractor.add_graphs(graphs: Sequence[GraphType] | Dataset[GraphType]) \u2192 None </p> <p>Add new graphs to compile and run.</p> <p> source method BaseExtractor.run() \u2192 BaseExtracted </p> <p>Run compiled graphs.</p> <p>You will need to call <code>self.compile</code> first, to make sure that the graphs are compiled.</p> <p> Returns </p> <ul> <li> <p>BaseExtracted \u2014 Data extracted by this extractor.</p> <p>Not all extractors may return the same data, so please take a look at the documentation of the extractor you are using.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>Exception</p> </li> </ul> <p> source class QutipExtractor(compiler: BaseGraphCompiler[GraphType], device: Device = pl.devices.AnalogDevice, path: Path | None = None) </p> <p><p>Bases : BaseExtractor[GraphType]</p></p> <p>A Extractor that uses the Qutip Emulator to run sequences compiled from graphs.</p> <p>Performance note: emulating a quantum device on a classical computer requires considerable amount of resources, so this Extractor may be slow or require too much memory.</p> <p>See Also</p> <ul> <li>EmuMPSExtractor (alternative emulator, generally much faster)</li> <li>QPUExtractor (run on a physical QPU)</li> </ul> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>compiler :  BaseGraphCompiler[GraphType] \u2014 A graph compiler, in charge of converting graphs to Pulser Sequences, the format that can be executed on a quantum device.</p> </li> <li> <p>device :  Device \u2014 A device to use. For general experiments, the default device <code>AnalogDevice</code> is a perfectly reasonable choice.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run \u2014 Run the compiled graphs.</p> </li> </ul> <p> source method QutipExtractor.run(max_qubits: int = 8) \u2192 SyncExtracted </p> <p>Run the compiled graphs.</p> <p>As emulating a quantum device is slow consumes resources and time exponential in the number of qubits, for the sake of performance, we limit the number of qubits in the execution of this extractor.</p> <p> Parameters </p> <ul> <li> <p>max_qubits :  int \u2014 Skip any sequence that require strictly more than <code>max_qubits</code>. Defaults to 8.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>SyncExtracted \u2014 Processed data for all the sequences that were executed.</p> </li> </ul> <p> source class EmuMPSExtractor(compiler: BaseGraphCompiler[GraphType], device: Device = pl.devices.AnalogDevice, path: Path | None = None) </p> <p><p>Bases : BaseExtractor[GraphType]</p></p> <p>A Extractor that uses the emu-mps Emulator to run sequences compiled from graphs.</p> <p>Performance note: emulating a quantum device on a classical computer requires considerable amount of resources, so this Extractor may be slow or require too much memory. If should, however, be faster than QutipExtractor in most cases.</p> <p>See Also</p> <ul> <li>QPUExtractor (run on a physical QPU)</li> </ul> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>compiler :  BaseGraphCompiler[GraphType] \u2014 A graph compiler, in charge of converting graphs to Pulser Sequences, the format that can be executed on a quantum device.</p> </li> <li> <p>device :  Device \u2014 A device to use. For general experiments, the default device <code>AnalogDevice</code> is a perfectly reasonable choice.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run \u2014 Run the compiled graphs.</p> </li> </ul> <p> source method EmuMPSExtractor.run(max_qubits: int = 10, dt: int = 10) \u2192 BaseExtracted </p> <p>Run the compiled graphs.</p> <p>As emulating a quantum device is slow consumes resources and time exponential in the number of qubits, for the sake of performance, we limit the number of qubits in the execution of this extractor.</p> <p> Parameters </p> <ul> <li> <p>max_qubits :  int \u2014 Skip any sequence that require strictly more than <code>max_qubits</code>. Defaults to 8.</p> </li> <li> <p>dt :  int \u2014 The duration of the simulation step, in us. Defaults to 10.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>BaseExtracted \u2014 Processed data for all the sequences that were executed.</p> </li> </ul> <p> source class PasqalCloudExtracted(compiled: list[Compiled], batch_ids: list[str], sdk: SDK, state_extractor: Callable[[Job, pl.Sequence], dict[str, int] | None], path: Path | None = None) </p> <p><p>Bases : BaseExtracted</p></p> <p>Data extracted from the cloud API, i.e. we need wait for a remote server.</p> <p>Prepare for reception of data.</p> <p> Performance note </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you do not need to do so.</p> <p> Parameters </p> <ul> <li> <p>compiled :  list[Compiled] \u2014 The result of compiling a set of graphs.</p> </li> <li> <p>batch_ids :  list[str] \u2014 The ids of the batches on the cloud API, in the same order as <code>compiled</code>.</p> </li> <li> <p>state_extractor :  Callable[[Job, pl.Sequence], dict[str, int] | None] \u2014 A callback used to extract the counter from a job. Used as various cloud back-ends return different formats.</p> </li> <li> <p>path :  Path | None \u2014 If provided, a path at which to save the results once they're available.</p> </li> </ul> <p> source property PasqalCloudExtracted.processed_data: list[ProcessedData] </p> <p> source property PasqalCloudExtracted.raw_data: list[BaseGraph] </p> <p> source property PasqalCloudExtracted.targets: list[int] | None </p> <p> source property PasqalCloudExtracted.sequences: list[pl.Sequence] </p> <p> source property PasqalCloudExtracted.states: list[dict[str, int]] </p> <p> source class BaseRemoteExtractor(compiler: BaseGraphCompiler[GraphType], project_id: str, username: str, device_name: str, password: str | None = None, batch_ids: list[str] | None = None, path: Path | None = None) </p> <p><p>Bases : BaseExtractor[GraphType], Generic[GraphType]</p></p> <p>An Extractor that uses a remote Quantum Device published on Pasqal Cloud, to run sequences compiled from graphs.</p> <p> Performance note (servers and interactive applications only) </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you may ignore this performance note.</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> <li> <p>batch_id \u2014 Use this to resume a workflow e.g. after turning off your computer while the QPU was executing your sequences. Warning: A batch started with one executor MUST NOT be resumed with a different executor.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run \u2014 Launch the extraction.</p> </li> </ul> <p> source property BaseRemoteExtractor.batch_ids: list[str] | None </p> <p> source method BaseRemoteExtractor.run() \u2192 PasqalCloudExtracted </p> <p>Launch the extraction.</p> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source class RemoteQPUExtractor(compiler: BaseGraphCompiler[GraphType], project_id: str, username: str, device_name: str = 'FRESNEL', password: str | None = None, batch_ids: list[str] | None = None, path: Path | None = None) </p> <p><p>Bases : BaseRemoteExtractor[GraphType]</p></p> <p>An Extractor that uses a remote QPU published on Pasqal Cloud, to run sequences compiled from graphs.</p> <p> Performance note </p> <p>as of this writing, the waiting lines for a QPU may be very long. You may use this Extractor to resume your workflow with a computation that has been previously started.</p> <p> Performance note (servers and interactive applications only) </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you may ignore this performance note.</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> <li> <p>batch_id \u2014 Use this to resume a workflow e.g. after turning off your computer while the QPU was executing your sequences.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source method RemoteQPUExtractor.run() \u2192 PasqalCloudExtracted </p> <p> source class RemoteEmuMPSExtractor(compiler: BaseGraphCompiler[GraphType], project_id: str, username: str, device_name: str = 'FRESNEL', password: str | None = None, batch_ids: list[str] | None = None, path: Path | None = None) </p> <p><p>Bases : BaseRemoteExtractor[GraphType]</p></p> <p>An Extractor that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud, to run sequences compiled from graphs.</p> <p> Performance note (servers and interactive applications only) </p> <p>If your code is meant to be executed as part of an interactive application or a server, you should consider calling <code>await extracted</code> before your first call to any of the methods of <code>extracted</code>. Otherwise, you will block the main thread.</p> <p>If you are running this as part of an experiment, a Jupyter notebook, etc. you may ignore this performance note.</p> <p> Parameters </p> <ul> <li> <p>path :  Path | None \u2014 Path to store the result of the run, for future uses. To reload the result of a previous run, use <code>LoadExtractor</code>.</p> </li> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> <li> <p>batch_id \u2014 Use this to resume a workflow e.g. after turning off your computer while the QPU was executing your sequences.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source method RemoteEmuMPSExtractor.run(dt: int = 10) \u2192 PasqalCloudExtracted </p>"},{"location":"src/qek/data/extractors/","title":"qek.data.extractors","text":"qek.data.extractors<p> docs module qek.data.extractors </p> <pre><code>\"\"\"\nHigh-Level API to compile raw data (graphs) and process it on a quantum device, either a local emulator,\na remote emulator or a physical QPI.\n\"\"\"\n\nimport abc\nimport asyncio\nfrom dataclasses import dataclass\nimport itertools\nimport json\nimport logging\nfrom math import ceil\nfrom uuid import UUID\nimport time\nfrom typing import Any, Callable, Generator, Generic, Sequence, TypeVar, cast\nfrom numpy.typing import NDArray\nfrom pasqal_cloud import SDK\nfrom pasqal_cloud.batch import Batch\nfrom pasqal_cloud.device import BaseConfig, EmuTNConfig, EmulatorType\nfrom pasqal_cloud.job import Job\nfrom pasqal_cloud.utils.filters import BatchFilters\nfrom pathlib import Path\nimport numpy as np\nimport os\nimport pulser as pl\nfrom pulser.devices import Device\nfrom pulser.json.abstract_repr.deserializer import deserialize_device\nfrom pulser_simulation import QutipEmulator\nfrom torch.utils.data import Dataset\n\nfrom qek.data import processed_data\nfrom qek.data.graphs import BaseGraph, BaseGraphCompiler\nfrom qek.data.processed_data import ProcessedData\nfrom qek.shared.error import CompilationError\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Compiled:docs\n    \"\"\"\n    The result of compiling a graph for execution on a quantum device.\n    \"\"\"\n\n    # Future plans: as of this writing, this class (or a reworked version of it)\n    # is expected to move to the `qool-layer` library.\n\n    # The graph itself.\n    graph: BaseGraph\n\n    # A sequence adapted to the quantum device.\n    sequence: pl.Sequence\n\n\n@dataclass\nclass Feature:docs\n    \"\"\"\n    A feature extracted from raw data.\n    \"\"\"\n\n    data: NDArray[np.floating]\n\n\nclass BaseExtracted(abc.ABC):docs\n    \"\"\"\n    Data extracted by one of the subclasses of `BaseExtractor`.\n\n    Note that the list of processed data will generally *not* contain all the graphs ingested\n    by the Extractor, as not all graphs may not be compiled for a given device.\n    \"\"\"\n\n    def __init__(self, device: Device):\n        self.device = device\n\n    def __await__(self) -&gt; Generator[Any, Any, None]:\n        \"\"\"\n        Wait asynchronously until execution is ready.\n\n        This will avoid blocking your main thread, so calling this method once,\n        before the first call to `processed_data`, is strongly recommended\n        for use on a server or an interactive application.\n        \"\"\"\n        # By default, no need to wait.\n        yield None\n\n    @property\n    @abc.abstractmethod\n    def processed_data(self) -&gt; list[processed_data.ProcessedData]:docs\n        pass\n\n    @property\n    @abc.abstractmethod\n    def raw_data(self) -&gt; list[BaseGraph]:docs\n        \"\"\"\n        A subset of the graphs ingested by the Extractor.\n        \"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def targets(self) -&gt; list[int] | None:docs\n        \"\"\"\n        If available, the machine-learning targets for these graphs, in the same order and with the same number of entrie as `raw_data`.\n        \"\"\"\n        pass\n\n    @property\n    def states(self) -&gt; list[dict[str, int]]:docs\n        \"\"\"\n        The quantum states extracted from `raw_data` by executing `sequences` on the device, in the same order and with the same number of entries as `raw_data`.\n        \"\"\"\n        return [data.state_dict for data in self.processed_data]\n\n    def features(self, size_max: int | None) -&gt; list[Feature]:docs\n        \"\"\"\n        The features extracted from `raw_data` by processing `states`, in the same order and with the same number of entries as `raw_data`.\n\n        By default, the features extracted are the distribution of excitation levels based on `states`. However, subclasses may override\n        this method to provide custom features extraction.\n\n        Arguments:\n            size_max (optional) Performance/precision lever. If specified, specifies the number of qubits to take into account from all\n                the `states`. If `size_max` is lower than the number of qubits used to extract `self.states[i]` (i.e. the number of qubits\n                in `self.sequences[i]`), then only take into account the `size_max` first qubits of this state to extract\n                `self.features(size_max)[i]`. If, on the other hand, `size_max` is greater than the number of qubits used to extract\n                `self.states[i]`, pad `self.features(size_max)[i]` with 0s.\n                If unspecified, use the largest number of qubits in `selfsequences`.\n        \"\"\"\n        if size_max is None:\n            for data in self.processed_data:\n                seq = data._sequence\n                if size_max is None or len(seq.qubit_info) &gt; size_max:\n                    size_max = len(seq.qubit_info)\n        if size_max is None:\n            # The only way size_max can be None is if `self.sequences` is empty.\n            return []\n\n        return [Feature(processed_data.dist_excitation(state, size_max)) for state in self.states]\n\n    def save_dataset(self, file_path: Path) -&gt; None:docs\n        \"\"\"Saves the processed dataset to a JSON file.\n\n        Note: This does NOT attempt to save the graphs.\n\n        Args:\n            dataset: The dataset to be saved.\n            file_path: The path where the dataset will be saved as a JSON\n                file.\n\n        Note:\n            The data is stored in a format suitable for loading with load_dataset.\n        \"\"\"\n        with open(file_path, \"w\") as file:\n            states = self.states\n            targets = self.targets\n            data = [\n                {\n                    \"sequence\": self.processed_data[i]._sequence.to_abstract_repr(),\n                    # Some emulators will actually be `dict[str, int64]` instead of `dict[str, int]` and `int64`\n                    # is not JSON-serializable.\n                    #\n                    # The reason for which `int64` is not JSON-serializable is that JSON limits ints to 2^53-1.\n                    # However, in practice, this should not be a problem, since the `int`/`int64` in our dict is\n                    # limited to the number of runs, and we don't expect to be launching 2^53 consecutive runs\n                    # for a single sequence on a device in any foreseeable future (assuming a run of 1ns,\n                    # this would still take ~4 billion years to execute).\n                    \"state_dict\": {key: int(value) for (key, value) in states[i].items()},\n                    \"target\": targets[i] if targets is not None else None,\n                }\n                for i in range(len(self.processed_data))\n            ]\n            json.dump(data, file)\n        logger.info(\"processed data saved to %s\", file_path)\n\n\nclass SyncExtracted(BaseExtracted):docs\n    \"\"\"\n    Data extracted synchronously, i.e. no need to wait for a remote server.\n    \"\"\"\n\n    def __init__(\n        self,\n        raw_data: list[BaseGraph],\n        targets: list[int] | None,\n        sequences: list[pl.Sequence],\n        states: list[dict[str, int]],\n    ):\n        assert len(raw_data) == len(sequences)\n        assert len(sequences) == len(states)\n        if targets is not None:\n            if len(targets) &lt; len(sequences):\n                # Not all graphs come with a target.\n                #\n                # This Extracted will not be usable as the training sample, so ignore all targets.\n                if len(targets) != 0:\n                    logger.debug(\n                        \"We compiled %s graphs but we only have %s targets, ignoring all targets\",\n                        len(sequences),\n                        len(targets),\n                    )\n                targets = None\n        self._raw_data = raw_data\n        self._targets = targets\n        self._sequences = sequences\n        self._states = states\n        self._processed_data = [\n            ProcessedData(\n                sequence=seq, state_dict=cast(dict[str, int | np.int64], state), target=target\n            )\n            for (seq, state, target) in itertools.zip_longest(sequences, states, targets or [])\n        ]\n\n    @property\n    def processed_data(self) -&gt; list[ProcessedData]:docs\n        return self._processed_data\n\n    @property\n    def raw_data(self) -&gt; list[BaseGraph]:docs\n        return self._raw_data\n\n    @property\n    def targets(self) -&gt; list[int] | None:docs\n        return self._targets\n\n    @property\n    def sequences(self) -&gt; list[pl.Sequence]:docs\n        return self._sequences\n\n    @property\n    def states(self) -&gt; list[dict[str, int]]:docs\n        return self._states\n\n\n# Type variable for BaseExtractor[GraphType].\nGraphType = TypeVar(\"GraphType\")\n\n\nclass BaseExtractor(abc.ABC, Generic[GraphType]):docs\n    \"\"\"\n    The base of the hierarchy of extractors.\n\n    The role of extractors is to take a list of raw data (here, labelled graphs) into\n    processed data containing machine-learning features (here, excitation vectors).\n\n    Args:\n        path: If specified, the processed data will be saved to this file as JSON once\n            the execution is complete.\n        device: A quantum device for which the data should be prepared.\n        compiler: A graph compiler, in charge of converting graphs to Pulser Sequences,\n            the format that can be executed on a quantum device.\n    \"\"\"\n\n    def __init__(\n        self, device: Device, compiler: BaseGraphCompiler[GraphType], path: Path | None = None\n    ) -&gt; None:\n        self.path = path\n\n        # The list of graphs (raw data). Fill it with `self.add_graphs`.\n        self.graphs: list[BaseGraph] = []\n        self.device: Device = device\n\n        # The compiled sequences. Filled with `self.compile`.\n        # Note that the list of compiled sequences may be shorter than the list of\n        # raw data, as not all graphs may be compiled to a given `device`.\n        self.sequences: list[Compiled] = []\n        self.compiler = compiler\n\n        # A counter used to give a unique id to each graph.\n        self._counter = 0\n\n    def save(self, snapshot: list[ProcessedData]) -&gt; None:docs\n        \"\"\"Saves a dataset to a JSON file.\n\n        Args:\n            dataset (list[ProcessedData]): The dataset to be saved, containing\n                RegisterData instances.\n            file_path (str): The path where the dataset will be saved as a JSON\n                file.\n\n        Note:\n            The data is stored in a format suitable for loading with load_dataset.\n        \"\"\"\n        if self.path is not None:\n            with open(self.path, \"w\") as file:\n                data = [\n                    {\n                        \"sequence\": instance._sequence.to_abstract_repr(),\n                        \"state_dict\": instance.state_dict,\n                        \"target\": instance.target,\n                    }\n                    for instance in snapshot\n                ]\n                json.dump(data, file)\n            logger.info(\"processed data saved to %s\", self.path)\n\n    def compile(docs\n        self, filter: Callable[[BaseGraph, pl.Sequence, int], bool] | None = None\n    ) -&gt; list[Compiled]:\n        \"\"\"\n        Compile all pending graphs into Pulser sequences that the Quantum Device may execute.\n\n        Once this method has succeeded, the results are stored in `self.sequences`.\n        \"\"\"\n        if len(self.graphs) == 0:\n            raise Exception(\"No graphs to compile, did you forget to call `import_graphs`?\")\n        if filter is None:\n            filter = lambda _graph, sequence, _index: True  # noqa: E731\n        self.sequences = []\n        for graph in self.graphs:\n            try:\n                register = graph.compile_register()\n                pulse = graph.compile_pulse()\n                sequence = pl.Sequence(register=register.register, device=graph.device)\n                sequence.declare_channel(\"ising\", \"rydberg_global\")\n                sequence.add(pulse.pulse, \"ising\")\n            except CompilationError as e:\n                # In some cases, we produce graphs that pass `is_embeddable` but cannot be compiled.\n                # It _looks_ like this is due to rounding errors. We're investigating this in issue #29,\n                # but for the time being, we're simply logging and skipping them.\n                logger.debug(\"Graph #%s could not be compiled (%s), skipping\", graph.id, e)\n                continue\n            if not filter(graph, sequence, graph.id):\n                logger.debug(\"Graph #%s did not pass filtering, skipping\", graph.id)\n                continue\n            logger.debug(\"Compiling graph #%s for execution on the device\", graph.id)\n            self.sequences.append(Compiled(graph=graph, sequence=sequence))\n        logger.debug(\"Compilation step complete, %s graphs compiled\", len(self.sequences))\n        return self.sequences\ndocs\n    def add_graphs(self, graphs: Sequence[GraphType] | Dataset[GraphType]) -&gt; None:\n        \"\"\"\n        Add new graphs to compile and run.\n        \"\"\"\n        for graph in graphs:\n            self._counter += 1\n            id = self._counter\n            logger.debug(\"ingesting # %s\", id)\n            processed = self.compiler.ingest(graph=graph, device=self.device, id=id)\n            # Skip graphs that are not embeddable.\n            if processed.is_embeddable():\n                logger.debug(\"graph # %s is embeddable, accepting\", id)\n                self.graphs.append(processed)\n            else:\n                logger.info(\"graph # %s is not embeddable, skipping\", id)\n        logger.info(\"imported %s graphs\", len(self.graphs))\n\n    @abc.abstractmethod\n    def run(self) -&gt; BaseExtracted:docs\n        \"\"\"\n        Run compiled graphs.\n\n        You will need to call `self.compile` first, to make sure that the graphs are compiled.\n\n        Returns:\n            Data extracted by this extractor.\n\n            Not all extractors may return the same data, so please take a look at the documentation\n            of the extractor you are using.\n        \"\"\"\n        raise Exception(\"Not implemented\")\n\n\nclass QutipExtractor(BaseExtractor[GraphType]):docs\n    \"\"\"\n    A Extractor that uses the Qutip Emulator to run sequences compiled\n    from graphs.\n\n    Performance note: emulating a quantum device on a classical\n    computer requires considerable amount of resources, so this\n    Extractor may be slow or require too much memory.\n\n    See also:\n    - EmuMPSExtractor (alternative emulator, generally much faster)\n    - QPUExtractor (run on a physical QPU)\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        compiler: A graph compiler, in charge of converting graphs to Pulser Sequences,\n            the format that can be executed on a quantum device.\n        device: A device to use. For general experiments, the default\n            device `AnalogDevice` is a perfectly reasonable choice.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        device: Device = pl.devices.AnalogDevice,\n        path: Path | None = None,\n    ):\n        super().__init__(path=path, device=device, compiler=compiler)\n        self.graphs: list[BaseGraph]\n        self.device = device\n\n    def run(self, max_qubits: int = 8) -&gt; SyncExtracted:docs\n        \"\"\"\n        Run the compiled graphs.\n\n        As emulating a quantum device is slow consumes resources and time exponential in the\n        number of qubits, for the sake of performance, we limit the number of qubits in the execution\n        of this extractor.\n\n        Args:\n            max_qubits: Skip any sequence that require strictly more than `max_qubits`. Defaults to 8.\n\n        Returns:\n            Processed data for all the sequences that were executed.\n        \"\"\"\n        if len(self.sequences) == 0:\n            logger.warning(\"No sequences to run, did you forget to call compile()?\")\n            return SyncExtracted(raw_data=[], targets=[], sequences=[], states=[])\n\n        raw_data: list[BaseGraph] = []\n        targets: list[int] = []\n        sequences: list[pl.Sequence] = []\n        states: list[dict[str, int]] = []\n        for compiled in self.sequences:\n            qubits_used = len(compiled.sequence.qubit_info)\n            if qubits_used &gt; max_qubits:\n                logger.info(\n                    \"Graph %s exceeds the qubit limit specified in QutipExtractor (%s &gt; %s), skipping\",\n                    id,\n                    qubits_used,\n                    max_qubits,\n                )\n                continue\n            logger.debug(\"Executing compiled graph # %s\", id)\n            simul = QutipEmulator.from_sequence(sequence=compiled.sequence)\n            counter = cast(dict[str, Any], simul.run().sample_final_state())\n            logger.debug(\"Execution of compiled graph # %s complete\", id)\n            raw_data.append(compiled.graph)\n            if compiled.graph.target is not None:\n                targets.append(compiled.graph.target)\n            sequences.append(compiled.sequence)\n            states.append(counter)\n\n        result = SyncExtracted(\n            raw_data=raw_data, targets=targets, sequences=sequences, states=states\n        )\n        logger.debug(\"Emulation step complete, %s compiled graphs executed\", len(raw_data))\n        if self.path is not None:\n            result.save_dataset(self.path)\n        return result\n\n\nif os.name == \"posix\":\n    # Any Unix including Linux and macOS\n\n    import emu_mps\n\n    class EmuMPSExtractor(BaseExtractor[GraphType]):docs\n        \"\"\"\n        A Extractor that uses the emu-mps Emulator to run sequences compiled\n        from graphs.\n\n        Performance note: emulating a quantum device on a classical\n        computer requires considerable amount of resources, so this\n        Extractor may be slow or require too much memory. If should,\n        however, be faster than QutipExtractor in most cases.\n\n        See also:\n        - QPUExtractor (run on a physical QPU)\n\n        Args:\n            path: Path to store the result of the run, for future uses.\n                To reload the result of a previous run, use `LoadExtractor`.\n            compiler: A graph compiler, in charge of converting graphs to Pulser Sequences,\n                the format that can be executed on a quantum device.\n            device: A device to use. For general experiments, the default\n                device `AnalogDevice` is a perfectly reasonable choice.\n        \"\"\"\n\n        def __init__(\n            self,\n            compiler: BaseGraphCompiler[GraphType],\n            device: Device = pl.devices.AnalogDevice,\n            path: Path | None = None,\n        ):\n            super().__init__(device=device, compiler=compiler, path=path)\n            self.graphs: list[BaseGraph]\n            self.device = device\ndocs\n        def run(self, max_qubits: int = 10, dt: int = 10) -&gt; BaseExtracted:\n            \"\"\"\n            Run the compiled graphs.\n\n            As emulating a quantum device is slow consumes resources and time exponential in the\n            number of qubits, for the sake of performance, we limit the number of qubits in the execution\n            of this extractor.\n\n            Args:\n                max_qubits: Skip any sequence that require strictly more than `max_qubits`. Defaults to 8.\n                dt: The duration of the simulation step, in us. Defaults to 10.\n\n            Returns:\n                Processed data for all the sequences that were executed.\n            \"\"\"\n            if len(self.sequences) == 0:\n                logger.warning(\"No sequences to run, did you forget to call compile()?\")\n                return SyncExtracted(raw_data=[], targets=[], sequences=[], states=[])\n\n            backend = emu_mps.MPSBackend()\n            raw_data = []\n            targets: list[int] = []\n            sequences = []\n            states = []\n            for compiled in self.sequences:\n                qubits_used = len(compiled.sequence.qubit_info)\n                if qubits_used &gt; max_qubits:\n                    logger.info(\n                        \"Graph %s exceeds the qubit limit specified in EmuMPSExtractor (%s &gt; %s), skipping\",\n                        id,\n                        qubits_used,\n                        max_qubits,\n                    )\n                    continue\n                logger.debug(\"Executing compiled graph # %s\", id)\n\n                # Configure observable.\n                cutoff_duration = int(ceil(compiled.sequence.get_duration() / dt) * dt)\n                observable = emu_mps.BitStrings(evaluation_times={cutoff_duration})\n                config = emu_mps.MPSConfig(observables=[observable], dt=dt)\n                counter: dict[str, Any] = backend.run(compiled.sequence, config)[observable.name][\n                    cutoff_duration\n                ]\n                logger.debug(\"Execution of compiled graph # %s complete\", id)\n                raw_data.append(compiled.graph)\n                if compiled.graph.target is not None:\n                    targets.append(compiled.graph.target)\n                sequences.append(compiled.sequence)\n                states.append(counter)\n\n            logger.debug(\"Emulation step complete, %s compiled graphs executed\", len(raw_data))\n\n            result = SyncExtracted(\n                raw_data=raw_data, targets=targets, sequences=sequences, states=states\n            )\n            logger.debug(\"Emulation step complete, %s compiled graphs executed\", len(raw_data))\n            if self.path is not None:\n                result.save_dataset(self.path)\n            return result\n\n\n# How many seconds to sleep while waiting for the results from the cloud.\nSLEEP_DELAY_S = 2\n\n\nclass PasqalCloudExtracted(BaseExtracted):docs\n    \"\"\"\n    Data extracted from the cloud API, i.e. we need wait for a remote server.\n\n    Performance note:\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        do not need to do so.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiled: list[Compiled],\n        batch_ids: list[str],\n        sdk: SDK,\n        state_extractor: Callable[[Job, pl.Sequence], dict[str, int] | None],\n        path: Path | None = None,\n    ):\n        \"\"\"\n        Prepare for reception of data.\n\n        Arguments:\n            compiled: The result of compiling a set of graphs.\n            batch_ids: The ids of the batches on the cloud API, in the same order as `compiled`.\n            state_extractor: A callback used to extract the counter from a job.\n                Used as various cloud back-ends return different formats.\n            path: If provided, a path at which to save the results once they're available.\n        \"\"\"\n        self._compiled = compiled\n        self._batch_ids = batch_ids\n        self._results: SyncExtracted | None = None\n        self._path = path\n        self._sdk = sdk\n        self._state_extractor = state_extractor\n\n    def _wait(self) -&gt; None:\n        \"\"\"\n        Wait synchronously until remote execution is ready.\n\n        This WILL BLOCK your main thread, possibly for a very long time.\n        \"\"\"\n        if self._results is not None:\n            # Results are already available.\n            return\n        pending_batch_ids: set[str] = set(self._batch_ids)\n        completed_batches: dict[str, Batch] = {}\n        while len(pending_batch_ids) &gt; 0:\n            time.sleep(SLEEP_DELAY_S)\n\n            # Fetch up to 100 pending batches (upstream limits).\n            MAX_BATCH_LEN = 100\n            check_ids: list[str | UUID] = [cast(str | UUID, id) for id in pending_batch_ids][\n                :MAX_BATCH_LEN\n            ]\n\n            # Update their status.\n            check_batches = self._sdk.get_batches(filters=BatchFilters(id=check_ids))\n            for batch in check_batches.results:\n                assert isinstance(batch, Batch)\n                if batch.status not in {\"PENDING\", \"RUNNING\"}:\n                    logger.debug(\"Job %s is now complete\", batch.id)\n                    pending_batch_ids.discard(batch.id)\n                    completed_batches[batch.id] = batch\n\n        # At this point, all batches are complete.\n        self._ingest(completed_batches)\n\n    def __await__(self) -&gt; Generator[Any, Any, None]:\n        \"\"\"\n        Wait asynchronously until remote execution is ready.\n\n        This will NOT block your main thread, so this method is strongly recommended\n        for use on a server or an interactive application.\n\n        Example:\n            await extracted\n        \"\"\"\n        if self._results is not None:\n            # Results are already available.\n            return\n        pending_batch_ids: set[str] = set(self._batch_ids)\n        completed_batches: dict[str, Batch] = {}\n        while len(pending_batch_ids) &gt; 0:\n            yield from asyncio.sleep(SLEEP_DELAY_S).__await__()\n\n            # Fetch up to 100 pending batches (upstream limits).\n            MAX_BATCH_LEN = 100\n            check_ids: list[str | UUID] = [cast(str | UUID, id) for id in pending_batch_ids][\n                :MAX_BATCH_LEN\n            ]\n\n            # Update their status.\n            check_batches = self._sdk.get_batches(\n                filters=BatchFilters(id=check_ids)\n            )  # Ideally, this should be async, see https://github.com/pasqal-io/pasqal-cloud/issues/162.\n            for batch in check_batches.results:\n                assert isinstance(batch, Batch)\n                if batch.status not in {\"PENDING\", \"RUNNING\"}:\n                    logger.debug(\"Job %s is now complete\", batch.id)\n                    pending_batch_ids.discard(batch.id)\n                    completed_batches[batch.id] = batch\n\n        # At this point, all batches are complete.\n        self._ingest(completed_batches)\n\n    def _ingest(self, batches: dict[str, Batch]) -&gt; None:\n        \"\"\"\n        Ingest data received from the remote server.\n\n        No I/O.\n        \"\"\"\n        assert len(batches) == len(self._batch_ids)\n\n        raw_data = []\n        targets: list[int] = []\n        sequences = []\n        states = []\n        for i, id in enumerate(self._batch_ids):\n            batch = batches[id]\n            compiled = self._compiled[i]\n            # Note: There's only one job per batch.\n            assert len(batch.jobs) == 1\n            for job in batch.jobs.values():\n                if job.status == \"DONE\":\n                    state_dict = self._state_extractor(job, compiled.sequence)\n                    if state_dict is None:\n                        logger.warning(\n                            \"Batch %s (graph %s) did not return a usable state, skipping\",\n                            i,\n                            compiled.graph.id,\n                        )\n                        continue\n                    raw_data.append(compiled.graph)\n                    if compiled.graph.target is not None:\n                        targets.append(compiled.graph.target)\n                    sequences.append(compiled.sequence)\n                    states.append(state_dict)\n                else:\n                    # If some sequences failed, let's skip them and proceed as well as we can.\n                    logger.warning(\n                        \"Batch %s (graph %s) failed with errors %s, skipping\",\n                        i,\n                        compiled.graph.id,\n                        job.status,\n                        job.errors,\n                    )\n        self._results = SyncExtracted(\n            raw_data=raw_data, targets=targets, sequences=sequences, states=states\n        )\n        if self._path is not None:\n            self.save_dataset(self._path)\n\n    @property\n    def processed_data(self) -&gt; list[ProcessedData]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.processed_data\n\n    @property\n    def raw_data(self) -&gt; list[BaseGraph]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.raw_data\n\n    @property\n    def targets(self) -&gt; list[int] | None:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.targets\n\n    @property\n    def sequences(self) -&gt; list[pl.Sequence]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.sequences\n\n    @property\n    def states(self) -&gt; list[dict[str, int]]:docs\n        self._wait()\n        assert self._results is not None\n        return self._results.states\n\n\nclass BaseRemoteExtractor(BaseExtractor[GraphType], Generic[GraphType]):docs\n    \"\"\"\n    An Extractor that uses a remote Quantum Device published\n    on Pasqal Cloud, to run sequences compiled from graphs.\n\n    Performance note (servers and interactive applications only):\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        may ignore this performance note.\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        project_id: The ID of the project on the Pasqal Cloud API.\n        username: Your username on the Pasqal Cloud API.\n        password: Your password on the Pasqal Cloud API. If you leave\n            this to None, you will need to enter your password manually.\n        device_name: The name of the device to use. As of this writing,\n            the default value of \"FRESNEL\" represents the latest QPU\n            available through the Pasqal Cloud API.\n        batch_id: Use this to resume a workflow e.g. after turning off\n            your computer while the QPU was executing your sequences.\n            Warning: A batch started with one executor MUST NOT be resumed\n            with a different executor.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        project_id: str,\n        username: str,\n        device_name: str,\n        password: str | None = None,\n        batch_ids: list[str] | None = None,\n        path: Path | None = None,\n    ):\n        sdk = SDK(username=username, project_id=project_id, password=password)\n\n        # Fetch the latest list of QPUs\n        specs = sdk.get_device_specs_dict()\n        device = cast(Device, deserialize_device(specs[device_name]))\n\n        super().__init__(device=device, compiler=compiler, path=path)\n        self._sdk = sdk\n        self._batch_ids: list[str] | None = batch_ids\n\n    @property\n    def batch_ids(self) -&gt; list[str] | None:docs\n        return self._batch_ids\n\n    @abc.abstractmethod\n    def run(docs\n        self,\n    ) -&gt; PasqalCloudExtracted:\n        \"\"\"\n        Launch the extraction.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _run(\n        self,\n        state_extractor: Callable[[Job, pl.Sequence], dict[str, int] | None],\n        emulator: EmulatorType | None,\n        config: BaseConfig | None,\n    ) -&gt; PasqalCloudExtracted:\n        if len(self.sequences) == 0:\n            logger.warning(\"No sequences to run, did you forget to call compile()?\")\n            return PasqalCloudExtracted(\n                compiled=[],\n                batch_ids=[],\n                sdk=self._sdk,\n                path=self.path,\n                state_extractor=state_extractor,\n            )\n\n        device: pl.devices.Device = self.sequences[0].sequence.device\n        # As of this writing, the API doesn't support runs longer than 500 jobs.\n        # If we want to add more runs, we'll need to split them across several jobs.\n        max_runs = device.max_runs if isinstance(device.max_runs, int) else 500\n\n        if self._batch_ids is None:\n            # Enqueue jobs.\n            self._batch_ids = []\n            for compiled in self.sequences:\n                logger.debug(\"Enqueuing execution of compiled graph #%s\", compiled.graph.id)\n                batch = self._sdk.create_batch(\n                    compiled.sequence.to_abstract_repr(),\n                    jobs=[{\"runs\": max_runs}],\n                    wait=False,\n                    emulator=emulator,\n                    configuration=config,\n                )\n                logger.info(\n                    \"Remote execution of compiled graph #%s starting, batched with id %s\",\n                    compiled.graph.id,\n                    batch.id,\n                )\n                self._batch_ids.append(batch.id)\n            logger.info(\n                \"All %s jobs enqueued for remote execution, with ids %s\",\n                len(self._batch_ids),\n                self._batch_ids,\n            )\n        assert len(self._batch_ids) == len(self.sequences)\n\n        return PasqalCloudExtracted(\n            compiled=self.sequences,\n            batch_ids=self._batch_ids,\n            sdk=self._sdk,\n            path=self.path,\n            state_extractor=state_extractor,\n        )\n\n\nclass RemoteQPUExtractor(BaseRemoteExtractor[GraphType]):docs\n    \"\"\"\n    An Extractor that uses a remote QPU published\n    on Pasqal Cloud, to run sequences compiled from graphs.\n\n    Performance note:\n        as of this writing, the waiting lines for a QPU\n        may be very long. You may use this Extractor to resume your workflow\n        with a computation that has been previously started.\n\n    Performance note (servers and interactive applications only):\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        may ignore this performance note.\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        project_id: The ID of the project on the Pasqal Cloud API.\n        username: Your username on the Pasqal Cloud API.\n        password: Your password on the Pasqal Cloud API. If you leave\n            this to None, you will need to enter your password manually.\n        device_name: The name of the device to use. As of this writing,\n            the default value of \"FRESNEL\" represents the latest QPU\n            available through the Pasqal Cloud API.\n        batch_id: Use this to resume a workflow e.g. after turning off\n            your computer while the QPU was executing your sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        project_id: str,\n        username: str,\n        device_name: str = \"FRESNEL\",\n        password: str | None = None,\n        batch_ids: list[str] | None = None,\n        path: Path | None = None,\n    ):\n        super().__init__(\n            compiler=compiler,\n            project_id=project_id,\n            username=username,\n            device_name=device_name,\n            password=password,\n            batch_ids=batch_ids,\n            path=path,\n        )\n\n    def run(self) -&gt; PasqalCloudExtracted:docs\n        return self._run(emulator=None, config=None, state_extractor=lambda job, _seq: job.result)\n\n\nclass RemoteEmuMPSExtractor(BaseRemoteExtractor[GraphType]):docs\n    \"\"\"\n    An Extractor that uses a remote high-performance emulator (EmuMPS)\n    published on Pasqal Cloud, to run sequences compiled from graphs.\n\n    Performance note (servers and interactive applications only):\n        If your code is meant to be executed as part of an interactive application or\n        a server, you should consider calling `await extracted` before your first call\n        to any of the methods of `extracted`. Otherwise, you will block the main thread.\n\n        If you are running this as part of an experiment, a Jupyter notebook, etc. you\n        may ignore this performance note.\n\n    Args:\n        path: Path to store the result of the run, for future uses.\n            To reload the result of a previous run, use `LoadExtractor`.\n        project_id: The ID of the project on the Pasqal Cloud API.\n        username: Your username on the Pasqal Cloud API.\n        password: Your password on the Pasqal Cloud API. If you leave\n            this to None, you will need to enter your password manually.\n        device_name: The name of the device to use. As of this writing,\n            the default value of \"FRESNEL\" represents the latest QPU\n            available through the Pasqal Cloud API.\n        batch_id: Use this to resume a workflow e.g. after turning off\n            your computer while the QPU was executing your sequences.\n    \"\"\"\n\n    def __init__(\n        self,\n        compiler: BaseGraphCompiler[GraphType],\n        project_id: str,\n        username: str,\n        device_name: str = \"FRESNEL\",\n        password: str | None = None,\n        batch_ids: list[str] | None = None,\n        path: Path | None = None,\n    ):\n        super().__init__(\n            compiler=compiler,\n            project_id=project_id,\n            username=username,\n            device_name=device_name,\n            password=password,\n            batch_ids=batch_ids,\n            path=path,\n        )\n\n    def run(self, dt: int = 10) -&gt; PasqalCloudExtracted:docs\n        def extractor(job: Job, sequence: pl.Sequence) -&gt; dict[str, int] | None:\n            cutoff_duration = int(ceil(sequence.get_duration() / dt) * dt)\n            full_result = job.full_result\n            if full_result is None:\n                return None\n            result = full_result[\"bitstring\"][cutoff_duration]\n            if result is None:\n                return None\n            assert isinstance(result, dict)\n            return result\n\n        return self._run(\n            emulator=EmulatorType.EMU_MPS,\n            config=EmuTNConfig(\n                dt=dt,\n            ),\n            state_extractor=extractor,\n        )\n</code></pre>"},{"location":"api/qek/data/graphs/","title":"qek.data.graphs","text":"qek.data.graphs<p> source module qek.data.graphs </p> <p>Loading graphs as raw data.</p> <p> Attributes </p> <ul> <li> <p>EPSILON_RADIUS_UM \u2014 Assumption of rounding error when determining whether a graph is a disk graph.</p> </li> <li> <p>EPSILON_RESCALE_FACTOR \u2014 A correction factor, attempting to cover for rounding error when rescaling a graph.</p> </li> </ul> <p> Classes </p> <ul> <li> <p>BaseGraph \u2014 A graph being prepared for embedding on a quantum device.</p> </li> <li> <p>MoleculeGraph \u2014 A graph based on molecular data, being prepared for embedding on a quantum device.</p> </li> <li> <p>PTCFMGraph \u2014 An ingester for molecule graphs using PTC-FM dataset conventions.</p> </li> <li> <p>BaseGraphCompiler \u2014 Abstract class, used to load a graph and compile a Pulser sequence for a device.</p> </li> <li> <p>PygWithPosCompiler \u2014 A compiler able to ingest torch_geometric graphs with positions.</p> </li> <li> <p>MoleculeGraphCompiler \u2014 A compiler able to ingest torch_geometric molecules with a target.</p> </li> <li> <p>PTCFMCompiler</p> </li> <li> <p>NXWithPos \u2014 A networkx graph and its position</p> </li> <li> <p>NXGraphCompiler</p> </li> </ul> <p> source class BaseGraph(id: int, data: pyg_data.Data, device: pl.devices.Device, target: int | None = None) </p> <p>A graph being prepared for embedding on a quantum device.</p> <p>Create a graph from geometric data.</p> <p> Parameters </p> <ul> <li> <p>id :  int \u2014 An identifier for this graph, used mostly for error messages.</p> </li> <li> <p>data :  pyg_data.Data \u2014 A homogeneous graph, in PyTorch Geometric format. Unchanged. It MUST have attributes 'pos'.</p> </li> <li> <p>device :  pl.devices.Device \u2014 The device for which the graph is prepared.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>is_disk_graph \u2014 A predicate to check if <code>self</code> is a disk graph with the specified radius, i.e. <code>self</code> is a connected graph and, for every pair of nodes <code>A</code> and <code>B</code> within <code>graph</code>, there exists an edge between <code>A</code> and <code>B</code> if and only if the positions of <code>A</code> and <code>B</code> within <code>self</code> are such that <code>|AB| &lt;= radius</code>.</p> </li> <li> <p>is_embeddable \u2014     A predicate to check if the graph can be embedded in the     quantum device.</p> </li> <li> <p>compile_register \u2014 Create a Quantum Register based on a graph.</p> </li> <li> <p>compile_pulse \u2014 Extract a Pulse for this graph.</p> </li> </ul> <p> source method BaseGraph.is_disk_graph(radius: float) \u2192 bool </p> <p>A predicate to check if <code>self</code> is a disk graph with the specified radius, i.e. <code>self</code> is a connected graph and, for every pair of nodes <code>A</code> and <code>B</code> within <code>graph</code>, there exists an edge between <code>A</code> and <code>B</code> if and only if the positions of <code>A</code> and <code>B</code> within <code>self</code> are such that <code>|AB| &lt;= radius</code>.</p> <p> Parameters </p> <ul> <li> <p>radius :  float \u2014 The maximal distance between two nodes of <code>self</code> connected be an edge.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>bool \u2014 <code>True</code> if the graph is a disk graph with the specified radius, <code>False</code> otherwise.</p> </li> </ul> <p> source method BaseGraph.is_embeddable() \u2192 bool </p> <pre><code>A predicate to check if the graph can be embedded in the\nquantum device.\n\nFor a graph to be embeddable on a device, all the following\ncriteria must be fulfilled:\n- the graph must be non-empty;\n- the device must have at least as many atoms as the graph has\n    nodes;\n- the device must be physically large enough to place all the\n    nodes (device.max_radial_distance);\n- the nodes must be distant enough that quantum interactions\n    may take place (device.min_atom_distance)\n</code></pre> <p> Returns </p> <ul> <li> <p>bool \u2014 True if possible, False if not</p> </li> </ul> <p> source method BaseGraph.compile_register() \u2192 targets.Register </p> <p>Create a Quantum Register based on a graph.</p> <p> Returns </p> <ul> <li> <p>pulser.Register \u2014 register</p> </li> </ul> <p> Raises </p> <ul> <li> <p>CompilationError</p> </li> </ul> <p> source method BaseGraph.compile_pulse(normalized_amplitude: float | None = None, normalized_duration: float | None = None) \u2192 targets.Pulse </p> <p>Extract a Pulse for this graph.</p> <p>A Pulse represents the laser applied to the atoms on the device.</p> <p> Parameters </p> <ul> <li> <p>normalized_amplitude :  optional \u2014 The normalized amplitude for the laser pulse, as a value in [0, 1], where 0 is no pulse and 1 is the maximal amplitude for the device. By default, use the value demonstrated in the companion paper.</p> </li> <li> <p>normalized_duration :  optional \u2014 The normalized duration of the laser pulse, as a value in [0, 1], where 0 is the shortest possible duration and 1 is the longest possible duration. By default, use the value demonstrated in the companion paper.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source class MoleculeGraph(id: Any, data: pyg_data.Data, device: pl.devices.Device, node_mapping: dict[int, str], edge_mapping: dict[int, Chem.BondType], target: int | None = None) </p> <p><p>Bases : BaseGraph</p></p> <p>A graph based on molecular data, being prepared for embedding on a quantum device.</p> <p>Compute the geometry for a molecule graph.</p> <p> Parameters </p> <ul> <li> <p>data :  pyg_data.Data \u2014 A homogeneous graph, in PyTorch Geometric format. Unchanged.</p> </li> <li> <p>blockade_radius \u2014 The radius of the Rydberg Blockade. Two connected nodes should be at a distance &lt; blockade_radius, while two disconnected nodes should be at a distance &gt; blockade_radius.</p> </li> <li> <p>node_mapping :  dict[int, str] \u2014 A mapping of node labels from numbers to strings, e.g. <code>5 =&gt; \"Cl\"</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>edge_mapping :  dict[int, Chem.BondType] \u2014 A mapping of edge labels from number to chemical bond types, e.g. <code>2 =&gt; DOUBLE</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>target :  int | None \u2014 If specified, a target for machine learning, as a value <code>0</code> or <code>1</code>.</p> </li> </ul> <p> source class PTCFMGraph(id: Any, data: pyg_data.Data, device: pl.devices.Device) </p> <p><p>Bases : MoleculeGraph</p></p> <p>An ingester for molecule graphs using PTC-FM dataset conventions.</p> <p>Compute the geometry for a molecule graph.</p> <p> Parameters </p> <ul> <li> <p>data :  pyg_data.Data \u2014 A homogeneous graph, in PyTorch Geometric format. Unchanged.</p> </li> <li> <p>blockade_radius \u2014 The radius of the Rydberg Blockade. Two connected nodes should be at a distance &lt; blockade_radius, while two disconnected nodes should be at a distance &gt; blockade_radius.</p> </li> <li> <p>node_mapping \u2014 A mapping of node labels from numbers to strings, e.g. <code>5 =&gt; \"Cl\"</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>edge_mapping \u2014 A mapping of edge labels from number to chemical bond types, e.g. <code>2 =&gt; DOUBLE</code>. Used when building molecules, e.g. to compute distances between nodes.</p> </li> <li> <p>target \u2014 If specified, a target for machine learning, as a value <code>0</code> or <code>1</code>.</p> </li> </ul> <p> source class BaseGraphCompiler() </p> <p><p>Bases : abc.ABC, Generic[GraphType]</p></p> <p>Abstract class, used to load a graph and compile a Pulser sequence for a device.</p> <p>You should probably use one of the subclasses.</p> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method BaseGraphCompiler.ingest(graph: GraphType, device: pl.devices.Device, id: int) \u2192 BaseGraph </p> <p> Raises </p> <ul> <li> <p>Exception</p> </li> </ul> <p> source class PygWithPosCompiler() </p> <p><p>Bases : BaseGraphCompiler[pyg_data.Data]</p></p> <p>A compiler able to ingest torch_geometric graphs with positions.</p> <p> Methods </p> <ul> <li> <p>ingest \u2014 Compile a Pulser sequence from a torch_geometric graph with position.</p> </li> </ul> <p> source method PygWithPosCompiler.ingest(graph: pyg_data.Data, device: pl.devices.Device, id: int) \u2192 BaseGraph </p> <p>Compile a Pulser sequence from a torch_geometric graph with position.</p> <p> Parameters </p> <ul> <li> <p>graph :  pyg_data.Data \u2014 A graph with positions (specified as attribute <code>pos</code>) and optionally a prediction target (specified as attribute <code>y</code>, which must be an <code>int</code>). The graph will not be changed.</p> </li> <li> <p>device :  pl.devices.Device \u2014 The device for which the sequence must be compiled.</p> </li> <li> <p>id :  int \u2014 A unique identifier for the graph, used mostly for logging and displaying error messages.</p> </li> </ul> <p> source class MoleculeGraphCompiler(node_mapping: dict[int, str], edge_mapping: dict[int, Chem.BondType]) </p> <p><p>Bases : BaseGraphCompiler[tuple[pyg_data.Data, int | None]]</p></p> <p>A compiler able to ingest torch_geometric molecules with a target.</p> <p>Setup a molecule graph compiler.</p> <p> Parameters </p> <ul> <li> <p>node_mapping :  dict[int, str] \u2014 A mapping from node labels (as integers) to atom names (e.g. \"C\", \"Ar\", ...).</p> </li> <li> <p>edge_mapping :  dict[int, Chem.BondType] \u2014 A mapping from node labels (as integers) to chemical bond types (e.g. simple bound, double bound).</p> </li> </ul> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method MoleculeGraphCompiler.ingest(graph: tuple[pyg_data.Data, int | None], device: pl.devices.Device, id: int) \u2192 MoleculeGraph </p> <p> source class PTCFMCompiler() </p> <p><p>Bases : BaseGraphCompiler[pyg_data.Data]</p></p> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method PTCFMCompiler.ingest(graph: pyg_data.Data, device: pl.devices.Device, id: int) \u2192 PTCFMGraph </p> <p> source dataclass NXWithPos(graph: nx.Graph, positions: dict[Any, np.ndarray], target: int | None) </p> <p>A networkx graph and its position</p> <p> source class NXGraphCompiler() </p> <p><p>Bases : BaseGraphCompiler[NXWithPos]</p></p> <p> Methods </p> <ul> <li> <p>ingest</p> </li> </ul> <p> source method NXGraphCompiler.ingest(graph: NXWithPos, device: pl.devices.Device, id: int) \u2192 BaseGraph </p>"},{"location":"src/qek/data/graphs/","title":"qek.data.graphs","text":"qek.data.graphs<p> docs module qek.data.graphs </p> <pre><code>\"\"\"\nLoading graphs as raw data.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nfrom dataclasses import dataclass\nimport logging\nimport math\nfrom typing import Any, Final, Generic, TypeVar\n\nimport networkx as nx\nimport numpy as np\nimport pulser as pl\nimport rdkit.Chem as Chem\nimport torch\nimport torch_geometric.data as pyg_data\nimport torch_geometric.utils as pyg_utils\nfrom rdkit.Chem import AllChem\n\nfrom qek.target import targets\nfrom qek.shared.error import CompilationError\nfrom qek.shared._utils import graph_to_mol\n\nlogger = logging.getLogger(__name__)\n\nEPSILON_RADIUS_UM = 0.01\n\"\"\"\nAssumption of rounding error when determining whether a graph is a disk graph.\n\"\"\"\n\nEPSILON_RESCALE_FACTOR = 1.000000001\n\"\"\"\nA correction factor, attempting to cover for rounding error when rescaling a graph.\n\"\"\"\n\n\nclass BaseGraph:docs\n    \"\"\"\n    A graph being prepared for embedding on a quantum device.\n    \"\"\"\n\n    device: Final[pl.devices.Device]\n\n    def __init__(\n        self, id: int, data: pyg_data.Data, device: pl.devices.Device, target: int | None = None\n    ):\n        \"\"\"\n        Create a graph from geometric data.\n\n        Args:\n            id: An identifier for this graph, used mostly for error messages.\n            data:  A homogeneous graph, in PyTorch Geometric format. Unchanged.\n                It MUST have attributes 'pos'.\n            device: The device for which the graph is prepared.\n        \"\"\"\n        if not hasattr(data, \"pos\"):\n            raise AttributeError(\"The graph should have an attribute 'pos'.\")\n\n        # The device for which the graph is prepared.\n        self.device = device\n\n        # The graph in torch geometric format.\n        self.pyg = data.clone()\n\n        # The graph in networkx format, undirected.\n        self.nx_graph: nx.Graph = pyg_utils.to_networkx(\n            data=data,\n            node_attrs=[\"x\"],\n            edge_attrs=[\"edge_attr\"] if data.edge_attr is not None else None,\n            to_undirected=True,\n        )\n        self.target = target\n        self.id = id\n\n    def is_disk_graph(self, radius: float) -&gt; bool:docs\n        \"\"\"\n        A predicate to check if `self` is a disk graph with the specified\n        radius, i.e. `self` is a connected graph and, for every pair of nodes\n        `A` and `B` within `graph`, there exists an edge between `A` and `B`\n        if and only if the positions of `A` and `B` within `self` are such\n        that `|AB| &lt;= radius`.\n\n        Args:\n            radius: The maximal distance between two nodes of `self`\n                connected be an edge.\n\n        Returns:\n            `True` if the graph is a disk graph with the specified radius,\n            `False` otherwise.\n        \"\"\"\n\n        if self.pyg.num_nodes == 0 or self.pyg.num_nodes is None:\n            logger.debug(\"graph %s doesn't have any nodes, it's not a disk graph\", self.id, self.id)\n            return False\n\n        # Check if the graph is connected.\n        if len(self.nx_graph) == 0 or not nx.is_connected(self.nx_graph):\n            logger.debug(\"graph %s is not connected, it's not a disk graph\", self.id)\n            return False\n\n        # Check the distances between all pairs of nodes.\n        pos = self.pyg.pos\n        assert pos is not None\n        for u, v in nx.non_edges(self.nx_graph):\n            distance_um = np.linalg.norm(np.array(pos[u]) - np.array(pos[v]))\n            if distance_um &lt;= radius:\n                # These disjointed nodes would interact with each other, so\n                # this is not an embeddable graph.\n                logger.debug(\n                    \"graph %s has non-edges that are too close to each other, it's not a disk graph\",\n                    self.id,\n                )\n                return False\n\n        for u, v in self.nx_graph.edges():\n            distance_um = np.linalg.norm(np.array(pos[u]) - np.array(pos[v]))\n            if distance_um &gt; radius:\n                # These joined nodes would not interact with each other, so\n                # this is not an embeddable graph.\n                logger.debug(\n                    \"graph %s has edges that are too distant from each other (%s &gt; %s), it's not a disk graph\",\n                    self.id,\n                    distance_um,\n                    radius,\n                )\n                return False\n\n        return True\n\n    def is_embeddable(self) -&gt; bool:docs\n        \"\"\"\n            A predicate to check if the graph can be embedded in the\n            quantum device.\n\n            For a graph to be embeddable on a device, all the following\n            criteria must be fulfilled:\n            - the graph must be non-empty;\n            - the device must have at least as many atoms as the graph has\n                nodes;\n            - the device must be physically large enough to place all the\n                nodes (device.max_radial_distance);\n            - the nodes must be distant enough that quantum interactions\n                may take place (device.min_atom_distance)\n\n        Returns:\n            bool: True if possible, False if not\n        \"\"\"\n\n        # Reject empty graphs.\n        if self.pyg.num_nodes == 0 or self.pyg.num_nodes is None:\n            logger.debug(\"graph %s is empty, it's not embeddable\", self.id)\n            return False\n\n        # Reject graphs that have more nodes than can be represented\n        # on the device.\n        if self.pyg.num_nodes &gt; self.device.max_atom_num:\n            logger.debug(\n                \"graph %s has too many nodes (%s), it's not embeddable\", self.id, self.pyg.num_nodes\n            )\n            return False\n\n        # Check the distance from the center\n        pos = self.pyg.pos\n        assert pos is not None\n        distance_from_center = np.linalg.norm(pos, ord=2, axis=-1)\n        if any(distance_from_center &gt; self.device.max_radial_distance):\n            logger.debug(\n                \"graph %s has nodes to far from the center (%s &gt; %s), it's not embeddable\",\n                self.id,\n                max(distance_from_center),\n                self.device.max_radial_distance,\n            )\n            return False\n\n        # Check distance between nodes\n        if not self.is_disk_graph(self.device.min_atom_distance + EPSILON_RADIUS_UM):\n            logger.debug(\"graph %s is not a disk graph, it's not embeddable\", self.id)\n            return False\n\n        for u, v in self.nx_graph.edges():\n            distance_um = np.linalg.norm(np.array(pos[u]) - np.array(pos[v]))\n            if distance_um &lt; self.device.min_atom_distance:\n                # These nodes are too close to each other, preventing quantum\n                # interactions on the device.\n                logger.debug(\n                    \"graph %s has nodes that are too close to each other (%s &lt; %s), it's not embeddable\",\n                    self.id,\n                    distance_um,\n                    self.device.min_atom_distance,\n                )\n                return False\n\n        return True\n\n    # Default values for the sequence.\n    #\n    # See the companion paper for an explanation.\n    SEQUENCE_DEFAULT_AMPLITUDE_RAD_PER_US = 1.0 * 2 * np.pi\n    SEQUENCE_DEFAULT_DURATION_NS = 660\n\n    def compile_register(self) -&gt; targets.Register:docs\n        \"\"\"Create a Quantum Register based on a graph.\n\n        Returns:\n            pulser.Register: register\n        \"\"\"\n        # Note: In the low-level API, we separate register and pulse compilation for\n        # pedagogical reasons, because we want to take the opportunity to teach them\n        # about registers and pulses, rather than pulser sequences.\n\n        if not self.is_embeddable():\n            raise CompilationError(f\"The graph is not compatible with {self.device}\")\n\n        # Compile register\n        pos = self.pyg.pos\n        assert pos is not None\n        reg = pl.Register.from_coordinates(coords=pos)\n        if self.device.requires_layout:\n            reg = reg.with_automatic_layout(device=self.device)\n\n        try:\n            # Due to issue #29, we can produce a register that will not work on this device,\n            # so we need to perform a second check.\n            pl.Sequence(register=reg, device=self.device)\n        except ValueError as e:\n            raise CompilationError(f\"The graph is not compatible with {self.device}: {e}\")\n        return targets.Register(device=self.device, register=reg)\n\n    def compile_pulse(docs\n        self,\n        normalized_amplitude: float | None = None,\n        normalized_duration: float | None = None,\n    ) -&gt; targets.Pulse:\n        \"\"\"Extract a Pulse for this graph.\n\n        A Pulse represents the laser applied to the atoms on the device.\n\n        Arguments:\n            normalized_amplitude (optional): The normalized amplitude for the laser pulse, as a value in [0, 1],\n                where 0 is no pulse and 1 is the maximal amplitude for the device. By default,\n                use the value demonstrated in the companion paper.\n            normalized_duration (optional): The normalized duration of the laser pulse, as a value in [0, 1],\n                where 0 is the shortest possible duration and 1 is the longest possible\n                duration. By default, use the value demonstrated in the companion paper.\n        \"\"\"\n        # Note: In the low-level API, we separate register and pulse compilation for\n        # pedagogical reasons, because we want to take the opportunity to teach them\n        # about registers and pulses, rather than pulser sequences.\n\n        channel = self.device.channels[\"rydberg_global\"]\n        assert channel is not None\n\n        max_amp = channel.max_amp\n        assert max_amp is not None\n\n        min_duration = channel.min_duration\n        max_duration = channel.max_duration\n        assert max_duration is not None\n\n        if normalized_amplitude is None:\n            absolute_amplitude = self.SEQUENCE_DEFAULT_AMPLITUDE_RAD_PER_US\n            if absolute_amplitude &gt; max_amp:\n                # Unlikely, but let's defend in depth.\n                raise ValueError(\n                    f\"This device does not support pulses with amplitude {absolute_amplitude} rad per us, amplitudes should be &lt;= {max_amp}\"\n                )\n        else:\n            if normalized_amplitude &lt; 0 or normalized_amplitude &gt; 1:\n                raise ValueError(\"Invalid amplitude, expected a value in [0, 1] or None\")\n            absolute_amplitude = normalized_amplitude * max_amp\n\n        if normalized_duration is None:\n            absolute_duration = self.SEQUENCE_DEFAULT_DURATION_NS\n            if absolute_duration &lt; min_duration or absolute_duration &gt; max_duration:\n                # Unlikely, but let's defend in depth.\n                raise ValueError(\n                    f\"This device does not support pulses with duration {absolute_duration} ns, pulses should be within [{min_duration}, {max_duration}]\"\n                )\n        else:\n            if normalized_duration &lt; 0 or normalized_duration &gt; 1:\n                raise ValueError(\"Invalid duration, expected a value in [0, 1] or None\")\n            absolute_duration = (\n                math.ceil(normalized_duration * (max_duration - min_duration)) + min_duration\n            )\n\n        # For an explanation on these constants, see the companion paper.\n        pulse = pl.Pulse.ConstantAmplitude(\n            amplitude=absolute_amplitude,\n            detuning=pl.waveforms.RampWaveform(absolute_duration, 0, 0),\n            phase=0.0,\n        )\n        return targets.Pulse(pulse)\n\n\nclass MoleculeGraph(BaseGraph):docs\n    \"\"\"\n    A graph based on molecular data, being prepared for embedding on a\n    quantum device.\n    \"\"\"\n\n    def __init__(\n        self,\n        id: Any,\n        data: pyg_data.Data,\n        device: pl.devices.Device,\n        node_mapping: dict[int, str],\n        edge_mapping: dict[int, Chem.BondType],\n        target: int | None = None,\n    ):\n        \"\"\"\n        Compute the geometry for a molecule graph.\n\n        Args:\n            data:  A homogeneous graph, in PyTorch Geometric format. Unchanged.\n            blockade_radius: The radius of the Rydberg Blockade. Two\n                connected nodes should be at a distance &lt; blockade_radius,\n                while two disconnected nodes should be at a\n                distance &gt; blockade_radius.\n            node_mapping: A mapping of node labels from numbers to strings,\n                e.g. `5 =&gt; \"Cl\"`. Used when building molecules, e.g. to compute\n                distances between nodes.\n            edge_mapping: A mapping of edge labels from number to chemical\n                bond types, e.g. `2 =&gt; DOUBLE`. Used when building molecules,\n                e.g. to compute distances between nodes.\n            target: If specified, a target for machine learning, as a value\n                `0` or `1`.\n        \"\"\"\n        pyg = data.clone()\n        pyg.pos = None  # Placeholder\n        super().__init__(id=id, data=pyg, device=device, target=target)\n\n        # Reconstruct the molecule.\n        tmp_mol = graph_to_mol(\n            graph=self.nx_graph,\n            node_mapping=node_mapping,\n            edge_mapping=edge_mapping,\n        )\n\n        # Extract the geometry.\n        AllChem.Compute2DCoords(tmp_mol, useRingTemplates=True)\n        original_pos = tmp_mol.GetConformer().GetPositions()[..., :2]  # Convert to 2D\n\n        # We now want to scale the geometry so that the smallest edge\n        # is as long as `device.min_atom_distance`.\n        pos = original_pos\n        pairs: list[tuple[Any, Any]] = []\n        for start, end in self.nx_graph.edges():\n            pairs.append((start, end))\n        for start, end in nx.non_edges(self.nx_graph):\n            pairs.append((start, end))\n\n        distances = []\n        for start, end in pairs:\n            distances.append(np.linalg.norm(pos[start] - pos[end]))\n        min_distance = np.min(distances)\n        pos = pos * device.min_atom_distance / min_distance\n\n        # The above transformation is sensitive to rouding errors, so if we realize that\n        # we accidentally made the smallest edge too small, we'll multiply by a small factor.\n        while True:\n            distances = []\n            for start, end in pairs:\n                distances.append(np.linalg.norm(pos[start] - pos[end]))\n            min_distance = np.min(distances)\n            if min_distance &gt;= device.min_atom_distance:\n                logger.debug(\n                    \"The minimal distance in graph #%s exceeds min atom distance: %s &gt; %s, it should now be compilable\",\n                    self.id,\n                    min_distance,\n                    device.min_atom_distance,\n                )\n                break\n            pos = pos * EPSILON_RESCALE_FACTOR\n\n        # Finally, store the position.\n        self.pyg.pos = pos\n\n\nclass PTCFMGraph(MoleculeGraph):docs\n    \"\"\"\n    An ingester for molecule graphs using\n    PTC-FM dataset conventions.\n    \"\"\"\n\n    # Constants used to decode the PTC-FM dataset, mapping\n    # integers (used as node attributes) to atom names.\n    PTCFM_ATOM_NAMES: Final[dict[int, str]] = {\n        0: \"In\",\n        1: \"P\",\n        2: \"C\",\n        3: \"O\",\n        4: \"N\",\n        5: \"Cl\",\n        6: \"S\",\n        7: \"Br\",\n        8: \"Na\",\n        9: \"F\",\n        10: \"As\",\n        11: \"K\",\n        12: \"Cu\",\n        13: \"I\",\n        14: \"Ba\",\n        15: \"Sn\",\n        16: \"Pb\",\n        17: \"Ca\",\n    }\n\n    # Constants used to decode the PTC-FM dataset, mapping\n    # integers (used as edge attributes) to bond types.\n    PTCFM_BOND_TYPES: Final[dict[int, Chem.BondType]] = {\n        0: Chem.BondType.TRIPLE,\n        1: Chem.BondType.SINGLE,\n        2: Chem.BondType.DOUBLE,\n        3: Chem.BondType.AROMATIC,\n    }\n\n    def __init__(\n        self,\n        id: Any,\n        data: pyg_data.Data,\n        device: pl.devices.Device,\n    ):\n        \"\"\"\n        Compute the geometry for a molecule graph.\n\n        Args:\n            data:  A homogeneous graph, in PyTorch Geometric format. Unchanged.\n            blockade_radius: The radius of the Rydberg Blockade. Two\n                connected nodes should be at a distance &lt; blockade_radius,\n                while two disconnected nodes should be at a\n                distance &gt; blockade_radius.\n            node_mapping: A mapping of node labels from numbers to strings,\n                e.g. `5 =&gt; \"Cl\"`. Used when building molecules, e.g. to compute\n                distances between nodes.\n            edge_mapping: A mapping of edge labels from number to chemical\n                bond types, e.g. `2 =&gt; DOUBLE`. Used when building molecules,\n                e.g. to compute distances between nodes.\n            target: If specified, a target for machine learning, as a value\n                `0` or `1`.\n        \"\"\"\n        target = data.y\n        if target is None:\n            raise AttributeError(\"The graph should have an attribute 'y'.\")\n\n        if isinstance(target, torch.Tensor):\n            target = target.item()\n        target = int(target)\n\n        super().__init__(\n            id=id,\n            data=data,\n            device=device,\n            node_mapping=PTCFMGraph.PTCFM_ATOM_NAMES,\n            edge_mapping=PTCFMGraph.PTCFM_BOND_TYPES,\n            target=target,\n        )\n\n\nGraphType = TypeVar(\"GraphType\")\n\n\nclass BaseGraphCompiler(abc.ABC, Generic[GraphType]):docs\n    \"\"\"\n    Abstract class, used to load a graph and compile a Pulser sequence for a device.\n\n    You should probably use one of the subclasses.\n    \"\"\"\n\n    @abc.abstractmethoddocs\n    def ingest(self, graph: GraphType, device: pl.devices.Device, id: int) -&gt; BaseGraph:\n        raise Exception(\"Please use one of the subclasses\")\n\n\nclass PygWithPosCompiler(BaseGraphCompiler[pyg_data.Data]):docs\n    \"\"\"\n    A compiler able to ingest torch_geometric graphs with positions.\n    \"\"\"\ndocs\n    def ingest(self, graph: pyg_data.Data, device: pl.devices.Device, id: int) -&gt; BaseGraph:\n        \"\"\"\n        Compile a Pulser sequence from a torch_geometric graph with position.\n\n        Args:\n            graph: A graph with positions (specified as attribute `pos`) and\n                optionally a prediction target (specified as attribute `y`, which\n                must be an `int`). The graph will not be changed.\n            device: The device for which the sequence must be compiled.\n            id: A unique identifier for the graph, used mostly for logging\n                and displaying error messages.\n        \"\"\"\n        return BaseGraph(id=id, data=graph, device=device)\n\ndocs\nclass MoleculeGraphCompiler(BaseGraphCompiler[tuple[pyg_data.Data, int | None]]):\n    \"\"\"\n    A compiler able to ingest torch_geometric molecules with a target.\n    \"\"\"\n\n    def __init__(\n        self,\n        node_mapping: dict[int, str],\n        edge_mapping: dict[int, Chem.BondType],\n    ):\n        \"\"\"\n        Setup a molecule graph compiler.\n\n        Args:\n            node_mapping: A mapping from node labels (as integers) to atom names (e.g. \"C\", \"Ar\", ...).\n            edge_mapping: A mapping from node labels (as integers) to chemical bond types (e.g. simple\n                bound, double bound).\n        \"\"\"\n        self.node_mapping = node_mapping\n        self.edge_mapping = edge_mapping\n\n    \"\"\"\n    Compile a Pulser sequence from a molecule, expressed as a torch_geometric\n    graph.\n\n    Args:\n        graph: A molecular graph.\n            This graph is expected to have:\n             - `int` labels on nodes, which may be converted into atom names\n                with `self.node_mapping`\n             - `int` labels on edges, which may be converted into chemical\n                bounds with `self.edge_mapping`\n            The graph will not be changed.\n        device: The device for which the sequence must be compiled.\n        id: A unique identifier for the graph, used mostly for logging\n            and displaying error messages.\n    \"\"\"\n\n    def ingest(docs\n        self, graph: tuple[pyg_data.Data, int | None], device: pl.devices.Device, id: int\n    ) -&gt; MoleculeGraph:\n        return MoleculeGraph(\n            id=id,\n            data=graph[0],\n            device=device,\n            node_mapping=self.node_mapping,\n            edge_mapping=self.edge_mapping,\n            target=graph[1],\n        )\n\n\nclass PTCFMCompiler(BaseGraphCompiler[pyg_data.Data]):docs\n    def ingest(self, graph: pyg_data.Data, device: pl.devices.Device, id: int) -&gt; PTCFMGraph:\n        return PTCFMGraph(\n            id=id,\n            data=graph,\n            device=device,\n        )\n\n\n@dataclass\nclass NXWithPos:docs\n    \"\"\"\n    A networkx graph and its position\n    \"\"\"\n\n    graph: nx.Graph\n\n    # Mapping from node to positions.\n    positions: dict[Any, np.ndarray]\n\n    # A machine learning target\n    target: int | None\n\n\nclass NXGraphCompiler(BaseGraphCompiler[NXWithPos]):docs\n    def ingest(self, graph: NXWithPos, device: pl.devices.Device, id: int) -&gt; BaseGraph:\n        pyg = pyg_utils.from_networkx(graph.graph)\n        pyg.y = graph.target\n        positions = np.array([graph.positions[node] for node in graph.graph.nodes()])\n        pyg.pos = torch.tensor(positions, dtype=torch.float)\n\n        return BaseGraph(id=id, device=device, data=pyg)\n</code></pre>"},{"location":"api/qek/data/processed_data/","title":"qek.data.processed_data","text":"qek.data.processed_data<p> source module qek.data.processed_data </p> <p>Loading, saving, manipulation or processed data.</p> <p> Classes </p> <ul> <li> <p>ProcessedData \u2014 Data on a single graph obtained from the Quantum Device.</p> </li> </ul> <p> Functions </p> <ul> <li> <p>dist_excitation \u2014 Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> </li> <li> <p>save_dataset \u2014 Saves a dataset to a JSON file.</p> </li> <li> <p>load_dataset \u2014 Loads a dataset from a JSON file.</p> </li> </ul> <p> source class ProcessedData(sequence: pl.Sequence, state_dict: dict[str, int | np.int64], target: int | None) </p> <p>Data on a single graph obtained from the Quantum Device.</p> <p> Attributes </p> <ul> <li> <p>register :  pl.Register \u2014 The geometry of atoms on the Quantum Device, obtained by compiling the graph for execution on the Quantum Device.</p> </li> <li> <p>pulse :  pl.Pulse \u2014 The laser pulse, obtained by compiling the graph for execution on the Quantum Device.</p> </li> <li> <p>state_dict :  Final[dict[str, int]] \u2014 A dictionary {bitstring: number of instances} for this graph.</p> </li> <li> <p>target :  Final[int | None] \u2014 If specified, the machine-learning target, as a value <code>0</code> or <code>1</code>.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>custom</p> </li> <li> <p>save_to_file</p> </li> <li> <p>dist_excitation \u2014 Return the distribution of excitations for this graph.</p> </li> <li> <p>draw_pulse \u2014 Draw the pulse on screen</p> </li> <li> <p>draw_register \u2014 Draw the register on screen</p> </li> <li> <p>draw_excitation \u2014 Draw an histogram for the excitation level on screen</p> </li> </ul> <p> source classmethod ProcessedData.custom(register: targets.Register, pulse: targets.Pulse, device: pl.devices.Device, state_dict: dict[str, int | np.int64], target: int | None) \u2192 ProcessedData </p> <p> source method ProcessedData.save_to_file(file_path: str) \u2192 None </p> <p> source method ProcessedData.dist_excitation(size: int | None = None) \u2192 np.ndarray </p> <p>Return the distribution of excitations for this graph.</p> <p> Parameters </p> <ul> <li> <p>size :  int | None \u2014 If specified, truncate or pad the array to this size.</p> </li> </ul> <p> source property ProcessedData.pulse: pl.Pulse </p> <p>The laser pulse used to process this data.</p> <p> source property ProcessedData.register: pl.Register </p> <p>The register to which the graph was compiled.</p> <p> source method ProcessedData.draw_pulse() \u2192 None </p> <p>Draw the pulse on screen</p> <p> source method ProcessedData.draw_register() \u2192 None </p> <p>Draw the register on screen</p> <p> source method ProcessedData.draw_excitation() \u2192 None </p> <p>Draw an histogram for the excitation level on screen</p> <p> source dist_excitation(state_dict: dict[str, int], size: int | None = None) \u2192 np.ndarray </p> <p>Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> <p> Parameters </p> <ul> <li> <p>size :  int | None \u2014 If specified, only keep <code>size</code> energy distributions in the output. Otherwise, keep all values.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>np.ndarray \u2014 A histogram of excitation energies. - index: an excitation level (i.e. a number of <code>1</code> bits in a     bitstring) - value: normalized count of samples with this excitation level.</p> </li> </ul> <p> source save_dataset(dataset: list[ProcessedData], file_path: str) \u2192 None </p> <p>Saves a dataset to a JSON file.</p> <p> Parameters </p> <ul> <li> <p>dataset :  list[ProcessedData] \u2014 The dataset to be saved, containing RegisterData instances.</p> </li> <li> <p>file_path :  str \u2014 The path where the dataset will be saved as a JSON file.</p> </li> </ul> <p>Note</p> <p>The data is stored in a format suitable for loading with load_dataset.</p> <p> Returns </p> <ul> <li> <p>None \u2014 None</p> </li> </ul> <p> source load_dataset(file_path: str) \u2192 list[ProcessedData] </p> <p>Loads a dataset from a JSON file.</p> <p> Parameters </p> <ul> <li> <p>file_path :  str \u2014 The path to the JSON file containing the dataset.</p> </li> </ul> <p>Note</p> <p>The data is loaded in the format that was used when saving with     save_dataset.</p> <p> Returns </p> <ul> <li> <p>list[ProcessedData] \u2014 A list of ProcessedData instances, corresponding to the data stored in     the JSON file.</p> </li> </ul>"},{"location":"src/qek/data/processed_data/","title":"qek.data.processed_data","text":"qek.data.processed_data<p> docs module qek.data.processed_data </p> <pre><code>\"\"\"\nLoading, saving, manipulation or processed data.\n\"\"\"\n\nimport collections\nimport json\nfrom typing import Final\nimport matplotlib\n\nimport logging\nimport numpy as np\nimport pulser as pl\n\nfrom qek.data.graphs import EPSILON_RADIUS_UM\nfrom qek.shared._utils import make_sequence\nfrom qek.target import targets\n\nlogger = logging.getLogger(__name__)\n\n\nclass ProcessedData:docs\n    \"\"\"\n    Data on a single graph obtained from the Quantum Device.\n\n    Attributes:\n        register: The geometry of atoms on the Quantum Device, obtained\n            by compiling the graph for execution on the Quantum Device.\n        pulse: The laser pulse, obtained by compiling the graph for\n            execution on the Quantum Device.\n        state_dict: A dictionary {bitstring: number of instances}\n            for this graph.\n        target: If specified, the machine-learning target, as a\n            value `0` or `1`.\n\n    The state dictionary represents an approximation of the quantum\n    state of the device for this graph after completion of the\n    algorithm.\n\n    - keys are bitstrings, i.e. strings of N time 0 or 1, where N\n      is the number of qubits, i.e. the number of nodes in the graph.\n      Each of these {0, 1} corresponds to a possible state for the\n      corresponding qubit.\n    - values are the number of samples observed with this specific\n      state of the register.\n\n    The sum of all values for the dictionary is equal to the total\n    number of samples observed on the quantum device (for this\n    specific graph).\n    \"\"\"\n\n    state_dict: Final[dict[str, int]]\n    _dist_excitation: Final[np.ndarray]\n    target: Final[int | None]\n\n    def __init__(\n        self, sequence: pl.Sequence, state_dict: dict[str, int | np.int64], target: int | None\n    ):\n        self._sequence = sequence\n        # Some emulators will actually be `dict[str, int64]` instead of `dict[str, int]` and `int64`\n        # is not JSON-serializable.\n        #\n        # The reason for which `int64` is not JSON-serializable is that JSON limits ints to 2^53-1.\n        # In practice, this should not be a problem, since the `int`/`int64` in our dict is\n        # limited to the number of runs, and we don't expect to be launching 2^53 consecutive runs\n        # for a single sequence on a device in any foreseeable future (assuming a run of 1ns,\n        # this would still take ~4 billion years to execute).\n        self.state_dict = {k: int(value) for k, value in state_dict.items()}\n        self._dist_excitation = dist_excitation(self.state_dict)\n        self.target = target\n\n    @classmethod\n    def custom(docs\n        cls,\n        register: targets.Register,\n        pulse: targets.Pulse,\n        device: pl.devices.Device,\n        state_dict: dict[str, int | np.int64],\n        target: int | None,\n    ) -&gt; \"ProcessedData\":\n        sequence = make_sequence(register=register, pulse=pulse, device=device)\n        return ProcessedData(sequence=sequence, state_dict=state_dict, target=target)\n\n    def save_to_file(self, file_path: str) -&gt; None:docs\n        with open(file_path, \"w\") as file:\n            tmp_dict = {\n                \"state_dict\": self.state_dict,\n                \"target\": self.target,\n            }\n            json.dump(tmp_dict, file)\n\n    def dist_excitation(self, size: int | None = None) -&gt; np.ndarray:docs\n        \"\"\"\n        Return the distribution of excitations for this graph.\n\n        Arguments:\n            size: If specified, truncate or pad the array to this\n                size.\n        \"\"\"\n        if size is None or size == len(self._dist_excitation):\n            return self._dist_excitation.copy()\n        if size &lt; len(self._dist_excitation):\n            return np.resize(self._dist_excitation, size)\n        return np.pad(self._dist_excitation, (0, size - len(self._dist_excitation)))\n\n    @property\n    def pulse(self) -&gt; pl.Pulse:docs\n        \"\"\"\n        The laser pulse used to process this data.\n        \"\"\"\n        # For the time being, we cannot obtain this information from a public API, see https://github.com/pasqal-io/Pulser/issues/801 .\n        pulse = self._sequence._schedule[\"ising\"].slots[-1].type\n        assert isinstance(pulse, pl.Pulse)\n        return pulse\n\n    @property\n    def register(self) -&gt; pl.Register:docs\n        \"\"\"\n        The register to which the graph was compiled.\n        \"\"\"\n        register = self._sequence.register\n        assert isinstance(register, pl.Register)\n        return register\n\n    def draw_pulse(self) -&gt; None:docs\n        \"\"\"\n        Draw the pulse on screen\n        \"\"\"\n        self.pulse.draw()\n\n    def draw_register(self) -&gt; None:docs\n        \"\"\"\n        Draw the register on screen\n        \"\"\"\n        self.register.draw(\n            # We increase slightly the blockade radius to take into account rounding errors.\n            blockade_radius=self._sequence.device.min_atom_distance\n            + EPSILON_RADIUS_UM\n        )\n\n    def draw_excitation(self) -&gt; None:docs\n        \"\"\"\n        Draw an histogram for the excitation level on screen\n        \"\"\"\n        x = [str(i) for i in range(len(self._dist_excitation))]\n        matplotlib.pyplot.bar(x, self._dist_excitation)\n\ndocs\ndef dist_excitation(state_dict: dict[str, int], size: int | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the distribution of excitation energies from a dictionary of\n    bitstrings to their respective counts.\n\n    Args:\n        size (int | None): If specified, only keep `size` energy\n            distributions in the output. Otherwise, keep all values.\n\n    Returns:\n        A histogram of excitation energies.\n        - index: an excitation level (i.e. a number of `1` bits in a\n            bitstring)\n        - value: normalized count of samples with this excitation level.\n    \"\"\"\n\n    if len(state_dict) == 0:\n        return np.ndarray(0)\n\n    if size is None:\n        # If size is not specified, it's the length of bitstrings.\n        # We assume that all bitstrings in `count_bitstring` have the\n        # same length and we have just checked that it's not empty.\n\n        # Pick the length of the first bitstring.\n        # We have already checked that `count_bitstring` is not empty.\n        bitstring = next(iter(state_dict.keys()))\n        size = len(bitstring)\n\n    # Make mypy realize that `size` is now always an `int`.\n    assert type(size) is int\n\n    count_occupation: dict[int, int] = collections.defaultdict(int)\n    total = 0.0\n    for bitstring, number in state_dict.items():\n        occupation = sum(1 for bit in bitstring if bit == \"1\")\n        count_occupation[occupation] += number\n        total += number\n\n    result = np.zeros(size + 1, dtype=float)\n    for occupation, count in count_occupation.items():\n        if occupation &lt; size:\n            result[occupation] = count / total\n\n    return result\n\n\ndef save_dataset(dataset: list[ProcessedData], file_path: str) -&gt; None:docs\n    \"\"\"Saves a dataset to a JSON file.\n\n    Args:\n        dataset (list[ProcessedData]): The dataset to be saved, containing\n            RegisterData instances.\n        file_path (str): The path where the dataset will be saved as a JSON\n            file.\n\n    Note:\n        The data is stored in a format suitable for loading with load_dataset.\n\n    Returns:\n        None\n    \"\"\"\n    with open(file_path, \"w\") as file:\n        data = [\n            {\n                \"sequence\": instance._sequence.to_abstract_repr(),\n                \"state_dict\": instance.state_dict,\n                \"target\": instance.target,\n            }\n            for instance in dataset\n        ]\n        json.dump(data, file)\n\n\ndef load_dataset(file_path: str) -&gt; list[ProcessedData]:docs\n    \"\"\"Loads a dataset from a JSON file.\n\n    Args:\n        file_path (str): The path to the JSON file containing the dataset.\n\n    Note:\n        The data is loaded in the format that was used when saving with\n            save_dataset.\n\n    Returns:\n        A list of ProcessedData instances, corresponding to the data stored in\n            the JSON file.\n    \"\"\"\n    with open(file_path) as file:\n        data = json.load(file)\n        return [\n            ProcessedData(\n                sequence=pl.Sequence.from_abstract_repr(item[\"sequence\"]),\n                state_dict=item[\"state_dict\"],\n                target=item[\"target\"],\n            )\n            for item in data\n        ]\n</code></pre>"},{"location":"api/qek/data/training_data/","title":"qek.data.training_data","text":"qek.data.training_data<p> source module qek.data.training_data </p> <p>Manipulating training data</p> <p> Functions </p> <ul> <li> <p>split_train_test \u2014     This function splits a torch dataset into train and val dataset.     As torch Dataset class is a mother class of pytorch_geometric dataset     class, it should work just fine for the latter.</p> </li> </ul> <p> source split_train_test(dataset: torch_data.Dataset, lengths: list[float], seed: int | None = None) \u2192 tuple[torch_data.Dataset, torch_data.Dataset] </p> <pre><code>This function splits a torch dataset into train and val dataset.\nAs torch Dataset class is a mother class of pytorch_geometric dataset\nclass, it should work just fine for the latter.\n</code></pre> <p> Parameters </p> <ul> <li> <p>dataset :  torch_data.Dataset \u2014 The original dataset to be splitted</p> </li> <li> <p>lengths :  list[float] \u2014 Percentage of the split. For instance [0.8, 0.2]</p> </li> <li> <p>seed :  int | None, optional \u2014 Seed for reproductibility. Defaults to</p> </li> <li> <p>None.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>tuple[torch_data.Dataset, torch_data.Dataset] \u2014 train and val dataset</p> </li> </ul>"},{"location":"src/qek/data/training_data/","title":"qek.data.training_data","text":"qek.data.training_data<p> docs module qek.data.training_data </p> <pre><code>\"\"\"\nManipulating training data\n\"\"\"\n\nimport torch\nimport torch.utils.data as torch_data\n\n\ndef split_train_test(docs\n    dataset: torch_data.Dataset,\n    lengths: list[float],\n    seed: int | None = None,\n) -&gt; tuple[torch_data.Dataset, torch_data.Dataset]:\n    \"\"\"\n        This function splits a torch dataset into train and val dataset.\n        As torch Dataset class is a mother class of pytorch_geometric dataset\n        class, it should work just fine for the latter.\n\n    Args:\n        dataset (torch_data.Dataset): The original dataset to be splitted\n        lengths (list[float]): Percentage of the split. For instance [0.8, 0.2]\n        seed (int | None, optional): Seed for reproductibility. Defaults to\n        None.\n\n    Returns:\n        tuple[torch_data.Dataset, torch_data.Dataset]: train and val dataset\n    \"\"\"\n    if seed is not None:\n        generator = torch.Generator().manual_seed(seed)\n    else:\n        generator = torch.Generator()\n    train, val = torch_data.random_split(dataset=dataset, lengths=lengths, generator=generator)\n    return train, val\n</code></pre>"},{"location":"api/qek/kernel/","title":"qek.kernel","text":"qek.kernel<p> source package qek.kernel </p> <p>The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> <p> Classes </p> <ul> <li> <p>FastQEK \u2014 FastQEK class.</p> </li> <li> <p>IntegratedQEK \u2014 A variant of the Quantum Evolution Kernel that supports fit/transform/fit_transform from raw data (graphs).</p> </li> </ul> <p> source class FastQEK() </p> <p><p>Bases : BaseKernel[ProcessedData]</p></p> <p>FastQEK class.</p> <p> Attributes </p> <ul> <li> <p>- X (Sequence[ProcessedData]) \u2014 Training data used for fitting the kernel.</p> </li> <li> <p>- kernel_matrix (np.ndarray) \u2014 Kernel matrix. This is assigned in the <code>fit()</code> method</p> </li> </ul> <p> Training parameters </p> <p>mu (float): Scaling factor for the Jensen-Shannon divergence size_max (int, optional): If specified, only consider the first <code>size_max</code>     qubits of bitstrings. Otherwise, consider all qubits. You may use this     to trade precision in favor of speed.</p> <p>Note: This class does not accept raw data, but rather <code>ProcessedData</code>. See class IntegratedQEK for a subclass that provides a more powerful API, at the expense of performance.</p> <p> Methods </p> <ul> <li> <p>to_processed_data \u2014 Convert the raw data into features.</p> </li> </ul> <p> source method FastQEK.to_processed_data(X: Sequence[ProcessedData]) \u2192 Sequence[ProcessedData] </p> <p>Convert the raw data into features.</p> <p> source class IntegratedQEK(mu: float, extractor: BaseExtractor[GraphType], size_max: int | None = None, similarity: Callable[[NDArray[np.floating], NDArray[np.floating]], np.floating] | None = None) </p> <p><p>Bases : BaseKernel[GraphType]</p></p> <p>A variant of the Quantum Evolution Kernel that supports fit/transform/fit_transform from raw data (graphs).</p> <p>Initialize an IntegratedQEK</p> <p> Performance note </p> <p>This class uses an extractor to convert the raw data into features. This can be very slow if you use, for instance, a remote QPU, as the waitlines to access a QPU can be very long. If you are using this in an interactive application or a server, this will block the entire thread during the wait.</p> <p>We recommend using this class only with local emulators.</p> <p> Training parameters </p> <p>mu (float): Scaling factor for the Jensen-Shannon divergence extractor: An extractor (e.g. a QPU or a Quantum emulator) used to conver the raw data (graphs) into features. size_max (int, optional): If specified, only consider the first <code>size_max</code>     qubits of bitstrings. Otherwise, consider all qubits. You may use this     to trade precision in favor of speed. similarity (optional): If specified, a custom similarity metric to use. Otherwise,     use the Jensen-Shannon divergence.</p> <p> Parameters </p> <ul> <li> <p>mu :  float \u2014 Scaling factor for the Jensen-Shannon divergence</p> </li> <li> <p>extractor :  BaseExtractor[GraphType] \u2014 An extractor (e.g. a QPU or a Quantum emulator) used to conver the raw data (graphs) into features.</p> </li> <li> <p>size_max :  int, optional \u2014 If specified, only consider the first <code>size_max</code> qubits of bitstrings. Otherwise, consider all qubits. You may use this to trade precision in favor of speed.</p> </li> <li> <p>similarity :  optional \u2014 If specified, a custom similarity metric to use. Otherwise, use the Jensen-Shannon divergence.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>to_processed_data \u2014 Convert the raw data into features.</p> </li> </ul> <p> source method IntegratedQEK.to_processed_data(X: Sequence[GraphType]) \u2192 Sequence[ProcessedData] </p> <p>Convert the raw data into features.</p> <p> Performance note </p> <p>This method can can be very slow if you use, for instance, a remote QPU, as the waitlines to access a QPU can be very long. If you are using this in an interactive application or a server, this will block the entire thread during the wait.</p>"},{"location":"src/qek/kernel/","title":"qek.kernel","text":"qek.kernel<p> docs package qek.kernel </p> <pre><code>\"\"\"\nThe Quantum Evolution Kernel itself, for use in a machine-learning pipeline.\n\"\"\"\n\nfrom .kernel import FastQEK, IntegratedQEK\n\n# Alias, for backwards compatibility.\nQuantumEvolutionKernel = FastQEK\n\n__all__ = [\"QuantumEvolutionKernel\", \"FastQEK\", \"IntegratedQEK\"]\n</code></pre>"},{"location":"api/qek/kernel/kernel/","title":"qek.kernel.kernel","text":"qek.kernel.kernel<p> source module qek.kernel.kernel </p> <p>The Quantum Evolution Kernel itself, for use in a machine-learning pipeline.</p> <p> Classes </p> <ul> <li> <p>BaseKernel \u2014 Base class for implementations of the Quantum Evolution Kernel.</p> </li> <li> <p>FastQEK \u2014 FastQEK class.</p> </li> <li> <p>IntegratedQEK \u2014 A variant of the Quantum Evolution Kernel that supports fit/transform/fit_transform from raw data (graphs).</p> </li> </ul> <p> Functions </p> <ul> <li> <p>count_occupation_from_bitstring \u2014 Counts the number of '1' bits in a binary string.</p> </li> <li> <p>dist_excitation_and_vec \u2014 Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> </li> </ul> <p> source class BaseKernel(mu: float, size_max: int | None = None, similarity: Callable[[NDArray[np.floating], NDArray[np.floating]], np.floating] | None = None) </p> <p><p>Bases : abc.ABC, Generic[KernelData]</p></p> <p>Base class for implementations of the Quantum Evolution Kernel.</p> <p>Unless you are implementing a new kernel, you should probably consider using one of the subclasses: - FastQEK (lower-level API, requires processed data, optimized); - IntegratedQEK (higher-level API, accepts graphs, slower).</p> <p>Initialize the kernel.</p> <p> Attributes </p> <ul> <li> <p>- X (Sequence[ProcessedData]) \u2014 Training data used for fitting the kernel.</p> </li> <li> <p>- kernel_matrix (np.ndarray) \u2014 Kernel matrix. This is assigned in the <code>fit()</code> method</p> </li> </ul> <p> Training parameters </p> <p>mu (float): Scaling factor for the Jensen-Shannon divergence size_max (int, optional): If specified, only consider the first <code>size_max</code>     qubits of bitstrings. Otherwise, consider all qubits. You may use this     to trade precision in favor of speed.</p> <p> Parameters </p> <ul> <li> <p>mu :  float \u2014 Scaling factor for the Jensen-Shannon divergence</p> </li> <li> <p>size_max :  int, optional \u2014 If specified, only consider the first <code>size_max</code> qubits of bitstrings. Otherwise, consider all qubits. You may use this to trade precision in favor of speed.</p> </li> <li> <p>similarity :  optional \u2014 If specified, a custom similarity metric to use. Otherwise, use the Jensen-Shannon divergence.</p> </li> </ul> <p>Note: This class does not accept raw data, but rather <code>ProcessedData</code>. See class IntegratedQuantumEvolutionKernel for a subclass that provides a more powerful API, at the expense of performance.</p> <p> Methods </p> <ul> <li> <p>to_processed_data \u2014 Convert the raw data into features.</p> </li> <li> <p>default_similarity \u2014 The Jensen-Shannon similarity metric used to compute the kernel, used when calling <code>kernel(X1, X2)</code>.</p> </li> <li> <p>similarity \u2014 Compute the similarity between two graphs using Jensen-Shannon divergence.</p> </li> <li> <p>fit \u2014 Fit the kernel to the training dataset by storing the dataset.</p> </li> <li> <p>transform \u2014 Transform the dataset into the kernel space with respect to the training dataset.</p> </li> <li> <p>fit_transform \u2014 Fit the kernel to the training dataset and transform it.</p> </li> <li> <p>create_train_kernel_matrix \u2014 Compute a kernel matrix for a given training dataset.</p> </li> <li> <p>create_test_kernel_matrix \u2014 Compute a kernel matrix for a given testing dataset and training set.</p> </li> <li> <p>set_params \u2014 Set multiple parameters for the kernel.</p> </li> <li> <p>get_params \u2014 Retrieve the value of all parameters.</p> </li> </ul> <p> source method BaseKernel.to_processed_data(X: Sequence[KernelData]) \u2192 Sequence[ProcessedData] </p> <p>Convert the raw data into features.</p> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source method BaseKernel.default_similarity(row: NDArray[np.floating], col: NDArray[np.floating]) \u2192 np.floating </p> <p>The Jensen-Shannon similarity metric used to compute the kernel, used when calling <code>kernel(X1, X2)</code>.</p> <p>This is the default similarity, if no parameter <code>similarity</code> is provided.</p> <p> source method BaseKernel.similarity(graph_1: KernelData, graph_2: KernelData) \u2192 float </p> <p>Compute the similarity between two graphs using Jensen-Shannon divergence.</p> <p>This method computes the square of the Jensen-Shannon divergence (JSD) between two probability distributions over bitstrings. The JSD is a measure of the difference between two probability distributions, and it can be used as a kernel for machine learning algorithms that require a similarity function.</p> <p>The input graphs are assumed to have been processed using the ProcessedData class from qek_os.data_io.dataset.</p> <p> Parameters </p> <ul> <li> <p>graph_1 :  KernelData \u2014 First graph.</p> </li> <li> <p>graph_2 :  KernelData \u2014 Second graph.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>float \u2014 Similarity between the two graphs, scaled by a factor that depends on mu.</p> </li> </ul> <p>Notes</p> <p>The JSD is computed using the jensenshannon function from <code>scipy.spatial.distance</code>, and it is squared because scipy function <code>jensenshannon</code> outputs the distance instead of the divergence.</p> <p> source method BaseKernel.fit(X: Sequence[KernelData], y: list | None = None) \u2192 None </p> <p>Fit the kernel to the training dataset by storing the dataset.</p> <p> Parameters </p> <ul> <li> <p>X :  Sequence[KernelData] \u2014 The training dataset.</p> </li> <li> <p>y :  list | None \u2014 list: Target variable for the dataset sequence. This argument is ignored, provided only for compatibility with machine-learning libraries.</p> </li> </ul> <p> source method BaseKernel.transform(X_test: Sequence[KernelData], y_test: list | None = None) \u2192 np.ndarray </p> <p>Transform the dataset into the kernel space with respect to the training dataset.</p> <p> Parameters </p> <ul> <li> <p>X_test :  Sequence[KernelData] \u2014 The dataset to transform. y_test: list: Target variable for the dataset sequence.     This argument is ignored, provided only for compatibility     with machine-learning libraries.</p> </li> <li> <p>Returns \u2014 np.ndarray: Kernel matrix where each entry represents the similarity between             the given dataset and the training dataset.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul> <p> source method BaseKernel.fit_transform(X: Sequence[KernelData], y: list | None = None) \u2192 np.ndarray </p> <p>Fit the kernel to the training dataset and transform it.</p> <p> Parameters </p> <ul> <li> <p>X :  Sequence[KernelData] \u2014 The dataset to fit and transform. y: list: Target variable for the dataset sequence.     This argument is ignored, provided only for compatibility     with machine-learning libraries.</p> </li> <li> <p>Returns \u2014 np.ndarray: Kernel matrix for the training dataset.</p> </li> </ul> <p> source method BaseKernel.create_train_kernel_matrix(train_dataset: Sequence[KernelData]) \u2192 np.ndarray </p> <p>Compute a kernel matrix for a given training dataset.</p> <p>This method computes a symmetric N x N kernel matrix from the Jensen-Shannon divergences between all pairs of graphs in the input dataset. The resulting matrix can be used as a similarity metric for machine learning algorithms. Args:     train_dataset: A list of objects to compute the kernel matrix from. Returns:     np.ndarray: An N x N symmetric matrix where the entry at row i and     column j represents the similarity between the graphs in positions     i and j of the input dataset.</p> <p> source method BaseKernel.create_test_kernel_matrix(test_dataset: Sequence[KernelData], train_dataset: Sequence[KernelData]) \u2192 np.ndarray </p> <p>Compute a kernel matrix for a given testing dataset and training set.</p> <p>This method computes an N x M kernel matrix from the Jensen-Shannon divergences between all pairs of graphs in the input testing dataset and the training dataset. The resulting matrix can be used as a similarity metric for machine learning algorithms, particularly when evaluating the performance on the test dataset using a trained model. Args:     test_dataset: The testing dataset.     train_dataset: The training set. Returns:     np.ndarray: An M x N matrix where the entry at row i and column j     represents the similarity between the graph in position i of the     test dataset and the graph in position j of the training set.</p> <p> source method BaseKernel.set_params(**kwargs: dict[str, Any]) \u2192 None </p> <p>Set multiple parameters for the kernel.</p> <p> Parameters </p> <ul> <li> <p>**kwargs :  dict[str, Any] \u2014 Arbitrary keyword dictionary where keys are attribute names</p> </li> <li> <p>and values are their respective values</p> </li> </ul> <p> source method BaseKernel.get_params(deep: bool = True) \u2192 dict[str, Any] </p> <p>Retrieve the value of all parameters.</p> <p> Parameters </p> <ul> <li> <p>deep :  bool \u2014 Ignored for the time being. Added for compatibility with various machine learning libraries, such as scikit-learn.</p> </li> </ul> <p>Returns     dict: A dictionary of parameters and their respective values.         Note that this method always performs a copy of the dictionary.</p> <p> source class FastQEK() </p> <p><p>Bases : BaseKernel[ProcessedData]</p></p> <p>FastQEK class.</p> <p> Attributes </p> <ul> <li> <p>- X (Sequence[ProcessedData]) \u2014 Training data used for fitting the kernel.</p> </li> <li> <p>- kernel_matrix (np.ndarray) \u2014 Kernel matrix. This is assigned in the <code>fit()</code> method</p> </li> </ul> <p> Training parameters </p> <p>mu (float): Scaling factor for the Jensen-Shannon divergence size_max (int, optional): If specified, only consider the first <code>size_max</code>     qubits of bitstrings. Otherwise, consider all qubits. You may use this     to trade precision in favor of speed.</p> <p>Note: This class does not accept raw data, but rather <code>ProcessedData</code>. See class IntegratedQEK for a subclass that provides a more powerful API, at the expense of performance.</p> <p> Methods </p> <ul> <li> <p>to_processed_data \u2014 Convert the raw data into features.</p> </li> </ul> <p> source method FastQEK.to_processed_data(X: Sequence[ProcessedData]) \u2192 Sequence[ProcessedData] </p> <p>Convert the raw data into features.</p> <p> source class IntegratedQEK(mu: float, extractor: BaseExtractor[GraphType], size_max: int | None = None, similarity: Callable[[NDArray[np.floating], NDArray[np.floating]], np.floating] | None = None) </p> <p><p>Bases : BaseKernel[GraphType]</p></p> <p>A variant of the Quantum Evolution Kernel that supports fit/transform/fit_transform from raw data (graphs).</p> <p>Initialize an IntegratedQEK</p> <p> Performance note </p> <p>This class uses an extractor to convert the raw data into features. This can be very slow if you use, for instance, a remote QPU, as the waitlines to access a QPU can be very long. If you are using this in an interactive application or a server, this will block the entire thread during the wait.</p> <p>We recommend using this class only with local emulators.</p> <p> Training parameters </p> <p>mu (float): Scaling factor for the Jensen-Shannon divergence extractor: An extractor (e.g. a QPU or a Quantum emulator) used to conver the raw data (graphs) into features. size_max (int, optional): If specified, only consider the first <code>size_max</code>     qubits of bitstrings. Otherwise, consider all qubits. You may use this     to trade precision in favor of speed. similarity (optional): If specified, a custom similarity metric to use. Otherwise,     use the Jensen-Shannon divergence.</p> <p> Parameters </p> <ul> <li> <p>mu :  float \u2014 Scaling factor for the Jensen-Shannon divergence</p> </li> <li> <p>extractor :  BaseExtractor[GraphType] \u2014 An extractor (e.g. a QPU or a Quantum emulator) used to conver the raw data (graphs) into features.</p> </li> <li> <p>size_max :  int, optional \u2014 If specified, only consider the first <code>size_max</code> qubits of bitstrings. Otherwise, consider all qubits. You may use this to trade precision in favor of speed.</p> </li> <li> <p>similarity :  optional \u2014 If specified, a custom similarity metric to use. Otherwise, use the Jensen-Shannon divergence.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>to_processed_data \u2014 Convert the raw data into features.</p> </li> </ul> <p> source method IntegratedQEK.to_processed_data(X: Sequence[GraphType]) \u2192 Sequence[ProcessedData] </p> <p>Convert the raw data into features.</p> <p> Performance note </p> <p>This method can can be very slow if you use, for instance, a remote QPU, as the waitlines to access a QPU can be very long. If you are using this in an interactive application or a server, this will block the entire thread during the wait.</p> <p> source count_occupation_from_bitstring(bitstring: str) \u2192 int </p> <p>Counts the number of '1' bits in a binary string.</p> <p> Parameters </p> <ul> <li> <p>bitstring :  str \u2014 A binary string containing only '0's and '1's.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>int \u2014 The number of '1' bits found in the input string.</p> </li> </ul> <p> source dist_excitation_and_vec(count_bitstring: dict[str, int], size_max: int | None = None) \u2192 np.ndarray </p> <p>Calculates the distribution of excitation energies from a dictionary of bitstrings to their respective counts.</p> <p> Parameters </p> <ul> <li> <p>count_bitstring :  dict[str, int] \u2014 A dictionary mapping binary strings to their counts.</p> </li> <li> <p>size_max :  int | None \u2014 If specified, only keep <code>size_max</code> energy distributions in the output. Otherwise, keep all values.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>np.ndarray \u2014 A NumPy array where keys are the number of '1' bits     in each binary string and values are the normalized counts.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>ValueError</p> </li> </ul>"},{"location":"src/qek/kernel/kernel/","title":"qek.kernel.kernel","text":"qek.kernel.kernel<p> docs module qek.kernel.kernel </p> <pre><code>\"\"\"\nThe Quantum Evolution Kernel itself, for use in a machine-learning pipeline.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport abc\nfrom typing import Any, Callable, Generic, TypeVar, cast\nimport collections\nimport copy\nfrom collections.abc import Sequence\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom scipy.spatial.distance import jensenshannon\n\nfrom qek.data.processed_data import ProcessedData\nfrom qek.data.extractors import BaseExtractor, GraphType\n\nKernelData = TypeVar(\"KernelData\")\n\n\nclass BaseKernel(abc.ABC, Generic[KernelData]):docs\n    \"\"\"\n    Base class for implementations of the Quantum Evolution Kernel.\n\n    Unless you are implementing a new kernel, you should probably consider\n    using one of the subclasses:\n    - FastQEK (lower-level API, requires processed data, optimized);\n    - IntegratedQEK (higher-level API, accepts graphs, slower).\n\n    Attributes:\n    - X (Sequence[ProcessedData]): Training data used for fitting the kernel.\n    - kernel_matrix (np.ndarray): Kernel matrix. This is assigned in the `fit()` method\n\n    Training parameters:\n        mu (float): Scaling factor for the Jensen-Shannon divergence\n        size_max (int, optional): If specified, only consider the first `size_max`\n            qubits of bitstrings. Otherwise, consider all qubits. You may use this\n            to trade precision in favor of speed.\n\n    Note: This class does **not** accept raw data, but rather `ProcessedData`. See\n    class IntegratedQuantumEvolutionKernel for a subclass that provides a more powerful API,\n    at the expense of performance.\n    \"\"\"\n\n    def __init__(\n        self,\n        mu: float,\n        size_max: int | None = None,\n        similarity: (\n            Callable[[NDArray[np.floating], NDArray[np.floating]], np.floating] | None\n        ) = None,\n    ):\n        \"\"\"Initialize the kernel.\n\n        Args:\n            mu (float): Scaling factor for the Jensen-Shannon divergence\n            size_max (int, optional): If specified, only consider the first `size_max`\n                qubits of bitstrings. Otherwise, consider all qubits. You may use this\n                to trade precision in favor of speed.\n            similarity (optional): If specified, a custom similarity metric to use. Otherwise,\n                use the Jensen-Shannon divergence.\n        \"\"\"\n        self._params: dict[str, Any] = {\n            \"mu\": mu,\n            \"size_max\": size_max,\n            \"similarity\": similarity,\n        }\n        self.X: Sequence[ProcessedData]\n        self.kernel_matrix: np.ndarray\n\n    @abc.abstractmethoddocs\n    def to_processed_data(self, X: Sequence[KernelData]) -&gt; Sequence[ProcessedData]:\n        \"\"\"\n        Convert the raw data into features.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(\n        self,\n        X1: Sequence[KernelData],\n        X2: Sequence[KernelData] | None = None,\n    ) -&gt; NDArray[np.floating]:\n        \"\"\"Compute a kernel matrix from two sequences of processed data.\n\n        This method computes a M x N kernel matrix from the Jensen-Shannon divergences\n        between all pairs of graphs in the two datasets. The resulting matrix can be used\n        as a similarity metric for machine learning algorithms.\n\n        If `X1` and `X2` are two sequences representing the processed data for a\n        single graph each, the resulting matrix can be used as a measure of similarity\n        between both graphs.\n\n        Args:\n            X1: processed data to be used as rows.\n            X2 (optional): processed data to be used as columns. If unspecified, use X1\n                as both rows and columns.\n        Returns:\n            np.ndarray: A len(X1) x len(X2) matrix where entry[i, j] represents the\n            similarity between rows[i] and columns[j], scaled by a factor that depends\n            on mu.\n        Notes:\n            The JSD is computed using the jensenshannon function from\n            `scipy.spatial.distance`, and it is squared because scipy function\n            `jensenshannon` outputs the distance instead of the divergence.\n        \"\"\"\n        # Convert as needed.\n        # This can be *very* slow, depending on the implementation of `to_processed_data`.\n        p1 = self.to_processed_data(X1)\n        p2 = None\n        if X2 is not None:\n            p2 = self.to_processed_data(X2)\n\n        # If size is not specified, set it to the length of the largest bitstring.\n        size_max = self._params[\"size_max\"]\n        if size_max is None:\n            if p2 is None:\n                # No need to walk the same source twice.\n                sources = [p1]\n            else:\n                sources = [p1, p2]\n            for source in sources:\n                for data in source:\n                    length = len(data._sequence.qubit_info)\n                    if size_max is None or size_max &lt;= length:\n                        size_max = length\n\n        # Note: At this stage, size_max could theoretically still be `None``, if both `X1` and `X2`\n        # are empty. In such cases, `dist_excitation` will never be called, so we're ok.\n\n        feat_rows = [row.dist_excitation(size_max) for row in p1]\n        similarity = cast(\n            Callable[[NDArray[np.floating], NDArray[np.floating]], np.floating],\n            self._params[\"similarity\"],\n        )\n\n        if similarity is None:\n            similarity = self.default_similarity\n\n        if p2 is None:\n            # Fast path:\n            # - rows and columns are identical, so no need to compute a `feat_cols`;\n            # - the matrix is symmetric, we only need to compute half of it.\n            #\n            # We could avoid computing kernel[i, i], as we know that it's always 1,\n            # but we do not perform this specific optimization, as it is a useful\n            # canary to detect some bugs.\n            kernel = np.zeros([len(p1), len(p1)])\n            for i, dist_row in enumerate(feat_rows):\n                for j in range(i, len(feat_rows)):\n                    dist_col = feat_rows[j]\n                    s = similarity(dist_row, dist_col)\n                    kernel[i, j] = s\n                    if j != i:\n                        kernel[j, i] = s\n        else:\n            # Slow path:\n            # - we need to compute a `feat_columns`\n            # - the matrix is generally not symmetric and diagonal entries are generally not 1.\n            kernel = np.zeros([len(p1), len(p2)])\n            feat_columns = [col.dist_excitation(size_max) for col in p2]\n            for i, dist_row in enumerate(feat_rows):\n                for j, dist_col in enumerate(feat_columns):\n                    kernel[i, j] = similarity(dist_row, dist_col)\n        return kernel\n\n    def default_similarity(docs\n        self, row: NDArray[np.floating], col: NDArray[np.floating]\n    ) -&gt; np.floating:\n        \"\"\"\n        The Jensen-Shannon similarity metric used to compute the kernel, used when calling `kernel(X1, X2)`.\n\n        This is the default similarity, if no parameter `similarity` is provided.\n        \"\"\"\n        js = jensenshannon(row, col) ** 2\n        mu = float(self._params[\"mu\"])\n        return np.exp(-mu * js)\ndocs\n    def similarity(self, graph_1: KernelData, graph_2: KernelData) -&gt; float:\n        \"\"\"Compute the similarity between two graphs using Jensen-Shannon\n        divergence.\n\n        This method computes the square of the Jensen-Shannon divergence (JSD)\n        between two probability distributions over bitstrings. The JSD is a\n        measure of the difference between two probability distributions, and it\n        can be used as a kernel for machine learning algorithms that require a\n        similarity function.\n\n        The input graphs are assumed to have been processed using the\n        ProcessedData class from qek_os.data_io.dataset.\n\n        Args:\n            graph_1: First graph.\n            graph_2: Second graph.\n\n        Returns:\n            float: Similarity between the two graphs, scaled by a factor that\n            depends on mu.\n\n        Notes:\n            The JSD is computed using the jensenshannon function from\n            `scipy.spatial.distance`, and it is squared because scipy function\n            `jensenshannon` outputs the distance instead of the divergence.\n        \"\"\"\n        matrix = self([graph_1], [graph_2])\n        return float(matrix[0, 0])\ndocs\n    def fit(self, X: Sequence[KernelData], y: list | None = None) -&gt; None:\n        \"\"\"Fit the kernel to the training dataset by storing the dataset.\n\n        Args:\n            X: The training dataset.\n            y: list: Target variable for the dataset sequence.\n                This argument is ignored, provided only for compatibility\n                with machine-learning libraries.\n        \"\"\"\n        self._X = X\n        self._kernel_matrix = self.create_train_kernel_matrix(self._X)\ndocs\n    def transform(self, X_test: Sequence[KernelData], y_test: list | None = None) -&gt; np.ndarray:\n        \"\"\"Transform the dataset into the kernel space with respect to the training dataset.\n\n        Args:\n            X_test: The dataset to transform.\n            y_test: list: Target variable for the dataset sequence.\n                This argument is ignored, provided only for compatibility\n                with machine-learning libraries.\n        Returns:\n            np.ndarray: Kernel matrix where each entry represents the similarity between\n                        the given dataset and the training dataset.\n        \"\"\"\n        if self._X is None:\n            raise ValueError(\"The kernel must be fit to a training dataset before transforming.\")\n\n        return self.create_test_kernel_matrix(X_test, self._X)\ndocs\n    def fit_transform(self, X: Sequence[KernelData], y: list | None = None) -&gt; np.ndarray:\n        \"\"\"Fit the kernel to the training dataset and transform it.\n\n        Args:\n            X: The dataset to fit and transform.\n            y: list: Target variable for the dataset sequence.\n                This argument is ignored, provided only for compatibility\n                with machine-learning libraries.\n        Returns:\n            np.ndarray: Kernel matrix for the training dataset.\n        \"\"\"\n        self.fit(X)\n        return self._kernel_matrix\ndocs\n    def create_train_kernel_matrix(self, train_dataset: Sequence[KernelData]) -&gt; np.ndarray:\n        \"\"\"Compute a kernel matrix for a given training dataset.\n\n        This method computes a symmetric N x N kernel matrix from the\n        Jensen-Shannon divergences between all pairs of graphs in the input\n        dataset. The resulting matrix can be used as a similarity metric for\n        machine learning algorithms.\n        Args:\n            train_dataset: A list of objects to compute the kernel matrix from.\n        Returns:\n            np.ndarray: An N x N symmetric matrix where the entry at row i and\n            column j represents the similarity between the graphs in positions\n            i and j of the input dataset.\n        \"\"\"\n        return self(train_dataset)\n\n    def create_test_kernel_matrix(docs\n        self,\n        test_dataset: Sequence[KernelData],\n        train_dataset: Sequence[KernelData],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Compute a kernel matrix for a given testing dataset and training\n        set.\n\n        This method computes an N x M kernel matrix from the Jensen-Shannon\n        divergences between all pairs of graphs in the input testing dataset\n        and the training dataset.\n        The resulting matrix can be used as a similarity metric for machine\n        learning algorithms,\n        particularly when evaluating the performance on the test dataset using\n        a trained model.\n        Args:\n            test_dataset: The testing dataset.\n            train_dataset: The training set.\n        Returns:\n            np.ndarray: An M x N matrix where the entry at row i and column j\n            represents the similarity between the graph in position i of the\n            test dataset and the graph in position j of the training set.\n        \"\"\"\n        return self(test_dataset, train_dataset)\n\n    def set_params(self, **kwargs: dict[str, Any]) -&gt; None:docs\n        \"\"\"Set multiple parameters for the kernel.\n\n        Args:\n            **kwargs: Arbitrary keyword dictionary where keys are attribute names\n            and values are their respective values\n        \"\"\"\n        for key, value in kwargs.items():\n            self._params[key] = value\n\n    def get_params(self, deep: bool = True) -&gt; dict[str, Any]:docs\n        \"\"\"Retrieve the value of all parameters.\n\n         Args:\n            deep (bool): Ignored for the time being. Added for compatibility with\n                various machine learning libraries, such as scikit-learn.\n\n        Returns\n            dict: A dictionary of parameters and their respective values.\n                Note that this method always performs a copy of the dictionary.\n        \"\"\"\n        return copy.deepcopy(self._params)\n\n\nclass FastQEK(BaseKernel[ProcessedData]):docs\n    \"\"\"FastQEK class.\n\n    Attributes:\n    - X (Sequence[ProcessedData]): Training data used for fitting the kernel.\n    - kernel_matrix (np.ndarray): Kernel matrix. This is assigned in the `fit()` method\n\n    Training parameters:\n        mu (float): Scaling factor for the Jensen-Shannon divergence\n        size_max (int, optional): If specified, only consider the first `size_max`\n            qubits of bitstrings. Otherwise, consider all qubits. You may use this\n            to trade precision in favor of speed.\n\n    Note: This class does **not** accept raw data, but rather `ProcessedData`. See\n    class IntegratedQEK for a subclass that provides a more powerful API,\n    at the expense of performance.\n    \"\"\"\ndocs\n    def to_processed_data(self, X: Sequence[ProcessedData]) -&gt; Sequence[ProcessedData]:\n        \"\"\"\n        Convert the raw data into features.\n        \"\"\"\n        return X\n\n\nclass IntegratedQEK(BaseKernel[GraphType]):docs\n    \"\"\"\n    A variant of the Quantum Evolution Kernel that supports fit/transform/fit_transform from raw data (graphs).\n\n    Performance note:\n        This class uses an extractor to convert the raw data into features. This can be very slow if\n        you use, for instance, a remote QPU, as the waitlines to access a QPU can be very long. If you\n        are using this in an interactive application or a server, this will block the entire thread\n        during the wait.\n\n        We recommend using this class only with local emulators.\n\n    Training parameters:\n        mu (float): Scaling factor for the Jensen-Shannon divergence\n        extractor: An extractor (e.g. a QPU or a Quantum emulator) used to conver the raw data (graphs) into features.\n        size_max (int, optional): If specified, only consider the first `size_max`\n            qubits of bitstrings. Otherwise, consider all qubits. You may use this\n            to trade precision in favor of speed.\n        similarity (optional): If specified, a custom similarity metric to use. Otherwise,\n            use the Jensen-Shannon divergence.\n    \"\"\"\n\n    def __init__(\n        self,\n        mu: float,\n        extractor: BaseExtractor[GraphType],\n        size_max: int | None = None,\n        similarity: (\n            Callable[[NDArray[np.floating], NDArray[np.floating]], np.floating] | None\n        ) = None,\n    ):\n        \"\"\"\n        Initialize an IntegratedQEK\n\n        Arguments:\n            mu (float): Scaling factor for the Jensen-Shannon divergence\n            extractor: An extractor (e.g. a QPU or a Quantum emulator) used to conver the raw data (graphs) into features.\n            size_max (int, optional): If specified, only consider the first `size_max`\n                qubits of bitstrings. Otherwise, consider all qubits. You may use this\n                to trade precision in favor of speed.\n            similarity (optional): If specified, a custom similarity metric to use. Otherwise,\n                use the Jensen-Shannon divergence.\n        \"\"\"\n        super().__init__(mu=mu, size_max=size_max, similarity=similarity)\n        self._params[\"extractor\"] = extractor\ndocs\n    def to_processed_data(self, X: Sequence[GraphType]) -&gt; Sequence[ProcessedData]:\n        \"\"\"\n        Convert the raw data into features.\n\n        Performance note:\n            This method can can be very slow if you use, for instance, a remote QPU, as the waitlines to\n            access a QPU can be very long. If you are using this in an interactive application or a server,\n            this will block the entire thread during the wait.\n        \"\"\"\n        if len(X) == 0:\n            return []\n        if isinstance(X[0], ProcessedData):\n            return cast(Sequence[ProcessedData], X)\n        graphs = [cast(GraphType, g) for g in X]\n        extractor: BaseExtractor[GraphType] = self._params[\"extractor\"]\n        extractor.add_graphs(graphs)\n        extracted = extractor.run()\n        # Performance warning: this line can take hours to execute, if there's a long wait before\n        # being allocated a QPU!\n        return extracted.processed_data\n\n\ndef count_occupation_from_bitstring(bitstring: str) -&gt; int:docs\n    \"\"\"Counts the number of '1' bits in a binary string.\n\n    Args:\n        bitstring (str): A binary string containing only '0's and '1's.\n\n    Returns:\n        int: The number of '1' bits found in the input string.\n    \"\"\"\n    return sum(int(bit) for bit in bitstring)\n\n\ndef dist_excitation_and_vec(docs\n    count_bitstring: dict[str, int], size_max: int | None = None\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculates the distribution of excitation energies from a dictionary of\n    bitstrings to their respective counts.\n\n    Args:\n        count_bitstring (dict[str, int]): A dictionary mapping binary strings\n            to their counts.\n        size_max (int | None): If specified, only keep `size_max` energy\n            distributions in the output. Otherwise, keep all values.\n\n    Returns:\n        np.ndarray: A NumPy array where keys are the number of '1' bits\n            in each binary string and values are the normalized counts.\n    \"\"\"\n\n    if len(count_bitstring) == 0:\n        raise ValueError(\"The input counter is empty\")\n\n    if size_max is None:\n        # If size is not specified, it's the length of bitstrings.\n        # We assume that all bitstrings in `count_bitstring` have the\n        # same length and we have just checked that it's not empty.\n\n        # Pick the length of the first bitstring.\n        # We have already checked that `count_bitstring` is not empty.\n        bitstring = next(iter(count_bitstring.keys()))\n        size_max = len(bitstring)\n\n    # Make mypy realize that `size_max` is now always an `int`.\n    assert type(size_max) is int\n\n    count_occupation: dict[int, int] = collections.defaultdict(int)\n    total = 0.0\n    for k, v in count_bitstring.items():\n        occupation = count_occupation_from_bitstring(k)\n        count_occupation[occupation] += v\n        total += v\n\n    numpy_vec = np.zeros(size_max + 1, dtype=float)\n    for occupation, count in count_occupation.items():\n        if occupation &lt; size_max:\n            numpy_vec[occupation] = count / total\n\n    return numpy_vec\n</code></pre>"},{"location":"api/qek/shared/","title":"qek.shared","text":"qek.shared<p> source package qek.shared </p> <p>Shared utility code.</p> <p> Modules </p> <ul> <li> <p>qek.shared.error \u2014 Exceptions raised within this library.</p> </li> <li> <p>qek.shared.retrier \u2014 Backoff-and-retry utilities.</p> </li> </ul>"},{"location":"src/qek/shared/","title":"qek.shared","text":"qek.shared<p> docs package qek.shared </p> <pre><code>\"\"\"\nShared utility code.\n\"\"\"\n</code></pre>"},{"location":"api/qek/shared/error/","title":"qek.shared.error","text":"qek.shared.error<p> source module qek.shared.error </p> <p>Exceptions raised within this library.</p> <p> Classes </p> <ul> <li> <p>CompilationError \u2014 An error raised when attempting to compile a graph for an architecture that does not support it, e.g. because it requires too many qubits or because the physical constraints on the geometry are not satisfied.</p> </li> </ul> <p> source class CompilationError() </p> <p><p>Bases : Exception</p></p> <p>An error raised when attempting to compile a graph for an architecture that does not support it, e.g. because it requires too many qubits or because the physical constraints on the geometry are not satisfied.</p>"},{"location":"src/qek/shared/error/","title":"qek.shared.error","text":"qek.shared.error<p> docs module qek.shared.error </p> <pre><code>\"\"\"\nExceptions raised within this library.\n\"\"\"\n\n\nclass CompilationError(Exception):docs\n    \"\"\"\n    An error raised when attempting to compile a graph for an architecture\n    that does not support it, e.g. because it requires too many qubits or\n    because the physical constraints on the geometry are not satisfied.\n    \"\"\"\n</code></pre>"},{"location":"api/qek/shared/retrier/","title":"qek.shared.retrier","text":"qek.shared.retrier<p> source module qek.shared.retrier </p> <p>Backoff-and-retry utilities.</p> <p> Classes </p> <ul> <li> <p>PygRetrier \u2014 Our test harness attempts to run tests concurrently, but the pyg dataset loader does not work well with concurrency.</p> </li> </ul> <p> source class PygRetrier(max_attempts: int = 3, name: str = 'PygRetrier') </p> <p>Our test harness attempts to run tests concurrently, but the pyg dataset loader does not work well with concurrency.</p> <p>We work around this by simply retrying the loads a few times, until it succeeds.</p> <p>Create a PygRetrier</p> <p> Parameters </p> <ul> <li> <p>max_attempts :  optional \u2014 The max number of attempts to undertake before giving up. Defaults to 3.</p> </li> <li> <p>name :  optional \u2014 A name to use during logging.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>insist \u2014 Attempt to call a function or constructor repeatedly until, hopefully, it works.</p> </li> </ul> <p> source method PygRetrier.insist(callback: Type[Dataset], **kwargs: Any) \u2192 Dataset </p> <p>Attempt to call a function or constructor repeatedly until, hopefully, it works.</p> <p> Raises </p> <ul> <li> <p>exn</p> </li> </ul>"},{"location":"src/qek/shared/retrier/","title":"qek.shared.retrier","text":"qek.shared.retrier<p> docs module qek.shared.retrier </p> <pre><code>\"\"\"\nBackoff-and-retry utilities.\n\"\"\"\n\nimport logging\nfrom time import sleep\nfrom typing import Any, Final, Type\n\nfrom torch_geometric.data import Dataset\n\nlogger = logging.getLogger(__name__)\n\n\nclass PygRetrier:docs\n    \"\"\"\n    Our test harness attempts to run tests concurrently, but the pyg dataset loader does not\n    work well with concurrency.\n\n    We work around this by simply retrying the loads a few times, until it succeeds.\n    \"\"\"\n\n    def __init__(self, max_attempts: int = 3, name: str = \"PygRetrier\"):\n        \"\"\"\n        Create a PygRetrier\n\n        Arguments:\n            max_attempts (optional): The max number of attempts to undertake before\n                giving up. Defaults to 3.\n            name (optional): A name to use during logging.\n        \"\"\"\n        self._max_attempts: Final[int] = max_attempts\n        self.name: Final[str] = name\n\n    def insist(self, callback: Type[Dataset], **kwargs: Any) -&gt; Dataset:docs\n        \"\"\"\n        Attempt to call a function or constructor repeatedly until, hopefully,\n        it works.\n        \"\"\"\n        exn: FileNotFoundError | RuntimeError | OSError | None = None\n        result = None\n        for i in range(self._max_attempts):\n            sleep(i * i)\n            try:\n                logger.debug(\"%s: attempt %s\", self.name, i + 1)\n                result = callback(**kwargs)  # type: ignore\n                logger.debug(\"%s: attempt %s succeeded\", self.name, i + 1)\n                exn = None\n                break\n            except (FileNotFoundError, RuntimeError, OSError) as e:\n                logger.warning(\"%s: attempt %s failed: %s\", self.name, i + 1, e)\n                exn = e\n        if exn is not None:\n            logger.warning(\"%s: all attempts failed, bailing out\", self.name)\n            raise exn\n        assert result is not None\n        return result\n</code></pre>"},{"location":"api/qek/target/","title":"qek.target","text":"qek.target<p> source package qek.target </p> <p>Quantum compilation targets</p> <p> Modules </p> <ul> <li> <p>qek.target.backends \u2014 Low-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.</p> </li> <li> <p>qek.target.targets \u2014 Code emitted by compilation.</p> </li> </ul>"},{"location":"src/qek/target/","title":"qek.target","text":"qek.target<p> docs package qek.target </p> <pre><code>\"\"\"\nQuantum compilation targets\n\"\"\"\n</code></pre>"},{"location":"api/qek/target/backends/","title":"qek.target.backends","text":"qek.target.backends<p> source module qek.target.backends </p> <p>Low-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.</p> <p> Classes </p> <ul> <li> <p>BaseBackend \u2014 Low-level abstraction to execute a Register and a Pulse on a Quantum Device.</p> </li> <li> <p>QutipBackend \u2014 Execute a Register and a Pulse on the Qutip Emulator.</p> </li> <li> <p>BaseRemoteBackend \u2014 Base hierarch for remote backends.</p> </li> <li> <p>RemoteQPUBackend \u2014 Execute on a remote QPU.</p> </li> <li> <p>RemoteEmuMPSBackend \u2014 A backend that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud.</p> </li> <li> <p>EmuMPSBackend \u2014 Execute a Register and a Pulse on the high-performance emu-mps Emulator.</p> </li> </ul> <p> source class BaseBackend(device: Device | None) </p> <p><p>Bases : abc.ABC</p></p> <p>Low-level abstraction to execute a Register and a Pulse on a Quantum Device.</p> <p>For higher-level abstractions, see <code>BaseExtractor</code> and its subclasses.</p> <p>The sole role of these abstractions is to provide the same API for all backends. They might be removed in a future version, once Pulser has gained a similar API.</p> <p> Methods </p> <ul> <li> <p>run \u2014 Execute a register and a pulse.</p> </li> </ul> <p> source async method BaseBackend.run(self, register: targets.Register, pulse: targets.Pulse) \u2192 Counter[str] </p> <p>Execute a register and a pulse.</p> <p> Returns </p> <ul> <li> <p>Counter[str] \u2014 A bitstring Counter, i.e. a data structure counting for each bitstring the number of measured instances of this bitstring.</p> </li> </ul> <p> Raises </p> <ul> <li> <p>NotImplementedError</p> </li> </ul> <p> source class QutipBackend(device: Device) </p> <p><p>Bases : BaseBackend</p></p> <p>Execute a Register and a Pulse on the Qutip Emulator.</p> <p>Please consider using EmuMPSBackend, which generally works much better with higher number of qubits.</p> <p> Performance warning </p> <p>Executing anything quantum related on an emulator takes an amount of resources polynomial in 2^N, where N is the number of qubits. This can easily go beyond the limit of the computer on which you're executing it.</p> <p> Methods </p> <ul> <li> <p>run \u2014 Execute a register and a pulse.</p> </li> </ul> <p> source async method QutipBackend.run(self, register: targets.Register, pulse: targets.Pulse) \u2192 Counter[str] </p> <p>Execute a register and a pulse.</p> <p> Parameters </p> <ul> <li> <p>register :  targets.Register \u2014 The register (geometry) to execute. Typically obtained by compiling a graph.</p> </li> <li> <p>pulse :  targets.Pulse \u2014 The pulse (lasers) to execute. Typically obtained by compiling a graph.</p> </li> </ul> <p> Returns </p> <ul> <li> <p>Counter[str] \u2014 A bitstring Counter, i.e. a data structure counting for each bitstring the number of instances of this bitstring observed at the end of runs.</p> </li> </ul> <p> source class BaseRemoteBackend(project_id: str, username: str, device_name: str = 'FRESNEL', password: str | None = None) </p> <p><p>Bases : BaseBackend</p></p> <p>Base hierarch for remote backends.</p> <p>Create a remote backend</p> <p> Performance warning </p> <p>As of this writing, using remote Backends to access a remote QPU or remote emulator is slower than using a RemoteExtractor, as the RemoteExtractor optimizes the number of connections used to communicate with the cloud server.</p> <p> Parameters </p> <ul> <li> <p>project_id :  str \u2014 The ID of the project on the Pasqal Cloud API.</p> </li> <li> <p>username :  str \u2014 Your username on the Pasqal Cloud API.</p> </li> <li> <p>password :  str | None \u2014 Your password on the Pasqal Cloud API. If you leave this to None, you will need to enter your password manually.</p> </li> <li> <p>device_name :  str \u2014 The name of the device to use. As of this writing, the default value of \"FRESNEL\" represents the latest QPU available through the Pasqal Cloud API.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>device \u2014 Make sure that we have fetched the latest specs for the device from the server.</p> </li> </ul> <p> source async method BaseRemoteBackend.device(self) \u2192 Device </p> <p>Make sure that we have fetched the latest specs for the device from the server.</p> <p> source class RemoteQPUBackend() </p> <p><p>Bases : BaseRemoteBackend</p></p> <p>Execute on a remote QPU.</p> <p> Performance note </p> <p>As of this writing, the waiting lines for a QPU may be very long. You may use this Extractor to resume your workflow with a computation that has been previously started.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method RemoteQPUBackend.run(self, register: targets.Register, pulse: targets.Pulse) \u2192 Counter[str] </p> <p> source class RemoteEmuMPSBackend() </p> <p><p>Bases : BaseRemoteBackend</p></p> <p>A backend that uses a remote high-performance emulator (EmuMPS) published on Pasqal Cloud.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method RemoteEmuMPSBackend.run(self, register: targets.Register, pulse: targets.Pulse, dt: int = 10) \u2192 Counter[str] </p> <p> source class EmuMPSBackend(device: Device) </p> <p><p>Bases : BaseBackend</p></p> <p>Execute a Register and a Pulse on the high-performance emu-mps Emulator.</p> <p>As of this writing, this local emulator is only available under Unix. However, the RemoteEmuMPSBackend is available on all platforms.</p> <p> Performance warning </p> <p>Executing anything quantum related on an emulator takes an amount of resources polynomial in 2^N, where N is the number of qubits. This can easily go beyond the limit of the computer on which you're executing it.</p> <p> Methods </p> <ul> <li> <p>run</p> </li> </ul> <p> source async method EmuMPSBackend.run(self, register: targets.Register, pulse: targets.Pulse, dt: int = 10) \u2192 Counter[str] </p>"},{"location":"src/qek/target/backends/","title":"qek.target.backends","text":"qek.target.backends<p> docs module qek.target.backends </p> <pre><code>\"\"\"\nLow-level tools to execute compiled registers and pulses onto Quantum Devices, including local emulators, remote emulators and physical QPUs.\n\"\"\"\n\nimport abc\nimport asyncio\nfrom math import ceil\nfrom typing import Counter, cast\n\nimport os\nfrom pasqal_cloud import SDK\nfrom pasqal_cloud.device import BaseConfig, EmulatorType\nfrom pasqal_cloud.job import Job\nfrom pulser import Sequence\nfrom pulser.devices import Device\nfrom pulser_simulation import QutipEmulator\n\nfrom qek.data.extractors import deserialize_device\nfrom qek.shared.error import CompilationError\nfrom qek.shared._utils import make_sequence\nfrom qek.target import targets\n\n\nclass BaseBackend(abc.ABC):docs\n    \"\"\"\n    Low-level abstraction to execute a Register and a Pulse on a Quantum Device.\n\n    For higher-level abstractions, see `BaseExtractor` and its subclasses.\n\n    The sole role of these abstractions is to provide the same API for all backends.\n    They might be removed in a future version, once Pulser has gained a similar API.\n    \"\"\"\n\n    def __init__(self, device: Device | None):\n        self._device = device\n\n    def _make_sequence(self, register: targets.Register, pulse: targets.Pulse) -&gt; Sequence:\n        assert self._device is not None\n        return make_sequence(register=register, pulse=pulse, device=self._device)\n\n    @abc.abstractmethoddocs\n    async def run(self, register: targets.Register, pulse: targets.Pulse) -&gt; Counter[str]:\n        \"\"\"\n        Execute a register and a pulse.\n\n        Returns:\n            A bitstring Counter, i.e. a data structure counting for each bitstring\n            the number of measured instances of this bitstring.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass QutipBackend(BaseBackend):docs\n    \"\"\"\n    Execute a Register and a Pulse on the Qutip Emulator.\n\n    Please consider using EmuMPSBackend, which generally works much better with\n    higher number of qubits.\n\n    Performance warning:\n        Executing anything quantum related on an emulator takes an amount of resources\n        polynomial in 2^N, where N is the number of qubits. This can easily go beyond\n        the limit of the computer on which you're executing it.\n    \"\"\"\n\n    def __init__(self, device: Device):\n        super().__init__(device)\ndocs\n    async def run(self, register: targets.Register, pulse: targets.Pulse) -&gt; Counter[str]:\n        \"\"\"\n        Execute a register and a pulse.\n\n        Arguments:\n            register: The register (geometry) to execute. Typically obtained by compiling a graph.\n            pulse: The pulse (lasers) to execute. Typically obtained by compiling a graph.\n\n        Returns:\n            A bitstring Counter, i.e. a data structure counting for each bitstring\n            the number of instances of this bitstring observed at the end of runs.\n        \"\"\"\n        sequence = self._make_sequence(register=register, pulse=pulse)\n        emulator = QutipEmulator.from_sequence(sequence)\n        result: Counter[str] = emulator.run().sample_final_state()\n        return result\n\n\nclass BaseRemoteBackend(BaseBackend):docs\n    \"\"\"\n    Base hierarch for remote backends.\n\n    Performance warning:\n        As of this writing, using remote Backends to access a remote QPU or remote emulator\n        is slower than using a RemoteExtractor, as the RemoteExtractor optimizes the number\n        of connections used to communicate with the cloud server.\n    \"\"\"\n\n    def __init__(\n        self,\n        project_id: str,\n        username: str,\n        device_name: str = \"FRESNEL\",\n        password: str | None = None,\n    ):\n        \"\"\"\n        Create a remote backend\n\n        Args:\n            project_id: The ID of the project on the Pasqal Cloud API.\n            username: Your username on the Pasqal Cloud API.\n            password: Your password on the Pasqal Cloud API. If you leave\n                this to None, you will need to enter your password manually.\n            device_name: The name of the device to use. As of this writing,\n                the default value of \"FRESNEL\" represents the latest QPU\n                available through the Pasqal Cloud API.\n        \"\"\"\n        self.device_name = device_name\n        self._sdk = SDK(username=username, project_id=project_id, password=password)\n        self._max_runs = 500\n        self._sequence = None\n        self._device = None\n\n    async def device(self) -&gt; Device:docs\n        \"\"\"\n        Make sure that we have fetched the latest specs for the device from the server.\n        \"\"\"\n        if self._device is not None:\n            return self._device\n\n        # Fetch the latest list of QPUs\n        # Implementation note: Currently sync, hopefully async in the future.\n        specs = self._sdk.get_device_specs_dict()\n        self._device = cast(Device, deserialize_device(specs[self.device_name]))\n\n        # As of this writing, the API doesn't support runs longer than 500 jobs.\n        # If we want to add more runs, we'll need to split them across several jobs.\n        if isinstance(self._device.max_runs, int):\n            self._max_runs = self._device.max_runs\n\n        return self._device\n\n    async def _run(\n        self,\n        register: targets.Register,\n        pulse: targets.Pulse,\n        emulator: EmulatorType | None,\n        config: BaseConfig | None = None,\n        sleep_sec: int = 2,\n    ) -&gt; Job:\n        \"\"\"\n        Run the pulse + register.\n\n        Arguments:\n            register: A register to run.\n            pulse: A pulse to execute.\n            emulator: The emulator to use, or None to run on a QPU.\n            config: The backend-specific config.\n            sleep_sec (optional): The amount of time to sleep when waiting for the remote server to respond, in seconds. Defaults to 2.\n\n        Raises:\n            CompilationError: If the register/pulse may not be executed on this device.\n        \"\"\"\n        device = await self.device()\n        try:\n            sequence = make_sequence(device=device, pulse=pulse, register=register)\n\n            self._sequence = sequence\n        except ValueError as e:\n            raise CompilationError(f\"This register/pulse cannot be executed on the device: {e}\")\n\n        # Enqueue execution.\n        batch = self._sdk.create_batch(\n            serialized_sequence=sequence.to_abstract_repr(),\n            jobs=[{\"runs\": self._max_runs}],\n            wait=False,\n            emulator=emulator,\n            configuration=config,\n        )\n\n        # Wait for execution to complete.\n        while True:\n            await asyncio.sleep(sleep_sec)\n            # Currently sync, hopefully async in the future.\n            batch.refresh()\n            if batch.status in {\"PENDING\", \"RUNNING\"}:\n                # Continue waiting.\n                continue\n            job = next(iter(batch.jobs.values()))\n            if job.status == \"ERROR\":\n                raise Exception(f\"Error while executing remote job: {job.errors}\")\n            return job\n\n\nclass RemoteQPUBackend(BaseRemoteBackend):docs\n    \"\"\"\n    Execute on a remote QPU.\n\n    Performance note:\n        As of this writing, the waiting lines for a QPU\n        may be very long. You may use this Extractor to resume your workflow\n        with a computation that has been previously started.\n    \"\"\"\ndocs\n    async def run(self, register: targets.Register, pulse: targets.Pulse) -&gt; Counter[str]:\n        job = await self._run(register, pulse, emulator=None, config=None)\n        return cast(Counter[str], job.result)\n\n\nclass RemoteEmuMPSBackend(BaseRemoteBackend):docs\n    \"\"\"\n    A backend that uses a remote high-performance emulator (EmuMPS)\n    published on Pasqal Cloud.\n    \"\"\"\n\n    async def run(docs\n        self, register: targets.Register, pulse: targets.Pulse, dt: int = 10\n    ) -&gt; Counter[str]:\n        job = await self._run(register, pulse, emulator=EmulatorType.EMU_MPS, config=None)\n        bag = cast(dict[str, dict[int, Counter[str]]], job.result)\n\n        assert self._sequence is not None\n        cutoff_duration = int(ceil(self._sequence.get_duration() / dt) * dt)\n        return bag[\"bitstring\"][cutoff_duration]\n\n\nif os.name == \"posix\":\n    import emu_mps\n\n    class EmuMPSBackend(BaseBackend):docs\n        \"\"\"\n        Execute a Register and a Pulse on the high-performance emu-mps Emulator.\n\n        As of this writing, this local emulator is only available under Unix. However,\n        the RemoteEmuMPSBackend is available on all platforms.\n\n        Performance warning:\n            Executing anything quantum related on an emulator takes an amount of resources\n            polynomial in 2^N, where N is the number of qubits. This can easily go beyond\n            the limit of the computer on which you're executing it.\n        \"\"\"\n\n        def __init__(self, device: Device):\n            super().__init__(device)\n\n        async def run(docs\n            self, register: targets.Register, pulse: targets.Pulse, dt: int = 10\n        ) -&gt; Counter[str]:\n            sequence = self._make_sequence(register=register, pulse=pulse)\n            backend = emu_mps.MPSBackend()\n\n            # Configure observable.\n            cutoff_duration = int(ceil(sequence.get_duration() / dt) * dt)\n            observable = emu_mps.BitStrings(evaluation_times={cutoff_duration})\n            config = emu_mps.MPSConfig(observables=[observable], dt=dt)\n            counter: Counter[str] = backend.run(sequence, config)[observable.name][cutoff_duration]\n            return counter\n</code></pre>"},{"location":"api/qek/target/targets/","title":"qek.target.targets","text":"qek.target.targets<p> source module qek.target.targets </p> <p>Code emitted by compilation.</p> <p>In practice, this code is a very thin layer around Pulser's representation.</p> <p> Classes </p> <ul> <li> <p>Pulse \u2014 Specification of a laser pulse to be executed on a quantum device</p> </li> <li> <p>Register \u2014 Specification of a geometry of atoms to be executed on a quantum device</p> </li> </ul> <p> source dataclass Pulse(pulse: pulser.Pulse) </p> <p>Specification of a laser pulse to be executed on a quantum device</p> <p> Attributes </p> <ul> <li> <p>pulse :  pulser.Pulse \u2014 The low-level Pulser pulse.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>draw \u2014 Draw the shape of this laser pulse.</p> </li> </ul> <p> source method Pulse.draw() \u2192 None </p> <p>Draw the shape of this laser pulse.</p> <p> source dataclass Register(device: pulser.devices.Device, register: pulser.Register) </p> <p>Specification of a geometry of atoms to be executed on a quantum device</p> <p> Attributes </p> <ul> <li> <p>device :  pulser.devices.Device \u2014 The quantum device targeted.</p> </li> <li> <p>register :  pulser.Register \u2014 The low-level Pulser register.</p> </li> </ul> <p> Methods </p> <ul> <li> <p>draw \u2014 Draw the geometry of this register.</p> </li> </ul> <p> source method Register.draw() \u2192 None </p> <p>Draw the geometry of this register.</p>"},{"location":"src/qek/target/targets/","title":"qek.target.targets","text":"qek.target.targets<p> docs module qek.target.targets </p> <pre><code>\"\"\"\nCode emitted by compilation.\n\nIn practice, this code is a very thin layer around Pulser's representation.\n\"\"\"\n\nfrom dataclasses import dataclass\nimport pulser\n\n\n@dataclass\nclass Pulse:docs\n    \"\"\"\n    Specification of a laser pulse to be executed on a quantum device\n\n    Attributes:\n        pulse: The low-level Pulser pulse.\n    \"\"\"\n\n    pulse: pulser.Pulse\n\n    def draw(self) -&gt; None:docs\n        \"\"\"\n        Draw the shape of this laser pulse.\n        \"\"\"\n        self.pulse.draw()\n\n\n@dataclass\nclass Register:docs\n    \"\"\"\n    Specification of a geometry of atoms to be executed on a quantum device\n\n    Attributes:\n        device: The quantum device targeted.\n        register: The low-level Pulser register.\n    \"\"\"\n\n    device: pulser.devices.Device\n    register: pulser.Register\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        The number of qubits in this register.\n        \"\"\"\n        return len(self.register.qubits)\n\n    def draw(self) -&gt; None:docs\n        \"\"\"\n        Draw the geometry of this register.\n        \"\"\"\n        self.register.draw(blockade_radius=self.device.min_atom_distance + 0.01)\n</code></pre>"}]}